{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例2: 构建自己的多层感知机: MNIST 手写数字识别\n",
    "\n",
    "### 本案例要求如下\n",
    "- #### 实现SGD优化器 (`./optimizer.py`)\n",
    "- #### 实现全连接层FCLayer前向和后向计算 (`layers/fc_layer.py`)\n",
    "- #### 实现激活层SigmoidLayer前向和后向计算 (`layers/sigmoid_layer.py`)\n",
    "- #### 实现激活层ReLULayer前向和后向计算 (`layers/relu_layer.py`)\n",
    "- #### 实现损失层EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### 实现损失层SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:44:10.806769Z",
     "start_time": "2023-11-10T08:44:03.355708Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:44:11.089564Z",
     "start_time": "2023-11-10T08:44:10.806769Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:44:11.095528Z",
     "start_time": "2023-11-10T08:44:11.090569Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # 归一化处理\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # 将标签变为one-hot编码\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:44:11.377742Z",
     "start_time": "2023-11-10T08:44:11.097106Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数设置"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### 超参数(修改)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:44:11.382492Z",
     "start_time": "2023-11-10T08:44:11.379254Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128   # 64 , 128 , 256\n",
    "max_epoch = 50\n",
    "init_std = 0.02\n",
    "\n",
    "learning_rate_SGD = 0.01   # 0.0001 , 0.001 , 0.01\n",
    "weight_decay = 0.005\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "学习率(Learning rate) -> 较高的学习率可能导致模型无法收敛，而较低的学习率可能导致模型收敛速度慢\n",
    "\n",
    "批处理大小 (Batch Size) -> 较小的批处理大小可以导致模型的训练过程更加随机，但可能需要更多的迭代来收敛\n",
    "\n",
    "权重初始化 (Weight Initialization) -> 选择合适的权重初始化方法可以帮助模型更快地收敛，并可能达到更高的准确率\n",
    "                                     (影响因素较大)\n",
    "                                     \n",
    "激活函数 -> 不同的激活函数对模型的训练和收敛速度有影响\n",
    "            ReLU表现良好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 使用欧式距离损失训练多层感知机(MLP with Euclidean Loss)\n",
    "第一部分将使用欧式距离损失训练多层感知机. \n",
    "分别使用**Sigmoid**激活函数和**ReLU**激活函数.\n",
    "### TODO\n",
    "执行以下代码之前，请完成 **./optimizer.py** 和 **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:44:11.394350Z",
     "start_time": "2023-11-10T08:44:11.384505Z"
    }
   },
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 使用欧式距离损失和Sigmoid激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用欧式距离损失和Sigmoid激活函数.\n",
    "\n",
    "### TODO\n",
    "执行以下代码之前，请完成 **layers/fc_layer.py** 和 **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:44:11.408465Z",
     "start_time": "2023-11-10T08:44:11.396175Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:45:15.075141Z",
     "start_time": "2023-11-10T08:44:11.410247Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\31393\\Desktop\\实验二\\mlp\\solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "Epoch [0][50]\t Batch [0][429]\t Training Loss 4.7274\t Accuracy 0.1016\n",
      "Epoch [0][50]\t Batch [50][429]\t Training Loss 0.6002\t Accuracy 0.1501\n",
      "Epoch [0][50]\t Batch [100][429]\t Training Loss 0.5183\t Accuracy 0.2043\n",
      "Epoch [0][50]\t Batch [150][429]\t Training Loss 0.4828\t Accuracy 0.2443\n",
      "Epoch [0][50]\t Batch [200][429]\t Training Loss 0.4563\t Accuracy 0.2900\n",
      "Epoch [0][50]\t Batch [250][429]\t Training Loss 0.4371\t Accuracy 0.3278\n",
      "Epoch [0][50]\t Batch [300][429]\t Training Loss 0.4207\t Accuracy 0.3637\n",
      "Epoch [0][50]\t Batch [350][429]\t Training Loss 0.4068\t Accuracy 0.3963\n",
      "Epoch [0][50]\t Batch [400][429]\t Training Loss 0.3947\t Accuracy 0.4249\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3882\t Average training accuracy 0.4399\n",
      "Epoch [0]\t Average validation loss 0.2859\t Average validation accuracy 0.6887\n",
      "\n",
      "Epoch [1][50]\t Batch [0][429]\t Training Loss 0.3271\t Accuracy 0.5312\n",
      "Epoch [1][50]\t Batch [50][429]\t Training Loss 0.2852\t Accuracy 0.6757\n",
      "Epoch [1][50]\t Batch [100][429]\t Training Loss 0.2815\t Accuracy 0.6843\n",
      "Epoch [1][50]\t Batch [150][429]\t Training Loss 0.2816\t Accuracy 0.6827\n",
      "Epoch [1][50]\t Batch [200][429]\t Training Loss 0.2772\t Accuracy 0.6907\n",
      "Epoch [1][50]\t Batch [250][429]\t Training Loss 0.2751\t Accuracy 0.6937\n",
      "Epoch [1][50]\t Batch [300][429]\t Training Loss 0.2719\t Accuracy 0.6992\n",
      "Epoch [1][50]\t Batch [350][429]\t Training Loss 0.2689\t Accuracy 0.7051\n",
      "Epoch [1][50]\t Batch [400][429]\t Training Loss 0.2662\t Accuracy 0.7095\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2643\t Average training accuracy 0.7132\n",
      "Epoch [1]\t Average validation loss 0.2256\t Average validation accuracy 0.7905\n",
      "\n",
      "Epoch [2][50]\t Batch [0][429]\t Training Loss 0.2671\t Accuracy 0.6797\n",
      "Epoch [2][50]\t Batch [50][429]\t Training Loss 0.2320\t Accuracy 0.7630\n",
      "Epoch [2][50]\t Batch [100][429]\t Training Loss 0.2310\t Accuracy 0.7672\n",
      "Epoch [2][50]\t Batch [150][429]\t Training Loss 0.2338\t Accuracy 0.7606\n",
      "Epoch [2][50]\t Batch [200][429]\t Training Loss 0.2315\t Accuracy 0.7634\n",
      "Epoch [2][50]\t Batch [250][429]\t Training Loss 0.2313\t Accuracy 0.7638\n",
      "Epoch [2][50]\t Batch [300][429]\t Training Loss 0.2299\t Accuracy 0.7663\n",
      "Epoch [2][50]\t Batch [350][429]\t Training Loss 0.2287\t Accuracy 0.7689\n",
      "Epoch [2][50]\t Batch [400][429]\t Training Loss 0.2275\t Accuracy 0.7702\n",
      "\n",
      "Epoch [2]\t Average training loss 0.2263\t Average training accuracy 0.7721\n",
      "Epoch [2]\t Average validation loss 0.1972\t Average validation accuracy 0.8287\n",
      "\n",
      "Epoch [3][50]\t Batch [0][429]\t Training Loss 0.1777\t Accuracy 0.8750\n",
      "Epoch [3][50]\t Batch [50][429]\t Training Loss 0.2064\t Accuracy 0.8001\n",
      "Epoch [3][50]\t Batch [100][429]\t Training Loss 0.2067\t Accuracy 0.8024\n",
      "Epoch [3][50]\t Batch [150][429]\t Training Loss 0.2100\t Accuracy 0.7947\n",
      "Epoch [3][50]\t Batch [200][429]\t Training Loss 0.2087\t Accuracy 0.7957\n",
      "Epoch [3][50]\t Batch [250][429]\t Training Loss 0.2091\t Accuracy 0.7946\n",
      "Epoch [3][50]\t Batch [300][429]\t Training Loss 0.2084\t Accuracy 0.7959\n",
      "Epoch [3][50]\t Batch [350][429]\t Training Loss 0.2078\t Accuracy 0.7972\n",
      "Epoch [3][50]\t Batch [400][429]\t Training Loss 0.2072\t Accuracy 0.7974\n",
      "\n",
      "Epoch [3]\t Average training loss 0.2064\t Average training accuracy 0.7989\n",
      "Epoch [3]\t Average validation loss 0.1806\t Average validation accuracy 0.8510\n",
      "\n",
      "Epoch [4][50]\t Batch [0][429]\t Training Loss 0.1560\t Accuracy 0.9141\n",
      "Epoch [4][50]\t Batch [50][429]\t Training Loss 0.1912\t Accuracy 0.8189\n",
      "Epoch [4][50]\t Batch [100][429]\t Training Loss 0.1918\t Accuracy 0.8206\n",
      "Epoch [4][50]\t Batch [150][429]\t Training Loss 0.1956\t Accuracy 0.8123\n",
      "Epoch [4][50]\t Batch [200][429]\t Training Loss 0.1948\t Accuracy 0.8124\n",
      "Epoch [4][50]\t Batch [250][429]\t Training Loss 0.1954\t Accuracy 0.8119\n",
      "Epoch [4][50]\t Batch [300][429]\t Training Loss 0.1951\t Accuracy 0.8125\n",
      "Epoch [4][50]\t Batch [350][429]\t Training Loss 0.1949\t Accuracy 0.8126\n",
      "Epoch [4][50]\t Batch [400][429]\t Training Loss 0.1946\t Accuracy 0.8126\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1939\t Average training accuracy 0.8140\n",
      "Epoch [4]\t Average validation loss 0.1700\t Average validation accuracy 0.8614\n",
      "\n",
      "Epoch [5][50]\t Batch [0][429]\t Training Loss 0.1741\t Accuracy 0.8438\n",
      "Epoch [5][50]\t Batch [50][429]\t Training Loss 0.1816\t Accuracy 0.8336\n",
      "Epoch [5][50]\t Batch [100][429]\t Training Loss 0.1820\t Accuracy 0.8327\n",
      "Epoch [5][50]\t Batch [150][429]\t Training Loss 0.1860\t Accuracy 0.8235\n",
      "Epoch [5][50]\t Batch [200][429]\t Training Loss 0.1853\t Accuracy 0.8240\n",
      "Epoch [5][50]\t Batch [250][429]\t Training Loss 0.1860\t Accuracy 0.8233\n",
      "Epoch [5][50]\t Batch [300][429]\t Training Loss 0.1861\t Accuracy 0.8236\n",
      "Epoch [5][50]\t Batch [350][429]\t Training Loss 0.1860\t Accuracy 0.8234\n",
      "Epoch [5][50]\t Batch [400][429]\t Training Loss 0.1859\t Accuracy 0.8230\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1852\t Average training accuracy 0.8244\n",
      "Epoch [5]\t Average validation loss 0.1624\t Average validation accuracy 0.8690\n",
      "\n",
      "Epoch [6][50]\t Batch [0][429]\t Training Loss 0.1948\t Accuracy 0.7734\n",
      "Epoch [6][50]\t Batch [50][429]\t Training Loss 0.1750\t Accuracy 0.8387\n",
      "Epoch [6][50]\t Batch [100][429]\t Training Loss 0.1750\t Accuracy 0.8393\n",
      "Epoch [6][50]\t Batch [150][429]\t Training Loss 0.1792\t Accuracy 0.8302\n",
      "Epoch [6][50]\t Batch [200][429]\t Training Loss 0.1785\t Accuracy 0.8311\n",
      "Epoch [6][50]\t Batch [250][429]\t Training Loss 0.1791\t Accuracy 0.8311\n",
      "Epoch [6][50]\t Batch [300][429]\t Training Loss 0.1794\t Accuracy 0.8312\n",
      "Epoch [6][50]\t Batch [350][429]\t Training Loss 0.1795\t Accuracy 0.8308\n",
      "Epoch [6][50]\t Batch [400][429]\t Training Loss 0.1795\t Accuracy 0.8304\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1789\t Average training accuracy 0.8316\n",
      "Epoch [6]\t Average validation loss 0.1566\t Average validation accuracy 0.8772\n",
      "\n",
      "Epoch [7][50]\t Batch [0][429]\t Training Loss 0.1672\t Accuracy 0.8281\n",
      "Epoch [7][50]\t Batch [50][429]\t Training Loss 0.1698\t Accuracy 0.8439\n",
      "Epoch [7][50]\t Batch [100][429]\t Training Loss 0.1697\t Accuracy 0.8448\n",
      "Epoch [7][50]\t Batch [150][429]\t Training Loss 0.1741\t Accuracy 0.8354\n",
      "Epoch [7][50]\t Batch [200][429]\t Training Loss 0.1733\t Accuracy 0.8370\n",
      "Epoch [7][50]\t Batch [250][429]\t Training Loss 0.1740\t Accuracy 0.8366\n",
      "Epoch [7][50]\t Batch [300][429]\t Training Loss 0.1744\t Accuracy 0.8366\n",
      "Epoch [7][50]\t Batch [350][429]\t Training Loss 0.1745\t Accuracy 0.8365\n",
      "Epoch [7][50]\t Batch [400][429]\t Training Loss 0.1746\t Accuracy 0.8360\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1740\t Average training accuracy 0.8370\n",
      "Epoch [7]\t Average validation loss 0.1520\t Average validation accuracy 0.8810\n",
      "\n",
      "Epoch [8][50]\t Batch [0][429]\t Training Loss 0.1534\t Accuracy 0.8750\n",
      "Epoch [8][50]\t Batch [50][429]\t Training Loss 0.1653\t Accuracy 0.8506\n",
      "Epoch [8][50]\t Batch [100][429]\t Training Loss 0.1653\t Accuracy 0.8503\n",
      "Epoch [8][50]\t Batch [150][429]\t Training Loss 0.1699\t Accuracy 0.8405\n",
      "Epoch [8][50]\t Batch [200][429]\t Training Loss 0.1693\t Accuracy 0.8421\n",
      "Epoch [8][50]\t Batch [250][429]\t Training Loss 0.1699\t Accuracy 0.8415\n",
      "Epoch [8][50]\t Batch [300][429]\t Training Loss 0.1703\t Accuracy 0.8415\n",
      "Epoch [8][50]\t Batch [350][429]\t Training Loss 0.1705\t Accuracy 0.8414\n",
      "Epoch [8][50]\t Batch [400][429]\t Training Loss 0.1707\t Accuracy 0.8408\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1702\t Average training accuracy 0.8416\n",
      "Epoch [8]\t Average validation loss 0.1485\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [9][50]\t Batch [0][429]\t Training Loss 0.1469\t Accuracy 0.8828\n",
      "Epoch [9][50]\t Batch [50][429]\t Training Loss 0.1620\t Accuracy 0.8539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9][50]\t Batch [100][429]\t Training Loss 0.1620\t Accuracy 0.8545\n",
      "Epoch [9][50]\t Batch [150][429]\t Training Loss 0.1667\t Accuracy 0.8448\n",
      "Epoch [9][50]\t Batch [200][429]\t Training Loss 0.1659\t Accuracy 0.8467\n",
      "Epoch [9][50]\t Batch [250][429]\t Training Loss 0.1665\t Accuracy 0.8459\n",
      "Epoch [9][50]\t Batch [300][429]\t Training Loss 0.1670\t Accuracy 0.8457\n",
      "Epoch [9][50]\t Batch [350][429]\t Training Loss 0.1672\t Accuracy 0.8456\n",
      "Epoch [9][50]\t Batch [400][429]\t Training Loss 0.1675\t Accuracy 0.8452\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1671\t Average training accuracy 0.8458\n",
      "Epoch [9]\t Average validation loss 0.1457\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [10][50]\t Batch [0][429]\t Training Loss 0.1506\t Accuracy 0.8750\n",
      "Epoch [10][50]\t Batch [50][429]\t Training Loss 0.1593\t Accuracy 0.8566\n",
      "Epoch [10][50]\t Batch [100][429]\t Training Loss 0.1593\t Accuracy 0.8576\n",
      "Epoch [10][50]\t Batch [150][429]\t Training Loss 0.1640\t Accuracy 0.8476\n",
      "Epoch [10][50]\t Batch [200][429]\t Training Loss 0.1632\t Accuracy 0.8495\n",
      "Epoch [10][50]\t Batch [250][429]\t Training Loss 0.1637\t Accuracy 0.8489\n",
      "Epoch [10][50]\t Batch [300][429]\t Training Loss 0.1644\t Accuracy 0.8485\n",
      "Epoch [10][50]\t Batch [350][429]\t Training Loss 0.1646\t Accuracy 0.8486\n",
      "Epoch [10][50]\t Batch [400][429]\t Training Loss 0.1649\t Accuracy 0.8481\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1645\t Average training accuracy 0.8488\n",
      "Epoch [10]\t Average validation loss 0.1433\t Average validation accuracy 0.8904\n",
      "\n",
      "Epoch [11][50]\t Batch [0][429]\t Training Loss 0.1677\t Accuracy 0.8594\n",
      "Epoch [11][50]\t Batch [50][429]\t Training Loss 0.1575\t Accuracy 0.8589\n",
      "Epoch [11][50]\t Batch [100][429]\t Training Loss 0.1574\t Accuracy 0.8601\n",
      "Epoch [11][50]\t Batch [150][429]\t Training Loss 0.1618\t Accuracy 0.8507\n",
      "Epoch [11][50]\t Batch [200][429]\t Training Loss 0.1610\t Accuracy 0.8524\n",
      "Epoch [11][50]\t Batch [250][429]\t Training Loss 0.1617\t Accuracy 0.8517\n",
      "Epoch [11][50]\t Batch [300][429]\t Training Loss 0.1623\t Accuracy 0.8512\n",
      "Epoch [11][50]\t Batch [350][429]\t Training Loss 0.1625\t Accuracy 0.8512\n",
      "Epoch [11][50]\t Batch [400][429]\t Training Loss 0.1627\t Accuracy 0.8509\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1624\t Average training accuracy 0.8513\n",
      "Epoch [11]\t Average validation loss 0.1417\t Average validation accuracy 0.8904\n",
      "\n",
      "Epoch [12][50]\t Batch [0][429]\t Training Loss 0.2032\t Accuracy 0.8281\n",
      "Epoch [12][50]\t Batch [50][429]\t Training Loss 0.1562\t Accuracy 0.8618\n",
      "Epoch [12][50]\t Batch [100][429]\t Training Loss 0.1557\t Accuracy 0.8620\n",
      "Epoch [12][50]\t Batch [150][429]\t Training Loss 0.1602\t Accuracy 0.8527\n",
      "Epoch [12][50]\t Batch [200][429]\t Training Loss 0.1592\t Accuracy 0.8551\n",
      "Epoch [12][50]\t Batch [250][429]\t Training Loss 0.1598\t Accuracy 0.8540\n",
      "Epoch [12][50]\t Batch [300][429]\t Training Loss 0.1604\t Accuracy 0.8536\n",
      "Epoch [12][50]\t Batch [350][429]\t Training Loss 0.1607\t Accuracy 0.8533\n",
      "Epoch [12][50]\t Batch [400][429]\t Training Loss 0.1610\t Accuracy 0.8528\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1606\t Average training accuracy 0.8533\n",
      "Epoch [12]\t Average validation loss 0.1401\t Average validation accuracy 0.8968\n",
      "\n",
      "Epoch [13][50]\t Batch [0][429]\t Training Loss 0.1551\t Accuracy 0.8750\n",
      "Epoch [13][50]\t Batch [50][429]\t Training Loss 0.1547\t Accuracy 0.8632\n",
      "Epoch [13][50]\t Batch [100][429]\t Training Loss 0.1540\t Accuracy 0.8639\n",
      "Epoch [13][50]\t Batch [150][429]\t Training Loss 0.1586\t Accuracy 0.8551\n",
      "Epoch [13][50]\t Batch [200][429]\t Training Loss 0.1575\t Accuracy 0.8575\n",
      "Epoch [13][50]\t Batch [250][429]\t Training Loss 0.1582\t Accuracy 0.8560\n",
      "Epoch [13][50]\t Batch [300][429]\t Training Loss 0.1587\t Accuracy 0.8556\n",
      "Epoch [13][50]\t Batch [350][429]\t Training Loss 0.1591\t Accuracy 0.8552\n",
      "Epoch [13][50]\t Batch [400][429]\t Training Loss 0.1594\t Accuracy 0.8546\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1591\t Average training accuracy 0.8550\n",
      "Epoch [13]\t Average validation loss 0.1389\t Average validation accuracy 0.8974\n",
      "\n",
      "Epoch [14][50]\t Batch [0][429]\t Training Loss 0.1378\t Accuracy 0.8750\n",
      "Epoch [14][50]\t Batch [50][429]\t Training Loss 0.1529\t Accuracy 0.8655\n",
      "Epoch [14][50]\t Batch [100][429]\t Training Loss 0.1521\t Accuracy 0.8671\n",
      "Epoch [14][50]\t Batch [150][429]\t Training Loss 0.1572\t Accuracy 0.8567\n",
      "Epoch [14][50]\t Batch [200][429]\t Training Loss 0.1560\t Accuracy 0.8593\n",
      "Epoch [14][50]\t Batch [250][429]\t Training Loss 0.1568\t Accuracy 0.8575\n",
      "Epoch [14][50]\t Batch [300][429]\t Training Loss 0.1573\t Accuracy 0.8573\n",
      "Epoch [14][50]\t Batch [350][429]\t Training Loss 0.1577\t Accuracy 0.8568\n",
      "Epoch [14][50]\t Batch [400][429]\t Training Loss 0.1580\t Accuracy 0.8562\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1577\t Average training accuracy 0.8567\n",
      "Epoch [14]\t Average validation loss 0.1374\t Average validation accuracy 0.8984\n",
      "\n",
      "Epoch [15][50]\t Batch [0][429]\t Training Loss 0.1407\t Accuracy 0.8750\n",
      "Epoch [15][50]\t Batch [50][429]\t Training Loss 0.1517\t Accuracy 0.8666\n",
      "Epoch [15][50]\t Batch [100][429]\t Training Loss 0.1507\t Accuracy 0.8688\n",
      "Epoch [15][50]\t Batch [150][429]\t Training Loss 0.1560\t Accuracy 0.8576\n",
      "Epoch [15][50]\t Batch [200][429]\t Training Loss 0.1547\t Accuracy 0.8605\n",
      "Epoch [15][50]\t Batch [250][429]\t Training Loss 0.1554\t Accuracy 0.8593\n",
      "Epoch [15][50]\t Batch [300][429]\t Training Loss 0.1560\t Accuracy 0.8587\n",
      "Epoch [15][50]\t Batch [350][429]\t Training Loss 0.1565\t Accuracy 0.8580\n",
      "Epoch [15][50]\t Batch [400][429]\t Training Loss 0.1568\t Accuracy 0.8574\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1565\t Average training accuracy 0.8580\n",
      "Epoch [15]\t Average validation loss 0.1364\t Average validation accuracy 0.8990\n",
      "\n",
      "Epoch [16][50]\t Batch [0][429]\t Training Loss 0.1422\t Accuracy 0.8750\n",
      "Epoch [16][50]\t Batch [50][429]\t Training Loss 0.1502\t Accuracy 0.8676\n",
      "Epoch [16][50]\t Batch [100][429]\t Training Loss 0.1492\t Accuracy 0.8698\n",
      "Epoch [16][50]\t Batch [150][429]\t Training Loss 0.1550\t Accuracy 0.8589\n",
      "Epoch [16][50]\t Batch [200][429]\t Training Loss 0.1537\t Accuracy 0.8619\n",
      "Epoch [16][50]\t Batch [250][429]\t Training Loss 0.1543\t Accuracy 0.8604\n",
      "Epoch [16][50]\t Batch [300][429]\t Training Loss 0.1549\t Accuracy 0.8596\n",
      "Epoch [16][50]\t Batch [350][429]\t Training Loss 0.1554\t Accuracy 0.8587\n",
      "Epoch [16][50]\t Batch [400][429]\t Training Loss 0.1557\t Accuracy 0.8581\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1554\t Average training accuracy 0.8587\n",
      "Epoch [16]\t Average validation loss 0.1351\t Average validation accuracy 0.9020\n",
      "\n",
      "Epoch [17][50]\t Batch [0][429]\t Training Loss 0.1696\t Accuracy 0.8516\n",
      "Epoch [17][50]\t Batch [50][429]\t Training Loss 0.1494\t Accuracy 0.8689\n",
      "Epoch [17][50]\t Batch [100][429]\t Training Loss 0.1486\t Accuracy 0.8708\n",
      "Epoch [17][50]\t Batch [150][429]\t Training Loss 0.1541\t Accuracy 0.8599\n",
      "Epoch [17][50]\t Batch [200][429]\t Training Loss 0.1527\t Accuracy 0.8630\n",
      "Epoch [17][50]\t Batch [250][429]\t Training Loss 0.1533\t Accuracy 0.8614\n",
      "Epoch [17][50]\t Batch [300][429]\t Training Loss 0.1539\t Accuracy 0.8609\n",
      "Epoch [17][50]\t Batch [350][429]\t Training Loss 0.1546\t Accuracy 0.8598\n",
      "Epoch [17][50]\t Batch [400][429]\t Training Loss 0.1548\t Accuracy 0.8593\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1546\t Average training accuracy 0.8597\n",
      "Epoch [17]\t Average validation loss 0.1341\t Average validation accuracy 0.9044\n",
      "\n",
      "Epoch [18][50]\t Batch [0][429]\t Training Loss 0.1408\t Accuracy 0.8906\n",
      "Epoch [18][50]\t Batch [50][429]\t Training Loss 0.1484\t Accuracy 0.8703\n",
      "Epoch [18][50]\t Batch [100][429]\t Training Loss 0.1477\t Accuracy 0.8728\n",
      "Epoch [18][50]\t Batch [150][429]\t Training Loss 0.1530\t Accuracy 0.8616\n",
      "Epoch [18][50]\t Batch [200][429]\t Training Loss 0.1518\t Accuracy 0.8646\n",
      "Epoch [18][50]\t Batch [250][429]\t Training Loss 0.1523\t Accuracy 0.8635\n",
      "Epoch [18][50]\t Batch [300][429]\t Training Loss 0.1531\t Accuracy 0.8622\n",
      "Epoch [18][50]\t Batch [350][429]\t Training Loss 0.1537\t Accuracy 0.8612\n",
      "Epoch [18][50]\t Batch [400][429]\t Training Loss 0.1538\t Accuracy 0.8606\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1538\t Average training accuracy 0.8609\n",
      "Epoch [18]\t Average validation loss 0.1333\t Average validation accuracy 0.9050\n",
      "\n",
      "Epoch [19][50]\t Batch [0][429]\t Training Loss 0.1046\t Accuracy 0.9609\n",
      "Epoch [19][50]\t Batch [50][429]\t Training Loss 0.1472\t Accuracy 0.8735\n",
      "Epoch [19][50]\t Batch [100][429]\t Training Loss 0.1468\t Accuracy 0.8745\n",
      "Epoch [19][50]\t Batch [150][429]\t Training Loss 0.1518\t Accuracy 0.8635\n",
      "Epoch [19][50]\t Batch [200][429]\t Training Loss 0.1509\t Accuracy 0.8662\n",
      "Epoch [19][50]\t Batch [250][429]\t Training Loss 0.1514\t Accuracy 0.8653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19][50]\t Batch [300][429]\t Training Loss 0.1522\t Accuracy 0.8638\n",
      "Epoch [19][50]\t Batch [350][429]\t Training Loss 0.1530\t Accuracy 0.8626\n",
      "Epoch [19][50]\t Batch [400][429]\t Training Loss 0.1530\t Accuracy 0.8622\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1529\t Average training accuracy 0.8625\n",
      "Epoch [19]\t Average validation loss 0.1326\t Average validation accuracy 0.9058\n",
      "\n",
      "Epoch [20][50]\t Batch [0][429]\t Training Loss 0.1287\t Accuracy 0.8906\n",
      "Epoch [20][50]\t Batch [50][429]\t Training Loss 0.1462\t Accuracy 0.8744\n",
      "Epoch [20][50]\t Batch [100][429]\t Training Loss 0.1460\t Accuracy 0.8749\n",
      "Epoch [20][50]\t Batch [150][429]\t Training Loss 0.1509\t Accuracy 0.8646\n",
      "Epoch [20][50]\t Batch [200][429]\t Training Loss 0.1502\t Accuracy 0.8670\n",
      "Epoch [20][50]\t Batch [250][429]\t Training Loss 0.1506\t Accuracy 0.8660\n",
      "Epoch [20][50]\t Batch [300][429]\t Training Loss 0.1515\t Accuracy 0.8644\n",
      "Epoch [20][50]\t Batch [350][429]\t Training Loss 0.1522\t Accuracy 0.8636\n",
      "Epoch [20][50]\t Batch [400][429]\t Training Loss 0.1522\t Accuracy 0.8632\n",
      "\n",
      "Epoch [20]\t Average training loss 0.1522\t Average training accuracy 0.8634\n",
      "Epoch [20]\t Average validation loss 0.1319\t Average validation accuracy 0.9083\n",
      "\n",
      "Epoch [21][50]\t Batch [0][429]\t Training Loss 0.1505\t Accuracy 0.8672\n",
      "Epoch [21][50]\t Batch [50][429]\t Training Loss 0.1456\t Accuracy 0.8758\n",
      "Epoch [21][50]\t Batch [100][429]\t Training Loss 0.1454\t Accuracy 0.8760\n",
      "Epoch [21][50]\t Batch [150][429]\t Training Loss 0.1500\t Accuracy 0.8664\n",
      "Epoch [21][50]\t Batch [200][429]\t Training Loss 0.1495\t Accuracy 0.8684\n",
      "Epoch [21][50]\t Batch [250][429]\t Training Loss 0.1500\t Accuracy 0.8673\n",
      "Epoch [21][50]\t Batch [300][429]\t Training Loss 0.1509\t Accuracy 0.8655\n",
      "Epoch [21][50]\t Batch [350][429]\t Training Loss 0.1516\t Accuracy 0.8647\n",
      "Epoch [21][50]\t Batch [400][429]\t Training Loss 0.1514\t Accuracy 0.8646\n",
      "\n",
      "Epoch [21]\t Average training loss 0.1516\t Average training accuracy 0.8644\n",
      "Epoch [21]\t Average validation loss 0.1313\t Average validation accuracy 0.9087\n",
      "\n",
      "Epoch [22][50]\t Batch [0][429]\t Training Loss 0.1482\t Accuracy 0.8906\n",
      "Epoch [22][50]\t Batch [50][429]\t Training Loss 0.1449\t Accuracy 0.8775\n",
      "Epoch [22][50]\t Batch [100][429]\t Training Loss 0.1449\t Accuracy 0.8760\n",
      "Epoch [22][50]\t Batch [150][429]\t Training Loss 0.1494\t Accuracy 0.8672\n",
      "Epoch [22][50]\t Batch [200][429]\t Training Loss 0.1488\t Accuracy 0.8692\n",
      "Epoch [22][50]\t Batch [250][429]\t Training Loss 0.1493\t Accuracy 0.8681\n",
      "Epoch [22][50]\t Batch [300][429]\t Training Loss 0.1503\t Accuracy 0.8664\n",
      "Epoch [22][50]\t Batch [350][429]\t Training Loss 0.1509\t Accuracy 0.8657\n",
      "Epoch [22][50]\t Batch [400][429]\t Training Loss 0.1508\t Accuracy 0.8657\n",
      "\n",
      "Epoch [22]\t Average training loss 0.1510\t Average training accuracy 0.8655\n",
      "Epoch [22]\t Average validation loss 0.1306\t Average validation accuracy 0.9089\n",
      "\n",
      "Epoch [23][50]\t Batch [0][429]\t Training Loss 0.1448\t Accuracy 0.8750\n",
      "Epoch [23][50]\t Batch [50][429]\t Training Loss 0.1449\t Accuracy 0.8764\n",
      "Epoch [23][50]\t Batch [100][429]\t Training Loss 0.1446\t Accuracy 0.8763\n",
      "Epoch [23][50]\t Batch [150][429]\t Training Loss 0.1489\t Accuracy 0.8678\n",
      "Epoch [23][50]\t Batch [200][429]\t Training Loss 0.1482\t Accuracy 0.8700\n",
      "Epoch [23][50]\t Batch [250][429]\t Training Loss 0.1486\t Accuracy 0.8689\n",
      "Epoch [23][50]\t Batch [300][429]\t Training Loss 0.1498\t Accuracy 0.8672\n",
      "Epoch [23][50]\t Batch [350][429]\t Training Loss 0.1503\t Accuracy 0.8666\n",
      "Epoch [23][50]\t Batch [400][429]\t Training Loss 0.1503\t Accuracy 0.8663\n",
      "\n",
      "Epoch [23]\t Average training loss 0.1503\t Average training accuracy 0.8662\n",
      "Epoch [23]\t Average validation loss 0.1299\t Average validation accuracy 0.9103\n",
      "\n",
      "Epoch [24][50]\t Batch [0][429]\t Training Loss 0.1779\t Accuracy 0.8203\n",
      "Epoch [24][50]\t Batch [50][429]\t Training Loss 0.1450\t Accuracy 0.8756\n",
      "Epoch [24][50]\t Batch [100][429]\t Training Loss 0.1447\t Accuracy 0.8759\n",
      "Epoch [24][50]\t Batch [150][429]\t Training Loss 0.1486\t Accuracy 0.8682\n",
      "Epoch [24][50]\t Batch [200][429]\t Training Loss 0.1479\t Accuracy 0.8709\n",
      "Epoch [24][50]\t Batch [250][429]\t Training Loss 0.1482\t Accuracy 0.8701\n",
      "Epoch [24][50]\t Batch [300][429]\t Training Loss 0.1493\t Accuracy 0.8682\n",
      "Epoch [24][50]\t Batch [350][429]\t Training Loss 0.1499\t Accuracy 0.8676\n",
      "Epoch [24][50]\t Batch [400][429]\t Training Loss 0.1499\t Accuracy 0.8672\n",
      "\n",
      "Epoch [24]\t Average training loss 0.1499\t Average training accuracy 0.8672\n",
      "Epoch [24]\t Average validation loss 0.1294\t Average validation accuracy 0.9093\n",
      "\n",
      "Epoch [25][50]\t Batch [0][429]\t Training Loss 0.1651\t Accuracy 0.8281\n",
      "Epoch [25][50]\t Batch [50][429]\t Training Loss 0.1448\t Accuracy 0.8747\n",
      "Epoch [25][50]\t Batch [100][429]\t Training Loss 0.1442\t Accuracy 0.8751\n",
      "Epoch [25][50]\t Batch [150][429]\t Training Loss 0.1481\t Accuracy 0.8683\n",
      "Epoch [25][50]\t Batch [200][429]\t Training Loss 0.1475\t Accuracy 0.8708\n",
      "Epoch [25][50]\t Batch [250][429]\t Training Loss 0.1477\t Accuracy 0.8704\n",
      "Epoch [25][50]\t Batch [300][429]\t Training Loss 0.1488\t Accuracy 0.8687\n",
      "Epoch [25][50]\t Batch [350][429]\t Training Loss 0.1494\t Accuracy 0.8678\n",
      "Epoch [25][50]\t Batch [400][429]\t Training Loss 0.1493\t Accuracy 0.8678\n",
      "\n",
      "Epoch [25]\t Average training loss 0.1494\t Average training accuracy 0.8676\n",
      "Epoch [25]\t Average validation loss 0.1288\t Average validation accuracy 0.9097\n",
      "\n",
      "Epoch [26][50]\t Batch [0][429]\t Training Loss 0.1666\t Accuracy 0.8125\n",
      "Epoch [26][50]\t Batch [50][429]\t Training Loss 0.1445\t Accuracy 0.8745\n",
      "Epoch [26][50]\t Batch [100][429]\t Training Loss 0.1439\t Accuracy 0.8759\n",
      "Epoch [26][50]\t Batch [150][429]\t Training Loss 0.1476\t Accuracy 0.8695\n",
      "Epoch [26][50]\t Batch [200][429]\t Training Loss 0.1471\t Accuracy 0.8717\n",
      "Epoch [26][50]\t Batch [250][429]\t Training Loss 0.1471\t Accuracy 0.8713\n",
      "Epoch [26][50]\t Batch [300][429]\t Training Loss 0.1485\t Accuracy 0.8693\n",
      "Epoch [26][50]\t Batch [350][429]\t Training Loss 0.1490\t Accuracy 0.8686\n",
      "Epoch [26][50]\t Batch [400][429]\t Training Loss 0.1489\t Accuracy 0.8686\n",
      "\n",
      "Epoch [26]\t Average training loss 0.1490\t Average training accuracy 0.8684\n",
      "Epoch [26]\t Average validation loss 0.1284\t Average validation accuracy 0.9089\n",
      "\n",
      "Epoch [27][50]\t Batch [0][429]\t Training Loss 0.1646\t Accuracy 0.8125\n",
      "Epoch [27][50]\t Batch [50][429]\t Training Loss 0.1441\t Accuracy 0.8745\n",
      "Epoch [27][50]\t Batch [100][429]\t Training Loss 0.1438\t Accuracy 0.8758\n",
      "Epoch [27][50]\t Batch [150][429]\t Training Loss 0.1472\t Accuracy 0.8696\n",
      "Epoch [27][50]\t Batch [200][429]\t Training Loss 0.1467\t Accuracy 0.8720\n",
      "Epoch [27][50]\t Batch [250][429]\t Training Loss 0.1467\t Accuracy 0.8721\n",
      "Epoch [27][50]\t Batch [300][429]\t Training Loss 0.1481\t Accuracy 0.8700\n",
      "Epoch [27][50]\t Batch [350][429]\t Training Loss 0.1487\t Accuracy 0.8693\n",
      "Epoch [27][50]\t Batch [400][429]\t Training Loss 0.1484\t Accuracy 0.8694\n",
      "\n",
      "Epoch [27]\t Average training loss 0.1487\t Average training accuracy 0.8689\n",
      "Epoch [27]\t Average validation loss 0.1278\t Average validation accuracy 0.9097\n",
      "\n",
      "Epoch [28][50]\t Batch [0][429]\t Training Loss 0.1133\t Accuracy 0.9375\n",
      "Epoch [28][50]\t Batch [50][429]\t Training Loss 0.1432\t Accuracy 0.8768\n",
      "Epoch [28][50]\t Batch [100][429]\t Training Loss 0.1433\t Accuracy 0.8770\n",
      "Epoch [28][50]\t Batch [150][429]\t Training Loss 0.1465\t Accuracy 0.8715\n",
      "Epoch [28][50]\t Batch [200][429]\t Training Loss 0.1462\t Accuracy 0.8734\n",
      "Epoch [28][50]\t Batch [250][429]\t Training Loss 0.1461\t Accuracy 0.8732\n",
      "Epoch [28][50]\t Batch [300][429]\t Training Loss 0.1476\t Accuracy 0.8710\n",
      "Epoch [28][50]\t Batch [350][429]\t Training Loss 0.1481\t Accuracy 0.8703\n",
      "Epoch [28][50]\t Batch [400][429]\t Training Loss 0.1479\t Accuracy 0.8704\n",
      "\n",
      "Epoch [28]\t Average training loss 0.1482\t Average training accuracy 0.8698\n",
      "Epoch [28]\t Average validation loss 0.1278\t Average validation accuracy 0.9101\n",
      "\n",
      "Epoch [29][50]\t Batch [0][429]\t Training Loss 0.1114\t Accuracy 0.9375\n",
      "Epoch [29][50]\t Batch [50][429]\t Training Loss 0.1425\t Accuracy 0.8776\n",
      "Epoch [29][50]\t Batch [100][429]\t Training Loss 0.1429\t Accuracy 0.8776\n",
      "Epoch [29][50]\t Batch [150][429]\t Training Loss 0.1459\t Accuracy 0.8721\n",
      "Epoch [29][50]\t Batch [200][429]\t Training Loss 0.1457\t Accuracy 0.8742\n",
      "Epoch [29][50]\t Batch [250][429]\t Training Loss 0.1456\t Accuracy 0.8738\n",
      "Epoch [29][50]\t Batch [300][429]\t Training Loss 0.1470\t Accuracy 0.8715\n",
      "Epoch [29][50]\t Batch [350][429]\t Training Loss 0.1477\t Accuracy 0.8707\n",
      "Epoch [29][50]\t Batch [400][429]\t Training Loss 0.1475\t Accuracy 0.8708\n",
      "\n",
      "Epoch [29]\t Average training loss 0.1479\t Average training accuracy 0.8700\n",
      "Epoch [29]\t Average validation loss 0.1274\t Average validation accuracy 0.9105\n",
      "\n",
      "Epoch [30][50]\t Batch [0][429]\t Training Loss 0.1196\t Accuracy 0.9141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30][50]\t Batch [50][429]\t Training Loss 0.1419\t Accuracy 0.8779\n",
      "Epoch [30][50]\t Batch [100][429]\t Training Loss 0.1426\t Accuracy 0.8776\n",
      "Epoch [30][50]\t Batch [150][429]\t Training Loss 0.1453\t Accuracy 0.8726\n",
      "Epoch [30][50]\t Batch [200][429]\t Training Loss 0.1453\t Accuracy 0.8742\n",
      "Epoch [30][50]\t Batch [250][429]\t Training Loss 0.1452\t Accuracy 0.8740\n",
      "Epoch [30][50]\t Batch [300][429]\t Training Loss 0.1466\t Accuracy 0.8718\n",
      "Epoch [30][50]\t Batch [350][429]\t Training Loss 0.1471\t Accuracy 0.8711\n",
      "Epoch [30][50]\t Batch [400][429]\t Training Loss 0.1471\t Accuracy 0.8711\n",
      "\n",
      "Epoch [30]\t Average training loss 0.1474\t Average training accuracy 0.8704\n",
      "Epoch [30]\t Average validation loss 0.1269\t Average validation accuracy 0.9097\n",
      "\n",
      "Epoch [31][50]\t Batch [0][429]\t Training Loss 0.1490\t Accuracy 0.8438\n",
      "Epoch [31][50]\t Batch [50][429]\t Training Loss 0.1419\t Accuracy 0.8768\n",
      "Epoch [31][50]\t Batch [100][429]\t Training Loss 0.1421\t Accuracy 0.8785\n",
      "Epoch [31][50]\t Batch [150][429]\t Training Loss 0.1450\t Accuracy 0.8732\n",
      "Epoch [31][50]\t Batch [200][429]\t Training Loss 0.1449\t Accuracy 0.8752\n",
      "Epoch [31][50]\t Batch [250][429]\t Training Loss 0.1450\t Accuracy 0.8747\n",
      "Epoch [31][50]\t Batch [300][429]\t Training Loss 0.1463\t Accuracy 0.8726\n",
      "Epoch [31][50]\t Batch [350][429]\t Training Loss 0.1468\t Accuracy 0.8721\n",
      "Epoch [31][50]\t Batch [400][429]\t Training Loss 0.1468\t Accuracy 0.8717\n",
      "\n",
      "Epoch [31]\t Average training loss 0.1471\t Average training accuracy 0.8712\n",
      "Epoch [31]\t Average validation loss 0.1264\t Average validation accuracy 0.9093\n",
      "\n",
      "Epoch [32][50]\t Batch [0][429]\t Training Loss 0.1676\t Accuracy 0.8047\n",
      "Epoch [32][50]\t Batch [50][429]\t Training Loss 0.1420\t Accuracy 0.8753\n",
      "Epoch [32][50]\t Batch [100][429]\t Training Loss 0.1420\t Accuracy 0.8781\n",
      "Epoch [32][50]\t Batch [150][429]\t Training Loss 0.1449\t Accuracy 0.8731\n",
      "Epoch [32][50]\t Batch [200][429]\t Training Loss 0.1448\t Accuracy 0.8748\n",
      "Epoch [32][50]\t Batch [250][429]\t Training Loss 0.1447\t Accuracy 0.8748\n",
      "Epoch [32][50]\t Batch [300][429]\t Training Loss 0.1460\t Accuracy 0.8726\n",
      "Epoch [32][50]\t Batch [350][429]\t Training Loss 0.1464\t Accuracy 0.8722\n",
      "Epoch [32][50]\t Batch [400][429]\t Training Loss 0.1465\t Accuracy 0.8717\n",
      "\n",
      "Epoch [32]\t Average training loss 0.1467\t Average training accuracy 0.8714\n",
      "Epoch [32]\t Average validation loss 0.1261\t Average validation accuracy 0.9099\n",
      "\n",
      "Epoch [33][50]\t Batch [0][429]\t Training Loss 0.2090\t Accuracy 0.7422\n",
      "Epoch [33][50]\t Batch [50][429]\t Training Loss 0.1428\t Accuracy 0.8735\n",
      "Epoch [33][50]\t Batch [100][429]\t Training Loss 0.1423\t Accuracy 0.8772\n",
      "Epoch [33][50]\t Batch [150][429]\t Training Loss 0.1450\t Accuracy 0.8721\n",
      "Epoch [33][50]\t Batch [200][429]\t Training Loss 0.1448\t Accuracy 0.8744\n",
      "Epoch [33][50]\t Batch [250][429]\t Training Loss 0.1446\t Accuracy 0.8744\n",
      "Epoch [33][50]\t Batch [300][429]\t Training Loss 0.1459\t Accuracy 0.8721\n",
      "Epoch [33][50]\t Batch [350][429]\t Training Loss 0.1463\t Accuracy 0.8718\n",
      "Epoch [33][50]\t Batch [400][429]\t Training Loss 0.1464\t Accuracy 0.8714\n",
      "\n",
      "Epoch [33]\t Average training loss 0.1465\t Average training accuracy 0.8712\n",
      "Epoch [33]\t Average validation loss 0.1257\t Average validation accuracy 0.9111\n",
      "\n",
      "Epoch [34][50]\t Batch [0][429]\t Training Loss 0.1792\t Accuracy 0.8203\n",
      "Epoch [34][50]\t Batch [50][429]\t Training Loss 0.1429\t Accuracy 0.8744\n",
      "Epoch [34][50]\t Batch [100][429]\t Training Loss 0.1421\t Accuracy 0.8778\n",
      "Epoch [34][50]\t Batch [150][429]\t Training Loss 0.1447\t Accuracy 0.8726\n",
      "Epoch [34][50]\t Batch [200][429]\t Training Loss 0.1445\t Accuracy 0.8751\n",
      "Epoch [34][50]\t Batch [250][429]\t Training Loss 0.1442\t Accuracy 0.8754\n",
      "Epoch [34][50]\t Batch [300][429]\t Training Loss 0.1456\t Accuracy 0.8729\n",
      "Epoch [34][50]\t Batch [350][429]\t Training Loss 0.1460\t Accuracy 0.8728\n",
      "Epoch [34][50]\t Batch [400][429]\t Training Loss 0.1461\t Accuracy 0.8721\n",
      "\n",
      "Epoch [34]\t Average training loss 0.1462\t Average training accuracy 0.8721\n",
      "Epoch [34]\t Average validation loss 0.1256\t Average validation accuracy 0.9117\n",
      "\n",
      "Epoch [35][50]\t Batch [0][429]\t Training Loss 0.1475\t Accuracy 0.8750\n",
      "Epoch [35][50]\t Batch [50][429]\t Training Loss 0.1428\t Accuracy 0.8745\n",
      "Epoch [35][50]\t Batch [100][429]\t Training Loss 0.1423\t Accuracy 0.8775\n",
      "Epoch [35][50]\t Batch [150][429]\t Training Loss 0.1446\t Accuracy 0.8724\n",
      "Epoch [35][50]\t Batch [200][429]\t Training Loss 0.1443\t Accuracy 0.8750\n",
      "Epoch [35][50]\t Batch [250][429]\t Training Loss 0.1439\t Accuracy 0.8758\n",
      "Epoch [35][50]\t Batch [300][429]\t Training Loss 0.1454\t Accuracy 0.8732\n",
      "Epoch [35][50]\t Batch [350][429]\t Training Loss 0.1458\t Accuracy 0.8728\n",
      "Epoch [35][50]\t Batch [400][429]\t Training Loss 0.1459\t Accuracy 0.8722\n",
      "\n",
      "Epoch [35]\t Average training loss 0.1459\t Average training accuracy 0.8722\n",
      "Epoch [35]\t Average validation loss 0.1253\t Average validation accuracy 0.9133\n",
      "\n",
      "Epoch [36][50]\t Batch [0][429]\t Training Loss 0.1294\t Accuracy 0.8984\n",
      "Epoch [36][50]\t Batch [50][429]\t Training Loss 0.1423\t Accuracy 0.8744\n",
      "Epoch [36][50]\t Batch [100][429]\t Training Loss 0.1419\t Accuracy 0.8784\n",
      "Epoch [36][50]\t Batch [150][429]\t Training Loss 0.1443\t Accuracy 0.8729\n",
      "Epoch [36][50]\t Batch [200][429]\t Training Loss 0.1438\t Accuracy 0.8758\n",
      "Epoch [36][50]\t Batch [250][429]\t Training Loss 0.1436\t Accuracy 0.8760\n",
      "Epoch [36][50]\t Batch [300][429]\t Training Loss 0.1451\t Accuracy 0.8736\n",
      "Epoch [36][50]\t Batch [350][429]\t Training Loss 0.1454\t Accuracy 0.8733\n",
      "Epoch [36][50]\t Batch [400][429]\t Training Loss 0.1457\t Accuracy 0.8727\n",
      "\n",
      "Epoch [36]\t Average training loss 0.1457\t Average training accuracy 0.8727\n",
      "Epoch [36]\t Average validation loss 0.1253\t Average validation accuracy 0.9123\n",
      "\n",
      "Epoch [37][50]\t Batch [0][429]\t Training Loss 0.1296\t Accuracy 0.9219\n",
      "Epoch [37][50]\t Batch [50][429]\t Training Loss 0.1418\t Accuracy 0.8745\n",
      "Epoch [37][50]\t Batch [100][429]\t Training Loss 0.1418\t Accuracy 0.8779\n",
      "Epoch [37][50]\t Batch [150][429]\t Training Loss 0.1440\t Accuracy 0.8733\n",
      "Epoch [37][50]\t Batch [200][429]\t Training Loss 0.1431\t Accuracy 0.8764\n",
      "Epoch [37][50]\t Batch [250][429]\t Training Loss 0.1433\t Accuracy 0.8762\n",
      "Epoch [37][50]\t Batch [300][429]\t Training Loss 0.1448\t Accuracy 0.8737\n",
      "Epoch [37][50]\t Batch [350][429]\t Training Loss 0.1451\t Accuracy 0.8732\n",
      "Epoch [37][50]\t Batch [400][429]\t Training Loss 0.1454\t Accuracy 0.8727\n",
      "\n",
      "Epoch [37]\t Average training loss 0.1454\t Average training accuracy 0.8728\n",
      "Epoch [37]\t Average validation loss 0.1250\t Average validation accuracy 0.9115\n",
      "\n",
      "Epoch [38][50]\t Batch [0][429]\t Training Loss 0.1288\t Accuracy 0.8906\n",
      "Epoch [38][50]\t Batch [50][429]\t Training Loss 0.1414\t Accuracy 0.8752\n",
      "Epoch [38][50]\t Batch [100][429]\t Training Loss 0.1414\t Accuracy 0.8782\n",
      "Epoch [38][50]\t Batch [150][429]\t Training Loss 0.1434\t Accuracy 0.8738\n",
      "Epoch [38][50]\t Batch [200][429]\t Training Loss 0.1429\t Accuracy 0.8769\n",
      "Epoch [38][50]\t Batch [250][429]\t Training Loss 0.1431\t Accuracy 0.8764\n",
      "Epoch [38][50]\t Batch [300][429]\t Training Loss 0.1445\t Accuracy 0.8741\n",
      "Epoch [38][50]\t Batch [350][429]\t Training Loss 0.1448\t Accuracy 0.8736\n",
      "Epoch [38][50]\t Batch [400][429]\t Training Loss 0.1452\t Accuracy 0.8731\n",
      "\n",
      "Epoch [38]\t Average training loss 0.1452\t Average training accuracy 0.8732\n",
      "Epoch [38]\t Average validation loss 0.1250\t Average validation accuracy 0.9123\n",
      "\n",
      "Epoch [39][50]\t Batch [0][429]\t Training Loss 0.1145\t Accuracy 0.9219\n",
      "Epoch [39][50]\t Batch [50][429]\t Training Loss 0.1409\t Accuracy 0.8753\n",
      "Epoch [39][50]\t Batch [100][429]\t Training Loss 0.1409\t Accuracy 0.8790\n",
      "Epoch [39][50]\t Batch [150][429]\t Training Loss 0.1431\t Accuracy 0.8745\n",
      "Epoch [39][50]\t Batch [200][429]\t Training Loss 0.1426\t Accuracy 0.8774\n",
      "Epoch [39][50]\t Batch [250][429]\t Training Loss 0.1428\t Accuracy 0.8771\n",
      "Epoch [39][50]\t Batch [300][429]\t Training Loss 0.1442\t Accuracy 0.8747\n",
      "Epoch [39][50]\t Batch [350][429]\t Training Loss 0.1445\t Accuracy 0.8744\n",
      "Epoch [39][50]\t Batch [400][429]\t Training Loss 0.1449\t Accuracy 0.8738\n",
      "\n",
      "Epoch [39]\t Average training loss 0.1450\t Average training accuracy 0.8738\n",
      "Epoch [39]\t Average validation loss 0.1247\t Average validation accuracy 0.9127\n",
      "\n",
      "Epoch [40][50]\t Batch [0][429]\t Training Loss 0.1100\t Accuracy 0.9219\n",
      "Epoch [40][50]\t Batch [50][429]\t Training Loss 0.1399\t Accuracy 0.8764\n",
      "Epoch [40][50]\t Batch [100][429]\t Training Loss 0.1405\t Accuracy 0.8788\n",
      "Epoch [40][50]\t Batch [150][429]\t Training Loss 0.1426\t Accuracy 0.8748\n",
      "Epoch [40][50]\t Batch [200][429]\t Training Loss 0.1423\t Accuracy 0.8772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40][50]\t Batch [250][429]\t Training Loss 0.1424\t Accuracy 0.8771\n",
      "Epoch [40][50]\t Batch [300][429]\t Training Loss 0.1439\t Accuracy 0.8742\n",
      "Epoch [40][50]\t Batch [350][429]\t Training Loss 0.1441\t Accuracy 0.8742\n",
      "Epoch [40][50]\t Batch [400][429]\t Training Loss 0.1446\t Accuracy 0.8736\n",
      "\n",
      "Epoch [40]\t Average training loss 0.1447\t Average training accuracy 0.8735\n",
      "Epoch [40]\t Average validation loss 0.1245\t Average validation accuracy 0.9129\n",
      "\n",
      "Epoch [41][50]\t Batch [0][429]\t Training Loss 0.1105\t Accuracy 0.9297\n",
      "Epoch [41][50]\t Batch [50][429]\t Training Loss 0.1395\t Accuracy 0.8781\n",
      "Epoch [41][50]\t Batch [100][429]\t Training Loss 0.1401\t Accuracy 0.8797\n",
      "Epoch [41][50]\t Batch [150][429]\t Training Loss 0.1419\t Accuracy 0.8760\n",
      "Epoch [41][50]\t Batch [200][429]\t Training Loss 0.1418\t Accuracy 0.8781\n",
      "Epoch [41][50]\t Batch [250][429]\t Training Loss 0.1421\t Accuracy 0.8779\n",
      "Epoch [41][50]\t Batch [300][429]\t Training Loss 0.1437\t Accuracy 0.8749\n",
      "Epoch [41][50]\t Batch [350][429]\t Training Loss 0.1437\t Accuracy 0.8749\n",
      "Epoch [41][50]\t Batch [400][429]\t Training Loss 0.1444\t Accuracy 0.8741\n",
      "\n",
      "Epoch [41]\t Average training loss 0.1445\t Average training accuracy 0.8739\n",
      "Epoch [41]\t Average validation loss 0.1241\t Average validation accuracy 0.9115\n",
      "\n",
      "Epoch [42][50]\t Batch [0][429]\t Training Loss 0.1173\t Accuracy 0.9141\n",
      "Epoch [42][50]\t Batch [50][429]\t Training Loss 0.1393\t Accuracy 0.8790\n",
      "Epoch [42][50]\t Batch [100][429]\t Training Loss 0.1398\t Accuracy 0.8800\n",
      "Epoch [42][50]\t Batch [150][429]\t Training Loss 0.1417\t Accuracy 0.8761\n",
      "Epoch [42][50]\t Batch [200][429]\t Training Loss 0.1415\t Accuracy 0.8786\n",
      "Epoch [42][50]\t Batch [250][429]\t Training Loss 0.1419\t Accuracy 0.8780\n",
      "Epoch [42][50]\t Batch [300][429]\t Training Loss 0.1434\t Accuracy 0.8753\n",
      "Epoch [42][50]\t Batch [350][429]\t Training Loss 0.1434\t Accuracy 0.8754\n",
      "Epoch [42][50]\t Batch [400][429]\t Training Loss 0.1442\t Accuracy 0.8744\n",
      "\n",
      "Epoch [42]\t Average training loss 0.1442\t Average training accuracy 0.8743\n",
      "Epoch [42]\t Average validation loss 0.1236\t Average validation accuracy 0.9103\n",
      "\n",
      "Epoch [43][50]\t Batch [0][429]\t Training Loss 0.1604\t Accuracy 0.8359\n",
      "Epoch [43][50]\t Batch [50][429]\t Training Loss 0.1395\t Accuracy 0.8784\n",
      "Epoch [43][50]\t Batch [100][429]\t Training Loss 0.1399\t Accuracy 0.8794\n",
      "Epoch [43][50]\t Batch [150][429]\t Training Loss 0.1417\t Accuracy 0.8760\n",
      "Epoch [43][50]\t Batch [200][429]\t Training Loss 0.1415\t Accuracy 0.8781\n",
      "Epoch [43][50]\t Batch [250][429]\t Training Loss 0.1417\t Accuracy 0.8778\n",
      "Epoch [43][50]\t Batch [300][429]\t Training Loss 0.1432\t Accuracy 0.8751\n",
      "Epoch [43][50]\t Batch [350][429]\t Training Loss 0.1433\t Accuracy 0.8750\n",
      "Epoch [43][50]\t Batch [400][429]\t Training Loss 0.1439\t Accuracy 0.8742\n",
      "\n",
      "Epoch [43]\t Average training loss 0.1441\t Average training accuracy 0.8740\n",
      "Epoch [43]\t Average validation loss 0.1237\t Average validation accuracy 0.9113\n",
      "\n",
      "Epoch [44][50]\t Batch [0][429]\t Training Loss 0.1591\t Accuracy 0.8516\n",
      "Epoch [44][50]\t Batch [50][429]\t Training Loss 0.1395\t Accuracy 0.8793\n",
      "Epoch [44][50]\t Batch [100][429]\t Training Loss 0.1397\t Accuracy 0.8803\n",
      "Epoch [44][50]\t Batch [150][429]\t Training Loss 0.1415\t Accuracy 0.8765\n",
      "Epoch [44][50]\t Batch [200][429]\t Training Loss 0.1415\t Accuracy 0.8783\n",
      "Epoch [44][50]\t Batch [250][429]\t Training Loss 0.1416\t Accuracy 0.8781\n",
      "Epoch [44][50]\t Batch [300][429]\t Training Loss 0.1429\t Accuracy 0.8757\n",
      "Epoch [44][50]\t Batch [350][429]\t Training Loss 0.1431\t Accuracy 0.8756\n",
      "Epoch [44][50]\t Batch [400][429]\t Training Loss 0.1437\t Accuracy 0.8747\n",
      "\n",
      "Epoch [44]\t Average training loss 0.1439\t Average training accuracy 0.8745\n",
      "Epoch [44]\t Average validation loss 0.1233\t Average validation accuracy 0.9109\n",
      "\n",
      "Epoch [45][50]\t Batch [0][429]\t Training Loss 0.1376\t Accuracy 0.8750\n",
      "Epoch [45][50]\t Batch [50][429]\t Training Loss 0.1396\t Accuracy 0.8788\n",
      "Epoch [45][50]\t Batch [100][429]\t Training Loss 0.1397\t Accuracy 0.8796\n",
      "Epoch [45][50]\t Batch [150][429]\t Training Loss 0.1414\t Accuracy 0.8762\n",
      "Epoch [45][50]\t Batch [200][429]\t Training Loss 0.1414\t Accuracy 0.8781\n",
      "Epoch [45][50]\t Batch [250][429]\t Training Loss 0.1414\t Accuracy 0.8782\n",
      "Epoch [45][50]\t Batch [300][429]\t Training Loss 0.1427\t Accuracy 0.8758\n",
      "Epoch [45][50]\t Batch [350][429]\t Training Loss 0.1429\t Accuracy 0.8757\n",
      "Epoch [45][50]\t Batch [400][429]\t Training Loss 0.1435\t Accuracy 0.8749\n",
      "\n",
      "Epoch [45]\t Average training loss 0.1438\t Average training accuracy 0.8745\n",
      "Epoch [45]\t Average validation loss 0.1233\t Average validation accuracy 0.9113\n",
      "\n",
      "Epoch [46][50]\t Batch [0][429]\t Training Loss 0.1148\t Accuracy 0.9141\n",
      "Epoch [46][50]\t Batch [50][429]\t Training Loss 0.1391\t Accuracy 0.8782\n",
      "Epoch [46][50]\t Batch [100][429]\t Training Loss 0.1392\t Accuracy 0.8803\n",
      "Epoch [46][50]\t Batch [150][429]\t Training Loss 0.1412\t Accuracy 0.8763\n",
      "Epoch [46][50]\t Batch [200][429]\t Training Loss 0.1411\t Accuracy 0.8784\n",
      "Epoch [46][50]\t Batch [250][429]\t Training Loss 0.1412\t Accuracy 0.8786\n",
      "Epoch [46][50]\t Batch [300][429]\t Training Loss 0.1425\t Accuracy 0.8762\n",
      "Epoch [46][50]\t Batch [350][429]\t Training Loss 0.1427\t Accuracy 0.8759\n",
      "Epoch [46][50]\t Batch [400][429]\t Training Loss 0.1432\t Accuracy 0.8754\n",
      "\n",
      "Epoch [46]\t Average training loss 0.1436\t Average training accuracy 0.8749\n",
      "Epoch [46]\t Average validation loss 0.1232\t Average validation accuracy 0.9117\n",
      "\n",
      "Epoch [47][50]\t Batch [0][429]\t Training Loss 0.1135\t Accuracy 0.9375\n",
      "Epoch [47][50]\t Batch [50][429]\t Training Loss 0.1384\t Accuracy 0.8802\n",
      "Epoch [47][50]\t Batch [100][429]\t Training Loss 0.1386\t Accuracy 0.8817\n",
      "Epoch [47][50]\t Batch [150][429]\t Training Loss 0.1409\t Accuracy 0.8768\n",
      "Epoch [47][50]\t Batch [200][429]\t Training Loss 0.1409\t Accuracy 0.8786\n",
      "Epoch [47][50]\t Batch [250][429]\t Training Loss 0.1411\t Accuracy 0.8786\n",
      "Epoch [47][50]\t Batch [300][429]\t Training Loss 0.1422\t Accuracy 0.8764\n",
      "Epoch [47][50]\t Batch [350][429]\t Training Loss 0.1425\t Accuracy 0.8759\n",
      "Epoch [47][50]\t Batch [400][429]\t Training Loss 0.1430\t Accuracy 0.8755\n",
      "\n",
      "Epoch [47]\t Average training loss 0.1434\t Average training accuracy 0.8750\n",
      "Epoch [47]\t Average validation loss 0.1230\t Average validation accuracy 0.9125\n",
      "\n",
      "Epoch [48][50]\t Batch [0][429]\t Training Loss 0.1345\t Accuracy 0.8906\n",
      "Epoch [48][50]\t Batch [50][429]\t Training Loss 0.1384\t Accuracy 0.8802\n",
      "Epoch [48][50]\t Batch [100][429]\t Training Loss 0.1380\t Accuracy 0.8826\n",
      "Epoch [48][50]\t Batch [150][429]\t Training Loss 0.1406\t Accuracy 0.8772\n",
      "Epoch [48][50]\t Batch [200][429]\t Training Loss 0.1407\t Accuracy 0.8787\n",
      "Epoch [48][50]\t Batch [250][429]\t Training Loss 0.1410\t Accuracy 0.8786\n",
      "Epoch [48][50]\t Batch [300][429]\t Training Loss 0.1421\t Accuracy 0.8765\n",
      "Epoch [48][50]\t Batch [350][429]\t Training Loss 0.1424\t Accuracy 0.8760\n",
      "Epoch [48][50]\t Batch [400][429]\t Training Loss 0.1429\t Accuracy 0.8756\n",
      "\n",
      "Epoch [48]\t Average training loss 0.1432\t Average training accuracy 0.8751\n",
      "Epoch [48]\t Average validation loss 0.1227\t Average validation accuracy 0.9113\n",
      "\n",
      "Epoch [49][50]\t Batch [0][429]\t Training Loss 0.1493\t Accuracy 0.8594\n",
      "Epoch [49][50]\t Batch [50][429]\t Training Loss 0.1389\t Accuracy 0.8797\n",
      "Epoch [49][50]\t Batch [100][429]\t Training Loss 0.1377\t Accuracy 0.8828\n",
      "Epoch [49][50]\t Batch [150][429]\t Training Loss 0.1407\t Accuracy 0.8769\n",
      "Epoch [49][50]\t Batch [200][429]\t Training Loss 0.1406\t Accuracy 0.8788\n",
      "Epoch [49][50]\t Batch [250][429]\t Training Loss 0.1409\t Accuracy 0.8788\n",
      "Epoch [49][50]\t Batch [300][429]\t Training Loss 0.1421\t Accuracy 0.8765\n",
      "Epoch [49][50]\t Batch [350][429]\t Training Loss 0.1423\t Accuracy 0.8761\n",
      "Epoch [49][50]\t Batch [400][429]\t Training Loss 0.1428\t Accuracy 0.8757\n",
      "\n",
      "Epoch [49]\t Average training loss 0.1431\t Average training accuracy 0.8751\n",
      "Epoch [49]\t Average validation loss 0.1225\t Average validation accuracy 0.9099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:45:17.001593Z",
     "start_time": "2023-11-10T08:45:15.075334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8837.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 使用欧式距离损失和ReLU激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用欧式距离损失和ReLU激活函数.\n",
    "\n",
    "### TODO\n",
    "执行以下代码之前，请完成 **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:45:17.011938Z",
     "start_time": "2023-11-10T08:45:17.004832Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# 使用FCLayer和ReLULayer构建多层感知机\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:46:16.363018Z",
     "start_time": "2023-11-10T08:45:17.013784Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][50]\t Batch [0][429]\t Training Loss 1.0659\t Accuracy 0.1016\n",
      "Epoch [0][50]\t Batch [50][429]\t Training Loss 0.5016\t Accuracy 0.3333\n",
      "Epoch [0][50]\t Batch [100][429]\t Training Loss 0.4181\t Accuracy 0.4527\n",
      "Epoch [0][50]\t Batch [150][429]\t Training Loss 0.3771\t Accuracy 0.5096\n",
      "Epoch [0][50]\t Batch [200][429]\t Training Loss 0.3420\t Accuracy 0.5630\n",
      "Epoch [0][50]\t Batch [250][429]\t Training Loss 0.3196\t Accuracy 0.5973\n",
      "Epoch [0][50]\t Batch [300][429]\t Training Loss 0.3014\t Accuracy 0.6240\n",
      "Epoch [0][50]\t Batch [350][429]\t Training Loss 0.2863\t Accuracy 0.6473\n",
      "Epoch [0][50]\t Batch [400][429]\t Training Loss 0.2744\t Accuracy 0.6647\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2680\t Average training accuracy 0.6745\n",
      "Epoch [0]\t Average validation loss 0.1563\t Average validation accuracy 0.8528\n",
      "\n",
      "Epoch [1][50]\t Batch [0][429]\t Training Loss 0.2316\t Accuracy 0.6875\n",
      "Epoch [1][50]\t Batch [50][429]\t Training Loss 0.1684\t Accuracy 0.8300\n",
      "Epoch [1][50]\t Batch [100][429]\t Training Loss 0.1681\t Accuracy 0.8277\n",
      "Epoch [1][50]\t Batch [150][429]\t Training Loss 0.1685\t Accuracy 0.8257\n",
      "Epoch [1][50]\t Batch [200][429]\t Training Loss 0.1643\t Accuracy 0.8323\n",
      "Epoch [1][50]\t Batch [250][429]\t Training Loss 0.1638\t Accuracy 0.8329\n",
      "Epoch [1][50]\t Batch [300][429]\t Training Loss 0.1618\t Accuracy 0.8351\n",
      "Epoch [1][50]\t Batch [350][429]\t Training Loss 0.1599\t Accuracy 0.8379\n",
      "Epoch [1][50]\t Batch [400][429]\t Training Loss 0.1587\t Accuracy 0.8395\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1574\t Average training accuracy 0.8411\n",
      "Epoch [1]\t Average validation loss 0.1224\t Average validation accuracy 0.8960\n",
      "\n",
      "Epoch [2][50]\t Batch [0][429]\t Training Loss 0.1949\t Accuracy 0.7344\n",
      "Epoch [2][50]\t Batch [50][429]\t Training Loss 0.1358\t Accuracy 0.8712\n",
      "Epoch [2][50]\t Batch [100][429]\t Training Loss 0.1369\t Accuracy 0.8686\n",
      "Epoch [2][50]\t Batch [150][429]\t Training Loss 0.1380\t Accuracy 0.8664\n",
      "Epoch [2][50]\t Batch [200][429]\t Training Loss 0.1354\t Accuracy 0.8705\n",
      "Epoch [2][50]\t Batch [250][429]\t Training Loss 0.1360\t Accuracy 0.8695\n",
      "Epoch [2][50]\t Batch [300][429]\t Training Loss 0.1350\t Accuracy 0.8704\n",
      "Epoch [2][50]\t Batch [350][429]\t Training Loss 0.1342\t Accuracy 0.8715\n",
      "Epoch [2][50]\t Batch [400][429]\t Training Loss 0.1338\t Accuracy 0.8722\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1331\t Average training accuracy 0.8729\n",
      "Epoch [2]\t Average validation loss 0.1068\t Average validation accuracy 0.9141\n",
      "\n",
      "Epoch [3][50]\t Batch [0][429]\t Training Loss 0.1068\t Accuracy 0.9062\n",
      "Epoch [3][50]\t Batch [50][429]\t Training Loss 0.1190\t Accuracy 0.8929\n",
      "Epoch [3][50]\t Batch [100][429]\t Training Loss 0.1210\t Accuracy 0.8895\n",
      "Epoch [3][50]\t Batch [150][429]\t Training Loss 0.1222\t Accuracy 0.8876\n",
      "Epoch [3][50]\t Batch [200][429]\t Training Loss 0.1203\t Accuracy 0.8898\n",
      "Epoch [3][50]\t Batch [250][429]\t Training Loss 0.1210\t Accuracy 0.8885\n",
      "Epoch [3][50]\t Batch [300][429]\t Training Loss 0.1205\t Accuracy 0.8892\n",
      "Epoch [3][50]\t Batch [350][429]\t Training Loss 0.1200\t Accuracy 0.8895\n",
      "Epoch [3][50]\t Batch [400][429]\t Training Loss 0.1200\t Accuracy 0.8897\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1195\t Average training accuracy 0.8902\n",
      "Epoch [3]\t Average validation loss 0.0971\t Average validation accuracy 0.9233\n",
      "\n",
      "Epoch [4][50]\t Batch [0][429]\t Training Loss 0.0776\t Accuracy 0.9453\n",
      "Epoch [4][50]\t Batch [50][429]\t Training Loss 0.1083\t Accuracy 0.9053\n",
      "Epoch [4][50]\t Batch [100][429]\t Training Loss 0.1102\t Accuracy 0.9028\n",
      "Epoch [4][50]\t Batch [150][429]\t Training Loss 0.1118\t Accuracy 0.9012\n",
      "Epoch [4][50]\t Batch [200][429]\t Training Loss 0.1103\t Accuracy 0.9024\n",
      "Epoch [4][50]\t Batch [250][429]\t Training Loss 0.1110\t Accuracy 0.9012\n",
      "Epoch [4][50]\t Batch [300][429]\t Training Loss 0.1107\t Accuracy 0.9012\n",
      "Epoch [4][50]\t Batch [350][429]\t Training Loss 0.1105\t Accuracy 0.9014\n",
      "Epoch [4][50]\t Batch [400][429]\t Training Loss 0.1107\t Accuracy 0.9013\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1103\t Average training accuracy 0.9019\n",
      "Epoch [4]\t Average validation loss 0.0904\t Average validation accuracy 0.9299\n",
      "\n",
      "Epoch [5][50]\t Batch [0][429]\t Training Loss 0.0845\t Accuracy 0.9297\n",
      "Epoch [5][50]\t Batch [50][429]\t Training Loss 0.1004\t Accuracy 0.9157\n",
      "Epoch [5][50]\t Batch [100][429]\t Training Loss 0.1024\t Accuracy 0.9120\n",
      "Epoch [5][50]\t Batch [150][429]\t Training Loss 0.1043\t Accuracy 0.9101\n",
      "Epoch [5][50]\t Batch [200][429]\t Training Loss 0.1029\t Accuracy 0.9113\n",
      "Epoch [5][50]\t Batch [250][429]\t Training Loss 0.1036\t Accuracy 0.9099\n",
      "Epoch [5][50]\t Batch [300][429]\t Training Loss 0.1036\t Accuracy 0.9093\n",
      "Epoch [5][50]\t Batch [350][429]\t Training Loss 0.1035\t Accuracy 0.9097\n",
      "Epoch [5][50]\t Batch [400][429]\t Training Loss 0.1038\t Accuracy 0.9093\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1034\t Average training accuracy 0.9099\n",
      "Epoch [5]\t Average validation loss 0.0855\t Average validation accuracy 0.9349\n",
      "\n",
      "Epoch [6][50]\t Batch [0][429]\t Training Loss 0.1003\t Accuracy 0.9062\n",
      "Epoch [6][50]\t Batch [50][429]\t Training Loss 0.0949\t Accuracy 0.9200\n",
      "Epoch [6][50]\t Batch [100][429]\t Training Loss 0.0965\t Accuracy 0.9181\n",
      "Epoch [6][50]\t Batch [150][429]\t Training Loss 0.0985\t Accuracy 0.9159\n",
      "Epoch [6][50]\t Batch [200][429]\t Training Loss 0.0972\t Accuracy 0.9174\n",
      "Epoch [6][50]\t Batch [250][429]\t Training Loss 0.0979\t Accuracy 0.9160\n",
      "Epoch [6][50]\t Batch [300][429]\t Training Loss 0.0980\t Accuracy 0.9156\n",
      "Epoch [6][50]\t Batch [350][429]\t Training Loss 0.0980\t Accuracy 0.9159\n",
      "Epoch [6][50]\t Batch [400][429]\t Training Loss 0.0983\t Accuracy 0.9155\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0980\t Average training accuracy 0.9159\n",
      "Epoch [6]\t Average validation loss 0.0813\t Average validation accuracy 0.9387\n",
      "\n",
      "Epoch [7][50]\t Batch [0][429]\t Training Loss 0.0883\t Accuracy 0.9453\n",
      "Epoch [7][50]\t Batch [50][429]\t Training Loss 0.0900\t Accuracy 0.9251\n",
      "Epoch [7][50]\t Batch [100][429]\t Training Loss 0.0917\t Accuracy 0.9232\n",
      "Epoch [7][50]\t Batch [150][429]\t Training Loss 0.0939\t Accuracy 0.9206\n",
      "Epoch [7][50]\t Batch [200][429]\t Training Loss 0.0926\t Accuracy 0.9224\n",
      "Epoch [7][50]\t Batch [250][429]\t Training Loss 0.0933\t Accuracy 0.9211\n",
      "Epoch [7][50]\t Batch [300][429]\t Training Loss 0.0935\t Accuracy 0.9205\n",
      "Epoch [7][50]\t Batch [350][429]\t Training Loss 0.0935\t Accuracy 0.9206\n",
      "Epoch [7][50]\t Batch [400][429]\t Training Loss 0.0939\t Accuracy 0.9203\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0936\t Average training accuracy 0.9207\n",
      "Epoch [7]\t Average validation loss 0.0780\t Average validation accuracy 0.9413\n",
      "\n",
      "Epoch [8][50]\t Batch [0][429]\t Training Loss 0.0777\t Accuracy 0.9531\n",
      "Epoch [8][50]\t Batch [50][429]\t Training Loss 0.0858\t Accuracy 0.9303\n",
      "Epoch [8][50]\t Batch [100][429]\t Training Loss 0.0877\t Accuracy 0.9277\n",
      "Epoch [8][50]\t Batch [150][429]\t Training Loss 0.0900\t Accuracy 0.9250\n",
      "Epoch [8][50]\t Batch [200][429]\t Training Loss 0.0888\t Accuracy 0.9267\n",
      "Epoch [8][50]\t Batch [250][429]\t Training Loss 0.0895\t Accuracy 0.9251\n",
      "Epoch [8][50]\t Batch [300][429]\t Training Loss 0.0897\t Accuracy 0.9245\n",
      "Epoch [8][50]\t Batch [350][429]\t Training Loss 0.0897\t Accuracy 0.9249\n",
      "Epoch [8][50]\t Batch [400][429]\t Training Loss 0.0902\t Accuracy 0.9243\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0899\t Average training accuracy 0.9248\n",
      "Epoch [8]\t Average validation loss 0.0751\t Average validation accuracy 0.9461\n",
      "\n",
      "Epoch [9][50]\t Batch [0][429]\t Training Loss 0.0707\t Accuracy 0.9609\n",
      "Epoch [9][50]\t Batch [50][429]\t Training Loss 0.0822\t Accuracy 0.9343\n",
      "Epoch [9][50]\t Batch [100][429]\t Training Loss 0.0842\t Accuracy 0.9322\n",
      "Epoch [9][50]\t Batch [150][429]\t Training Loss 0.0867\t Accuracy 0.9290\n",
      "Epoch [9][50]\t Batch [200][429]\t Training Loss 0.0855\t Accuracy 0.9309\n",
      "Epoch [9][50]\t Batch [250][429]\t Training Loss 0.0861\t Accuracy 0.9296\n",
      "Epoch [9][50]\t Batch [300][429]\t Training Loss 0.0864\t Accuracy 0.9286\n",
      "Epoch [9][50]\t Batch [350][429]\t Training Loss 0.0865\t Accuracy 0.9286\n",
      "Epoch [9][50]\t Batch [400][429]\t Training Loss 0.0870\t Accuracy 0.9281\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0868\t Average training accuracy 0.9284\n",
      "Epoch [9]\t Average validation loss 0.0729\t Average validation accuracy 0.9479\n",
      "\n",
      "Epoch [10][50]\t Batch [0][429]\t Training Loss 0.0654\t Accuracy 0.9688\n",
      "Epoch [10][50]\t Batch [50][429]\t Training Loss 0.0795\t Accuracy 0.9377\n",
      "Epoch [10][50]\t Batch [100][429]\t Training Loss 0.0814\t Accuracy 0.9353\n",
      "Epoch [10][50]\t Batch [150][429]\t Training Loss 0.0838\t Accuracy 0.9323\n",
      "Epoch [10][50]\t Batch [200][429]\t Training Loss 0.0827\t Accuracy 0.9337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10][50]\t Batch [250][429]\t Training Loss 0.0832\t Accuracy 0.9328\n",
      "Epoch [10][50]\t Batch [300][429]\t Training Loss 0.0837\t Accuracy 0.9320\n",
      "Epoch [10][50]\t Batch [350][429]\t Training Loss 0.0837\t Accuracy 0.9320\n",
      "Epoch [10][50]\t Batch [400][429]\t Training Loss 0.0843\t Accuracy 0.9311\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0841\t Average training accuracy 0.9315\n",
      "Epoch [10]\t Average validation loss 0.0707\t Average validation accuracy 0.9501\n",
      "\n",
      "Epoch [11][50]\t Batch [0][429]\t Training Loss 0.0652\t Accuracy 0.9609\n",
      "Epoch [11][50]\t Batch [50][429]\t Training Loss 0.0771\t Accuracy 0.9398\n",
      "Epoch [11][50]\t Batch [100][429]\t Training Loss 0.0790\t Accuracy 0.9380\n",
      "Epoch [11][50]\t Batch [150][429]\t Training Loss 0.0814\t Accuracy 0.9349\n",
      "Epoch [11][50]\t Batch [200][429]\t Training Loss 0.0803\t Accuracy 0.9359\n",
      "Epoch [11][50]\t Batch [250][429]\t Training Loss 0.0808\t Accuracy 0.9351\n",
      "Epoch [11][50]\t Batch [300][429]\t Training Loss 0.0813\t Accuracy 0.9343\n",
      "Epoch [11][50]\t Batch [350][429]\t Training Loss 0.0814\t Accuracy 0.9343\n",
      "Epoch [11][50]\t Batch [400][429]\t Training Loss 0.0819\t Accuracy 0.9334\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0817\t Average training accuracy 0.9337\n",
      "Epoch [11]\t Average validation loss 0.0691\t Average validation accuracy 0.9517\n",
      "\n",
      "Epoch [12][50]\t Batch [0][429]\t Training Loss 0.1029\t Accuracy 0.9062\n",
      "Epoch [12][50]\t Batch [50][429]\t Training Loss 0.0755\t Accuracy 0.9418\n",
      "Epoch [12][50]\t Batch [100][429]\t Training Loss 0.0771\t Accuracy 0.9401\n",
      "Epoch [12][50]\t Batch [150][429]\t Training Loss 0.0794\t Accuracy 0.9367\n",
      "Epoch [12][50]\t Batch [200][429]\t Training Loss 0.0784\t Accuracy 0.9378\n",
      "Epoch [12][50]\t Batch [250][429]\t Training Loss 0.0788\t Accuracy 0.9372\n",
      "Epoch [12][50]\t Batch [300][429]\t Training Loss 0.0792\t Accuracy 0.9365\n",
      "Epoch [12][50]\t Batch [350][429]\t Training Loss 0.0793\t Accuracy 0.9363\n",
      "Epoch [12][50]\t Batch [400][429]\t Training Loss 0.0798\t Accuracy 0.9357\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0797\t Average training accuracy 0.9359\n",
      "Epoch [12]\t Average validation loss 0.0675\t Average validation accuracy 0.9529\n",
      "\n",
      "Epoch [13][50]\t Batch [0][429]\t Training Loss 0.0903\t Accuracy 0.9219\n",
      "Epoch [13][50]\t Batch [50][429]\t Training Loss 0.0739\t Accuracy 0.9436\n",
      "Epoch [13][50]\t Batch [100][429]\t Training Loss 0.0753\t Accuracy 0.9421\n",
      "Epoch [13][50]\t Batch [150][429]\t Training Loss 0.0777\t Accuracy 0.9384\n",
      "Epoch [13][50]\t Batch [200][429]\t Training Loss 0.0765\t Accuracy 0.9396\n",
      "Epoch [13][50]\t Batch [250][429]\t Training Loss 0.0770\t Accuracy 0.9388\n",
      "Epoch [13][50]\t Batch [300][429]\t Training Loss 0.0773\t Accuracy 0.9385\n",
      "Epoch [13][50]\t Batch [350][429]\t Training Loss 0.0775\t Accuracy 0.9384\n",
      "Epoch [13][50]\t Batch [400][429]\t Training Loss 0.0780\t Accuracy 0.9379\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0779\t Average training accuracy 0.9380\n",
      "Epoch [13]\t Average validation loss 0.0661\t Average validation accuracy 0.9543\n",
      "\n",
      "Epoch [14][50]\t Batch [0][429]\t Training Loss 0.0681\t Accuracy 0.9453\n",
      "Epoch [14][50]\t Batch [50][429]\t Training Loss 0.0719\t Accuracy 0.9455\n",
      "Epoch [14][50]\t Batch [100][429]\t Training Loss 0.0734\t Accuracy 0.9438\n",
      "Epoch [14][50]\t Batch [150][429]\t Training Loss 0.0761\t Accuracy 0.9399\n",
      "Epoch [14][50]\t Batch [200][429]\t Training Loss 0.0748\t Accuracy 0.9413\n",
      "Epoch [14][50]\t Batch [250][429]\t Training Loss 0.0753\t Accuracy 0.9404\n",
      "Epoch [14][50]\t Batch [300][429]\t Training Loss 0.0756\t Accuracy 0.9402\n",
      "Epoch [14][50]\t Batch [350][429]\t Training Loss 0.0759\t Accuracy 0.9401\n",
      "Epoch [14][50]\t Batch [400][429]\t Training Loss 0.0764\t Accuracy 0.9395\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0763\t Average training accuracy 0.9397\n",
      "Epoch [14]\t Average validation loss 0.0648\t Average validation accuracy 0.9569\n",
      "\n",
      "Epoch [15][50]\t Batch [0][429]\t Training Loss 0.0709\t Accuracy 0.9375\n",
      "Epoch [15][50]\t Batch [50][429]\t Training Loss 0.0708\t Accuracy 0.9462\n",
      "Epoch [15][50]\t Batch [100][429]\t Training Loss 0.0719\t Accuracy 0.9451\n",
      "Epoch [15][50]\t Batch [150][429]\t Training Loss 0.0747\t Accuracy 0.9413\n",
      "Epoch [15][50]\t Batch [200][429]\t Training Loss 0.0734\t Accuracy 0.9430\n",
      "Epoch [15][50]\t Batch [250][429]\t Training Loss 0.0738\t Accuracy 0.9423\n",
      "Epoch [15][50]\t Batch [300][429]\t Training Loss 0.0741\t Accuracy 0.9419\n",
      "Epoch [15][50]\t Batch [350][429]\t Training Loss 0.0745\t Accuracy 0.9418\n",
      "Epoch [15][50]\t Batch [400][429]\t Training Loss 0.0749\t Accuracy 0.9410\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0749\t Average training accuracy 0.9413\n",
      "Epoch [15]\t Average validation loss 0.0636\t Average validation accuracy 0.9571\n",
      "\n",
      "Epoch [16][50]\t Batch [0][429]\t Training Loss 0.0685\t Accuracy 0.9297\n",
      "Epoch [16][50]\t Batch [50][429]\t Training Loss 0.0691\t Accuracy 0.9475\n",
      "Epoch [16][50]\t Batch [100][429]\t Training Loss 0.0703\t Accuracy 0.9468\n",
      "Epoch [16][50]\t Batch [150][429]\t Training Loss 0.0733\t Accuracy 0.9424\n",
      "Epoch [16][50]\t Batch [200][429]\t Training Loss 0.0722\t Accuracy 0.9438\n",
      "Epoch [16][50]\t Batch [250][429]\t Training Loss 0.0724\t Accuracy 0.9433\n",
      "Epoch [16][50]\t Batch [300][429]\t Training Loss 0.0728\t Accuracy 0.9431\n",
      "Epoch [16][50]\t Batch [350][429]\t Training Loss 0.0732\t Accuracy 0.9429\n",
      "Epoch [16][50]\t Batch [400][429]\t Training Loss 0.0736\t Accuracy 0.9422\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0736\t Average training accuracy 0.9426\n",
      "Epoch [16]\t Average validation loss 0.0625\t Average validation accuracy 0.9585\n",
      "\n",
      "Epoch [17][50]\t Batch [0][429]\t Training Loss 0.0819\t Accuracy 0.9297\n",
      "Epoch [17][50]\t Batch [50][429]\t Training Loss 0.0684\t Accuracy 0.9485\n",
      "Epoch [17][50]\t Batch [100][429]\t Training Loss 0.0694\t Accuracy 0.9478\n",
      "Epoch [17][50]\t Batch [150][429]\t Training Loss 0.0723\t Accuracy 0.9438\n",
      "Epoch [17][50]\t Batch [200][429]\t Training Loss 0.0710\t Accuracy 0.9452\n",
      "Epoch [17][50]\t Batch [250][429]\t Training Loss 0.0713\t Accuracy 0.9446\n",
      "Epoch [17][50]\t Batch [300][429]\t Training Loss 0.0716\t Accuracy 0.9445\n",
      "Epoch [17][50]\t Batch [350][429]\t Training Loss 0.0721\t Accuracy 0.9440\n",
      "Epoch [17][50]\t Batch [400][429]\t Training Loss 0.0725\t Accuracy 0.9434\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0725\t Average training accuracy 0.9437\n",
      "Epoch [17]\t Average validation loss 0.0616\t Average validation accuracy 0.9593\n",
      "\n",
      "Epoch [18][50]\t Batch [0][429]\t Training Loss 0.0637\t Accuracy 0.9609\n",
      "Epoch [18][50]\t Batch [50][429]\t Training Loss 0.0673\t Accuracy 0.9494\n",
      "Epoch [18][50]\t Batch [100][429]\t Training Loss 0.0683\t Accuracy 0.9486\n",
      "Epoch [18][50]\t Batch [150][429]\t Training Loss 0.0710\t Accuracy 0.9448\n",
      "Epoch [18][50]\t Batch [200][429]\t Training Loss 0.0699\t Accuracy 0.9461\n",
      "Epoch [18][50]\t Batch [250][429]\t Training Loss 0.0701\t Accuracy 0.9457\n",
      "Epoch [18][50]\t Batch [300][429]\t Training Loss 0.0706\t Accuracy 0.9454\n",
      "Epoch [18][50]\t Batch [350][429]\t Training Loss 0.0710\t Accuracy 0.9450\n",
      "Epoch [18][50]\t Batch [400][429]\t Training Loss 0.0714\t Accuracy 0.9444\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0714\t Average training accuracy 0.9446\n",
      "Epoch [18]\t Average validation loss 0.0607\t Average validation accuracy 0.9599\n",
      "\n",
      "Epoch [19][50]\t Batch [0][429]\t Training Loss 0.0386\t Accuracy 0.9844\n",
      "Epoch [19][50]\t Batch [50][429]\t Training Loss 0.0662\t Accuracy 0.9514\n",
      "Epoch [19][50]\t Batch [100][429]\t Training Loss 0.0673\t Accuracy 0.9501\n",
      "Epoch [19][50]\t Batch [150][429]\t Training Loss 0.0697\t Accuracy 0.9466\n",
      "Epoch [19][50]\t Batch [200][429]\t Training Loss 0.0689\t Accuracy 0.9474\n",
      "Epoch [19][50]\t Batch [250][429]\t Training Loss 0.0691\t Accuracy 0.9470\n",
      "Epoch [19][50]\t Batch [300][429]\t Training Loss 0.0696\t Accuracy 0.9467\n",
      "Epoch [19][50]\t Batch [350][429]\t Training Loss 0.0701\t Accuracy 0.9462\n",
      "Epoch [19][50]\t Batch [400][429]\t Training Loss 0.0703\t Accuracy 0.9457\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0704\t Average training accuracy 0.9458\n",
      "Epoch [19]\t Average validation loss 0.0600\t Average validation accuracy 0.9609\n",
      "\n",
      "Epoch [20][50]\t Batch [0][429]\t Training Loss 0.0473\t Accuracy 0.9844\n",
      "Epoch [20][50]\t Batch [50][429]\t Training Loss 0.0650\t Accuracy 0.9530\n",
      "Epoch [20][50]\t Batch [100][429]\t Training Loss 0.0663\t Accuracy 0.9511\n",
      "Epoch [20][50]\t Batch [150][429]\t Training Loss 0.0687\t Accuracy 0.9475\n",
      "Epoch [20][50]\t Batch [200][429]\t Training Loss 0.0680\t Accuracy 0.9483\n",
      "Epoch [20][50]\t Batch [250][429]\t Training Loss 0.0682\t Accuracy 0.9477\n",
      "Epoch [20][50]\t Batch [300][429]\t Training Loss 0.0687\t Accuracy 0.9476\n",
      "Epoch [20][50]\t Batch [350][429]\t Training Loss 0.0691\t Accuracy 0.9473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20][50]\t Batch [400][429]\t Training Loss 0.0694\t Accuracy 0.9467\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0696\t Average training accuracy 0.9467\n",
      "Epoch [20]\t Average validation loss 0.0593\t Average validation accuracy 0.9621\n",
      "\n",
      "Epoch [21][50]\t Batch [0][429]\t Training Loss 0.0611\t Accuracy 0.9688\n",
      "Epoch [21][50]\t Batch [50][429]\t Training Loss 0.0642\t Accuracy 0.9539\n",
      "Epoch [21][50]\t Batch [100][429]\t Training Loss 0.0656\t Accuracy 0.9520\n",
      "Epoch [21][50]\t Batch [150][429]\t Training Loss 0.0677\t Accuracy 0.9486\n",
      "Epoch [21][50]\t Batch [200][429]\t Training Loss 0.0672\t Accuracy 0.9492\n",
      "Epoch [21][50]\t Batch [250][429]\t Training Loss 0.0674\t Accuracy 0.9486\n",
      "Epoch [21][50]\t Batch [300][429]\t Training Loss 0.0678\t Accuracy 0.9484\n",
      "Epoch [21][50]\t Batch [350][429]\t Training Loss 0.0683\t Accuracy 0.9482\n",
      "Epoch [21][50]\t Batch [400][429]\t Training Loss 0.0685\t Accuracy 0.9477\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0688\t Average training accuracy 0.9475\n",
      "Epoch [21]\t Average validation loss 0.0586\t Average validation accuracy 0.9627\n",
      "\n",
      "Epoch [22][50]\t Batch [0][429]\t Training Loss 0.0620\t Accuracy 0.9688\n",
      "Epoch [22][50]\t Batch [50][429]\t Training Loss 0.0632\t Accuracy 0.9551\n",
      "Epoch [22][50]\t Batch [100][429]\t Training Loss 0.0648\t Accuracy 0.9530\n",
      "Epoch [22][50]\t Batch [150][429]\t Training Loss 0.0668\t Accuracy 0.9498\n",
      "Epoch [22][50]\t Batch [200][429]\t Training Loss 0.0663\t Accuracy 0.9502\n",
      "Epoch [22][50]\t Batch [250][429]\t Training Loss 0.0667\t Accuracy 0.9495\n",
      "Epoch [22][50]\t Batch [300][429]\t Training Loss 0.0671\t Accuracy 0.9491\n",
      "Epoch [22][50]\t Batch [350][429]\t Training Loss 0.0675\t Accuracy 0.9489\n",
      "Epoch [22][50]\t Batch [400][429]\t Training Loss 0.0677\t Accuracy 0.9485\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0680\t Average training accuracy 0.9483\n",
      "Epoch [22]\t Average validation loss 0.0580\t Average validation accuracy 0.9633\n",
      "\n",
      "Epoch [23][50]\t Batch [0][429]\t Training Loss 0.0707\t Accuracy 0.9531\n",
      "Epoch [23][50]\t Batch [50][429]\t Training Loss 0.0631\t Accuracy 0.9551\n",
      "Epoch [23][50]\t Batch [100][429]\t Training Loss 0.0643\t Accuracy 0.9532\n",
      "Epoch [23][50]\t Batch [150][429]\t Training Loss 0.0663\t Accuracy 0.9501\n",
      "Epoch [23][50]\t Batch [200][429]\t Training Loss 0.0656\t Accuracy 0.9508\n",
      "Epoch [23][50]\t Batch [250][429]\t Training Loss 0.0660\t Accuracy 0.9502\n",
      "Epoch [23][50]\t Batch [300][429]\t Training Loss 0.0664\t Accuracy 0.9498\n",
      "Epoch [23][50]\t Batch [350][429]\t Training Loss 0.0668\t Accuracy 0.9498\n",
      "Epoch [23][50]\t Batch [400][429]\t Training Loss 0.0671\t Accuracy 0.9492\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0672\t Average training accuracy 0.9493\n",
      "Epoch [23]\t Average validation loss 0.0574\t Average validation accuracy 0.9637\n",
      "\n",
      "Epoch [24][50]\t Batch [0][429]\t Training Loss 0.1091\t Accuracy 0.8984\n",
      "Epoch [24][50]\t Batch [50][429]\t Training Loss 0.0633\t Accuracy 0.9548\n",
      "Epoch [24][50]\t Batch [100][429]\t Training Loss 0.0642\t Accuracy 0.9536\n",
      "Epoch [24][50]\t Batch [150][429]\t Training Loss 0.0659\t Accuracy 0.9508\n",
      "Epoch [24][50]\t Batch [200][429]\t Training Loss 0.0652\t Accuracy 0.9515\n",
      "Epoch [24][50]\t Batch [250][429]\t Training Loss 0.0654\t Accuracy 0.9509\n",
      "Epoch [24][50]\t Batch [300][429]\t Training Loss 0.0659\t Accuracy 0.9505\n",
      "Epoch [24][50]\t Batch [350][429]\t Training Loss 0.0662\t Accuracy 0.9504\n",
      "Epoch [24][50]\t Batch [400][429]\t Training Loss 0.0665\t Accuracy 0.9499\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0666\t Average training accuracy 0.9500\n",
      "Epoch [24]\t Average validation loss 0.0569\t Average validation accuracy 0.9645\n",
      "\n",
      "Epoch [25][50]\t Batch [0][429]\t Training Loss 0.0912\t Accuracy 0.9141\n",
      "Epoch [25][50]\t Batch [50][429]\t Training Loss 0.0631\t Accuracy 0.9545\n",
      "Epoch [25][50]\t Batch [100][429]\t Training Loss 0.0636\t Accuracy 0.9537\n",
      "Epoch [25][50]\t Batch [150][429]\t Training Loss 0.0653\t Accuracy 0.9511\n",
      "Epoch [25][50]\t Batch [200][429]\t Training Loss 0.0647\t Accuracy 0.9518\n",
      "Epoch [25][50]\t Batch [250][429]\t Training Loss 0.0648\t Accuracy 0.9515\n",
      "Epoch [25][50]\t Batch [300][429]\t Training Loss 0.0653\t Accuracy 0.9512\n",
      "Epoch [25][50]\t Batch [350][429]\t Training Loss 0.0657\t Accuracy 0.9510\n",
      "Epoch [25][50]\t Batch [400][429]\t Training Loss 0.0660\t Accuracy 0.9505\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0661\t Average training accuracy 0.9505\n",
      "Epoch [25]\t Average validation loss 0.0564\t Average validation accuracy 0.9645\n",
      "\n",
      "Epoch [26][50]\t Batch [0][429]\t Training Loss 0.0777\t Accuracy 0.9453\n",
      "Epoch [26][50]\t Batch [50][429]\t Training Loss 0.0625\t Accuracy 0.9551\n",
      "Epoch [26][50]\t Batch [100][429]\t Training Loss 0.0631\t Accuracy 0.9544\n",
      "Epoch [26][50]\t Batch [150][429]\t Training Loss 0.0647\t Accuracy 0.9520\n",
      "Epoch [26][50]\t Batch [200][429]\t Training Loss 0.0641\t Accuracy 0.9525\n",
      "Epoch [26][50]\t Batch [250][429]\t Training Loss 0.0642\t Accuracy 0.9522\n",
      "Epoch [26][50]\t Batch [300][429]\t Training Loss 0.0648\t Accuracy 0.9517\n",
      "Epoch [26][50]\t Batch [350][429]\t Training Loss 0.0651\t Accuracy 0.9516\n",
      "Epoch [26][50]\t Batch [400][429]\t Training Loss 0.0654\t Accuracy 0.9510\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0655\t Average training accuracy 0.9510\n",
      "Epoch [26]\t Average validation loss 0.0560\t Average validation accuracy 0.9649\n",
      "\n",
      "Epoch [27][50]\t Batch [0][429]\t Training Loss 0.0864\t Accuracy 0.9297\n",
      "Epoch [27][50]\t Batch [50][429]\t Training Loss 0.0622\t Accuracy 0.9554\n",
      "Epoch [27][50]\t Batch [100][429]\t Training Loss 0.0629\t Accuracy 0.9542\n",
      "Epoch [27][50]\t Batch [150][429]\t Training Loss 0.0642\t Accuracy 0.9524\n",
      "Epoch [27][50]\t Batch [200][429]\t Training Loss 0.0637\t Accuracy 0.9528\n",
      "Epoch [27][50]\t Batch [250][429]\t Training Loss 0.0637\t Accuracy 0.9527\n",
      "Epoch [27][50]\t Batch [300][429]\t Training Loss 0.0644\t Accuracy 0.9520\n",
      "Epoch [27][50]\t Batch [350][429]\t Training Loss 0.0647\t Accuracy 0.9519\n",
      "Epoch [27][50]\t Batch [400][429]\t Training Loss 0.0648\t Accuracy 0.9515\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0650\t Average training accuracy 0.9513\n",
      "Epoch [27]\t Average validation loss 0.0555\t Average validation accuracy 0.9653\n",
      "\n",
      "Epoch [28][50]\t Batch [0][429]\t Training Loss 0.0509\t Accuracy 0.9609\n",
      "Epoch [28][50]\t Batch [50][429]\t Training Loss 0.0616\t Accuracy 0.9565\n",
      "Epoch [28][50]\t Batch [100][429]\t Training Loss 0.0625\t Accuracy 0.9548\n",
      "Epoch [28][50]\t Batch [150][429]\t Training Loss 0.0636\t Accuracy 0.9534\n",
      "Epoch [28][50]\t Batch [200][429]\t Training Loss 0.0632\t Accuracy 0.9532\n",
      "Epoch [28][50]\t Batch [250][429]\t Training Loss 0.0631\t Accuracy 0.9532\n",
      "Epoch [28][50]\t Batch [300][429]\t Training Loss 0.0638\t Accuracy 0.9526\n",
      "Epoch [28][50]\t Batch [350][429]\t Training Loss 0.0641\t Accuracy 0.9525\n",
      "Epoch [28][50]\t Batch [400][429]\t Training Loss 0.0643\t Accuracy 0.9521\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0646\t Average training accuracy 0.9517\n",
      "Epoch [28]\t Average validation loss 0.0552\t Average validation accuracy 0.9651\n",
      "\n",
      "Epoch [29][50]\t Batch [0][429]\t Training Loss 0.0432\t Accuracy 0.9844\n",
      "Epoch [29][50]\t Batch [50][429]\t Training Loss 0.0607\t Accuracy 0.9573\n",
      "Epoch [29][50]\t Batch [100][429]\t Training Loss 0.0620\t Accuracy 0.9552\n",
      "Epoch [29][50]\t Batch [150][429]\t Training Loss 0.0631\t Accuracy 0.9536\n",
      "Epoch [29][50]\t Batch [200][429]\t Training Loss 0.0627\t Accuracy 0.9537\n",
      "Epoch [29][50]\t Batch [250][429]\t Training Loss 0.0626\t Accuracy 0.9536\n",
      "Epoch [29][50]\t Batch [300][429]\t Training Loss 0.0632\t Accuracy 0.9531\n",
      "Epoch [29][50]\t Batch [350][429]\t Training Loss 0.0636\t Accuracy 0.9530\n",
      "Epoch [29][50]\t Batch [400][429]\t Training Loss 0.0638\t Accuracy 0.9526\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0641\t Average training accuracy 0.9523\n",
      "Epoch [29]\t Average validation loss 0.0548\t Average validation accuracy 0.9655\n",
      "\n",
      "Epoch [30][50]\t Batch [0][429]\t Training Loss 0.0490\t Accuracy 0.9531\n",
      "Epoch [30][50]\t Batch [50][429]\t Training Loss 0.0603\t Accuracy 0.9573\n",
      "Epoch [30][50]\t Batch [100][429]\t Training Loss 0.0616\t Accuracy 0.9554\n",
      "Epoch [30][50]\t Batch [150][429]\t Training Loss 0.0625\t Accuracy 0.9540\n",
      "Epoch [30][50]\t Batch [200][429]\t Training Loss 0.0622\t Accuracy 0.9541\n",
      "Epoch [30][50]\t Batch [250][429]\t Training Loss 0.0622\t Accuracy 0.9541\n",
      "Epoch [30][50]\t Batch [300][429]\t Training Loss 0.0628\t Accuracy 0.9535\n",
      "Epoch [30][50]\t Batch [350][429]\t Training Loss 0.0630\t Accuracy 0.9535\n",
      "Epoch [30][50]\t Batch [400][429]\t Training Loss 0.0633\t Accuracy 0.9530\n",
      "\n",
      "Epoch [30]\t Average training loss 0.0636\t Average training accuracy 0.9528\n",
      "Epoch [30]\t Average validation loss 0.0544\t Average validation accuracy 0.9661\n",
      "\n",
      "Epoch [31][50]\t Batch [0][429]\t Training Loss 0.0649\t Accuracy 0.9141\n",
      "Epoch [31][50]\t Batch [50][429]\t Training Loss 0.0603\t Accuracy 0.9573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31][50]\t Batch [100][429]\t Training Loss 0.0610\t Accuracy 0.9562\n",
      "Epoch [31][50]\t Batch [150][429]\t Training Loss 0.0622\t Accuracy 0.9544\n",
      "Epoch [31][50]\t Batch [200][429]\t Training Loss 0.0618\t Accuracy 0.9542\n",
      "Epoch [31][50]\t Batch [250][429]\t Training Loss 0.0619\t Accuracy 0.9542\n",
      "Epoch [31][50]\t Batch [300][429]\t Training Loss 0.0624\t Accuracy 0.9537\n",
      "Epoch [31][50]\t Batch [350][429]\t Training Loss 0.0626\t Accuracy 0.9538\n",
      "Epoch [31][50]\t Batch [400][429]\t Training Loss 0.0630\t Accuracy 0.9533\n",
      "\n",
      "Epoch [31]\t Average training loss 0.0632\t Average training accuracy 0.9532\n",
      "Epoch [31]\t Average validation loss 0.0539\t Average validation accuracy 0.9669\n",
      "\n",
      "Epoch [32][50]\t Batch [0][429]\t Training Loss 0.0739\t Accuracy 0.9375\n",
      "Epoch [32][50]\t Batch [50][429]\t Training Loss 0.0600\t Accuracy 0.9573\n",
      "Epoch [32][50]\t Batch [100][429]\t Training Loss 0.0607\t Accuracy 0.9565\n",
      "Epoch [32][50]\t Batch [150][429]\t Training Loss 0.0618\t Accuracy 0.9549\n",
      "Epoch [32][50]\t Batch [200][429]\t Training Loss 0.0616\t Accuracy 0.9548\n",
      "Epoch [32][50]\t Batch [250][429]\t Training Loss 0.0615\t Accuracy 0.9548\n",
      "Epoch [32][50]\t Batch [300][429]\t Training Loss 0.0621\t Accuracy 0.9542\n",
      "Epoch [32][50]\t Batch [350][429]\t Training Loss 0.0623\t Accuracy 0.9543\n",
      "Epoch [32][50]\t Batch [400][429]\t Training Loss 0.0626\t Accuracy 0.9539\n",
      "\n",
      "Epoch [32]\t Average training loss 0.0628\t Average training accuracy 0.9538\n",
      "Epoch [32]\t Average validation loss 0.0537\t Average validation accuracy 0.9667\n",
      "\n",
      "Epoch [33][50]\t Batch [0][429]\t Training Loss 0.1058\t Accuracy 0.9219\n",
      "Epoch [33][50]\t Batch [50][429]\t Training Loss 0.0605\t Accuracy 0.9571\n",
      "Epoch [33][50]\t Batch [100][429]\t Training Loss 0.0607\t Accuracy 0.9565\n",
      "Epoch [33][50]\t Batch [150][429]\t Training Loss 0.0618\t Accuracy 0.9552\n",
      "Epoch [33][50]\t Batch [200][429]\t Training Loss 0.0615\t Accuracy 0.9551\n",
      "Epoch [33][50]\t Batch [250][429]\t Training Loss 0.0613\t Accuracy 0.9551\n",
      "Epoch [33][50]\t Batch [300][429]\t Training Loss 0.0618\t Accuracy 0.9544\n",
      "Epoch [33][50]\t Batch [350][429]\t Training Loss 0.0620\t Accuracy 0.9545\n",
      "Epoch [33][50]\t Batch [400][429]\t Training Loss 0.0623\t Accuracy 0.9540\n",
      "\n",
      "Epoch [33]\t Average training loss 0.0625\t Average training accuracy 0.9539\n",
      "Epoch [33]\t Average validation loss 0.0530\t Average validation accuracy 0.9679\n",
      "\n",
      "Epoch [34][50]\t Batch [0][429]\t Training Loss 0.0823\t Accuracy 0.9531\n",
      "Epoch [34][50]\t Batch [50][429]\t Training Loss 0.0606\t Accuracy 0.9566\n",
      "Epoch [34][50]\t Batch [100][429]\t Training Loss 0.0605\t Accuracy 0.9566\n",
      "Epoch [34][50]\t Batch [150][429]\t Training Loss 0.0616\t Accuracy 0.9552\n",
      "Epoch [34][50]\t Batch [200][429]\t Training Loss 0.0612\t Accuracy 0.9554\n",
      "Epoch [34][50]\t Batch [250][429]\t Training Loss 0.0609\t Accuracy 0.9554\n",
      "Epoch [34][50]\t Batch [300][429]\t Training Loss 0.0615\t Accuracy 0.9548\n",
      "Epoch [34][50]\t Batch [350][429]\t Training Loss 0.0617\t Accuracy 0.9548\n",
      "Epoch [34][50]\t Batch [400][429]\t Training Loss 0.0620\t Accuracy 0.9544\n",
      "\n",
      "Epoch [34]\t Average training loss 0.0621\t Average training accuracy 0.9543\n",
      "Epoch [34]\t Average validation loss 0.0528\t Average validation accuracy 0.9677\n",
      "\n",
      "Epoch [35][50]\t Batch [0][429]\t Training Loss 0.0681\t Accuracy 0.9609\n",
      "Epoch [35][50]\t Batch [50][429]\t Training Loss 0.0605\t Accuracy 0.9568\n",
      "Epoch [35][50]\t Batch [100][429]\t Training Loss 0.0604\t Accuracy 0.9567\n",
      "Epoch [35][50]\t Batch [150][429]\t Training Loss 0.0614\t Accuracy 0.9552\n",
      "Epoch [35][50]\t Batch [200][429]\t Training Loss 0.0609\t Accuracy 0.9558\n",
      "Epoch [35][50]\t Batch [250][429]\t Training Loss 0.0606\t Accuracy 0.9558\n",
      "Epoch [35][50]\t Batch [300][429]\t Training Loss 0.0612\t Accuracy 0.9553\n",
      "Epoch [35][50]\t Batch [350][429]\t Training Loss 0.0614\t Accuracy 0.9552\n",
      "Epoch [35][50]\t Batch [400][429]\t Training Loss 0.0617\t Accuracy 0.9548\n",
      "\n",
      "Epoch [35]\t Average training loss 0.0619\t Average training accuracy 0.9546\n",
      "Epoch [35]\t Average validation loss 0.0526\t Average validation accuracy 0.9681\n",
      "\n",
      "Epoch [36][50]\t Batch [0][429]\t Training Loss 0.0591\t Accuracy 0.9609\n",
      "Epoch [36][50]\t Batch [50][429]\t Training Loss 0.0601\t Accuracy 0.9571\n",
      "Epoch [36][50]\t Batch [100][429]\t Training Loss 0.0601\t Accuracy 0.9570\n",
      "Epoch [36][50]\t Batch [150][429]\t Training Loss 0.0611\t Accuracy 0.9556\n",
      "Epoch [36][50]\t Batch [200][429]\t Training Loss 0.0604\t Accuracy 0.9564\n",
      "Epoch [36][50]\t Batch [250][429]\t Training Loss 0.0603\t Accuracy 0.9561\n",
      "Epoch [36][50]\t Batch [300][429]\t Training Loss 0.0609\t Accuracy 0.9557\n",
      "Epoch [36][50]\t Batch [350][429]\t Training Loss 0.0611\t Accuracy 0.9557\n",
      "Epoch [36][50]\t Batch [400][429]\t Training Loss 0.0614\t Accuracy 0.9552\n",
      "\n",
      "Epoch [36]\t Average training loss 0.0616\t Average training accuracy 0.9551\n",
      "Epoch [36]\t Average validation loss 0.0526\t Average validation accuracy 0.9679\n",
      "\n",
      "Epoch [37][50]\t Batch [0][429]\t Training Loss 0.0523\t Accuracy 0.9688\n",
      "Epoch [37][50]\t Batch [50][429]\t Training Loss 0.0598\t Accuracy 0.9579\n",
      "Epoch [37][50]\t Batch [100][429]\t Training Loss 0.0599\t Accuracy 0.9571\n",
      "Epoch [37][50]\t Batch [150][429]\t Training Loss 0.0608\t Accuracy 0.9558\n",
      "Epoch [37][50]\t Batch [200][429]\t Training Loss 0.0599\t Accuracy 0.9569\n",
      "Epoch [37][50]\t Batch [250][429]\t Training Loss 0.0600\t Accuracy 0.9564\n",
      "Epoch [37][50]\t Batch [300][429]\t Training Loss 0.0606\t Accuracy 0.9561\n",
      "Epoch [37][50]\t Batch [350][429]\t Training Loss 0.0608\t Accuracy 0.9560\n",
      "Epoch [37][50]\t Batch [400][429]\t Training Loss 0.0612\t Accuracy 0.9555\n",
      "\n",
      "Epoch [37]\t Average training loss 0.0613\t Average training accuracy 0.9553\n",
      "Epoch [37]\t Average validation loss 0.0522\t Average validation accuracy 0.9675\n",
      "\n",
      "Epoch [38][50]\t Batch [0][429]\t Training Loss 0.0528\t Accuracy 0.9609\n",
      "Epoch [38][50]\t Batch [50][429]\t Training Loss 0.0595\t Accuracy 0.9582\n",
      "Epoch [38][50]\t Batch [100][429]\t Training Loss 0.0596\t Accuracy 0.9571\n",
      "Epoch [38][50]\t Batch [150][429]\t Training Loss 0.0604\t Accuracy 0.9560\n",
      "Epoch [38][50]\t Batch [200][429]\t Training Loss 0.0597\t Accuracy 0.9569\n",
      "Epoch [38][50]\t Batch [250][429]\t Training Loss 0.0597\t Accuracy 0.9564\n",
      "Epoch [38][50]\t Batch [300][429]\t Training Loss 0.0603\t Accuracy 0.9562\n",
      "Epoch [38][50]\t Batch [350][429]\t Training Loss 0.0605\t Accuracy 0.9561\n",
      "Epoch [38][50]\t Batch [400][429]\t Training Loss 0.0609\t Accuracy 0.9556\n",
      "\n",
      "Epoch [38]\t Average training loss 0.0610\t Average training accuracy 0.9554\n",
      "Epoch [38]\t Average validation loss 0.0521\t Average validation accuracy 0.9673\n",
      "\n",
      "Epoch [39][50]\t Batch [0][429]\t Training Loss 0.0415\t Accuracy 0.9766\n",
      "Epoch [39][50]\t Batch [50][429]\t Training Loss 0.0592\t Accuracy 0.9585\n",
      "Epoch [39][50]\t Batch [100][429]\t Training Loss 0.0593\t Accuracy 0.9575\n",
      "Epoch [39][50]\t Batch [150][429]\t Training Loss 0.0600\t Accuracy 0.9564\n",
      "Epoch [39][50]\t Batch [200][429]\t Training Loss 0.0594\t Accuracy 0.9572\n",
      "Epoch [39][50]\t Batch [250][429]\t Training Loss 0.0594\t Accuracy 0.9569\n",
      "Epoch [39][50]\t Batch [300][429]\t Training Loss 0.0600\t Accuracy 0.9566\n",
      "Epoch [39][50]\t Batch [350][429]\t Training Loss 0.0602\t Accuracy 0.9564\n",
      "Epoch [39][50]\t Batch [400][429]\t Training Loss 0.0606\t Accuracy 0.9559\n",
      "\n",
      "Epoch [39]\t Average training loss 0.0608\t Average training accuracy 0.9556\n",
      "Epoch [39]\t Average validation loss 0.0518\t Average validation accuracy 0.9675\n",
      "\n",
      "Epoch [40][50]\t Batch [0][429]\t Training Loss 0.0409\t Accuracy 0.9609\n",
      "Epoch [40][50]\t Batch [50][429]\t Training Loss 0.0586\t Accuracy 0.9594\n",
      "Epoch [40][50]\t Batch [100][429]\t Training Loss 0.0589\t Accuracy 0.9581\n",
      "Epoch [40][50]\t Batch [150][429]\t Training Loss 0.0596\t Accuracy 0.9570\n",
      "Epoch [40][50]\t Batch [200][429]\t Training Loss 0.0591\t Accuracy 0.9577\n",
      "Epoch [40][50]\t Batch [250][429]\t Training Loss 0.0591\t Accuracy 0.9574\n",
      "Epoch [40][50]\t Batch [300][429]\t Training Loss 0.0597\t Accuracy 0.9569\n",
      "Epoch [40][50]\t Batch [350][429]\t Training Loss 0.0599\t Accuracy 0.9568\n",
      "Epoch [40][50]\t Batch [400][429]\t Training Loss 0.0603\t Accuracy 0.9561\n",
      "\n",
      "Epoch [40]\t Average training loss 0.0605\t Average training accuracy 0.9559\n",
      "Epoch [40]\t Average validation loss 0.0516\t Average validation accuracy 0.9677\n",
      "\n",
      "Epoch [41][50]\t Batch [0][429]\t Training Loss 0.0399\t Accuracy 0.9688\n",
      "Epoch [41][50]\t Batch [50][429]\t Training Loss 0.0583\t Accuracy 0.9602\n",
      "Epoch [41][50]\t Batch [100][429]\t Training Loss 0.0585\t Accuracy 0.9586\n",
      "Epoch [41][50]\t Batch [150][429]\t Training Loss 0.0591\t Accuracy 0.9577\n",
      "Epoch [41][50]\t Batch [200][429]\t Training Loss 0.0587\t Accuracy 0.9582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41][50]\t Batch [250][429]\t Training Loss 0.0588\t Accuracy 0.9576\n",
      "Epoch [41][50]\t Batch [300][429]\t Training Loss 0.0595\t Accuracy 0.9571\n",
      "Epoch [41][50]\t Batch [350][429]\t Training Loss 0.0595\t Accuracy 0.9570\n",
      "Epoch [41][50]\t Batch [400][429]\t Training Loss 0.0600\t Accuracy 0.9563\n",
      "\n",
      "Epoch [41]\t Average training loss 0.0603\t Average training accuracy 0.9561\n",
      "Epoch [41]\t Average validation loss 0.0514\t Average validation accuracy 0.9679\n",
      "\n",
      "Epoch [42][50]\t Batch [0][429]\t Training Loss 0.0432\t Accuracy 0.9766\n",
      "Epoch [42][50]\t Batch [50][429]\t Training Loss 0.0580\t Accuracy 0.9596\n",
      "Epoch [42][50]\t Batch [100][429]\t Training Loss 0.0582\t Accuracy 0.9588\n",
      "Epoch [42][50]\t Batch [150][429]\t Training Loss 0.0589\t Accuracy 0.9578\n",
      "Epoch [42][50]\t Batch [200][429]\t Training Loss 0.0585\t Accuracy 0.9582\n",
      "Epoch [42][50]\t Batch [250][429]\t Training Loss 0.0586\t Accuracy 0.9577\n",
      "Epoch [42][50]\t Batch [300][429]\t Training Loss 0.0592\t Accuracy 0.9572\n",
      "Epoch [42][50]\t Batch [350][429]\t Training Loss 0.0592\t Accuracy 0.9572\n",
      "Epoch [42][50]\t Batch [400][429]\t Training Loss 0.0598\t Accuracy 0.9564\n",
      "\n",
      "Epoch [42]\t Average training loss 0.0600\t Average training accuracy 0.9562\n",
      "Epoch [42]\t Average validation loss 0.0512\t Average validation accuracy 0.9681\n",
      "\n",
      "Epoch [43][50]\t Batch [0][429]\t Training Loss 0.0687\t Accuracy 0.9531\n",
      "Epoch [43][50]\t Batch [50][429]\t Training Loss 0.0581\t Accuracy 0.9591\n",
      "Epoch [43][50]\t Batch [100][429]\t Training Loss 0.0582\t Accuracy 0.9588\n",
      "Epoch [43][50]\t Batch [150][429]\t Training Loss 0.0587\t Accuracy 0.9579\n",
      "Epoch [43][50]\t Batch [200][429]\t Training Loss 0.0583\t Accuracy 0.9584\n",
      "Epoch [43][50]\t Batch [250][429]\t Training Loss 0.0583\t Accuracy 0.9581\n",
      "Epoch [43][50]\t Batch [300][429]\t Training Loss 0.0590\t Accuracy 0.9575\n",
      "Epoch [43][50]\t Batch [350][429]\t Training Loss 0.0590\t Accuracy 0.9575\n",
      "Epoch [43][50]\t Batch [400][429]\t Training Loss 0.0596\t Accuracy 0.9568\n",
      "\n",
      "Epoch [43]\t Average training loss 0.0598\t Average training accuracy 0.9566\n",
      "Epoch [43]\t Average validation loss 0.0509\t Average validation accuracy 0.9692\n",
      "\n",
      "Epoch [44][50]\t Batch [0][429]\t Training Loss 0.0645\t Accuracy 0.9609\n",
      "Epoch [44][50]\t Batch [50][429]\t Training Loss 0.0579\t Accuracy 0.9593\n",
      "Epoch [44][50]\t Batch [100][429]\t Training Loss 0.0580\t Accuracy 0.9592\n",
      "Epoch [44][50]\t Batch [150][429]\t Training Loss 0.0585\t Accuracy 0.9579\n",
      "Epoch [44][50]\t Batch [200][429]\t Training Loss 0.0582\t Accuracy 0.9584\n",
      "Epoch [44][50]\t Batch [250][429]\t Training Loss 0.0582\t Accuracy 0.9582\n",
      "Epoch [44][50]\t Batch [300][429]\t Training Loss 0.0587\t Accuracy 0.9577\n",
      "Epoch [44][50]\t Batch [350][429]\t Training Loss 0.0588\t Accuracy 0.9577\n",
      "Epoch [44][50]\t Batch [400][429]\t Training Loss 0.0594\t Accuracy 0.9569\n",
      "\n",
      "Epoch [44]\t Average training loss 0.0596\t Average training accuracy 0.9566\n",
      "Epoch [44]\t Average validation loss 0.0508\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [45][50]\t Batch [0][429]\t Training Loss 0.0453\t Accuracy 0.9844\n",
      "Epoch [45][50]\t Batch [50][429]\t Training Loss 0.0576\t Accuracy 0.9593\n",
      "Epoch [45][50]\t Batch [100][429]\t Training Loss 0.0578\t Accuracy 0.9592\n",
      "Epoch [45][50]\t Batch [150][429]\t Training Loss 0.0582\t Accuracy 0.9584\n",
      "Epoch [45][50]\t Batch [200][429]\t Training Loss 0.0580\t Accuracy 0.9586\n",
      "Epoch [45][50]\t Batch [250][429]\t Training Loss 0.0579\t Accuracy 0.9584\n",
      "Epoch [45][50]\t Batch [300][429]\t Training Loss 0.0585\t Accuracy 0.9579\n",
      "Epoch [45][50]\t Batch [350][429]\t Training Loss 0.0586\t Accuracy 0.9580\n",
      "Epoch [45][50]\t Batch [400][429]\t Training Loss 0.0591\t Accuracy 0.9572\n",
      "\n",
      "Epoch [45]\t Average training loss 0.0594\t Average training accuracy 0.9568\n",
      "Epoch [45]\t Average validation loss 0.0506\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [46][50]\t Batch [0][429]\t Training Loss 0.0350\t Accuracy 0.9922\n",
      "Epoch [46][50]\t Batch [50][429]\t Training Loss 0.0573\t Accuracy 0.9597\n",
      "Epoch [46][50]\t Batch [100][429]\t Training Loss 0.0573\t Accuracy 0.9600\n",
      "Epoch [46][50]\t Batch [150][429]\t Training Loss 0.0580\t Accuracy 0.9587\n",
      "Epoch [46][50]\t Batch [200][429]\t Training Loss 0.0577\t Accuracy 0.9589\n",
      "Epoch [46][50]\t Batch [250][429]\t Training Loss 0.0577\t Accuracy 0.9589\n",
      "Epoch [46][50]\t Batch [300][429]\t Training Loss 0.0582\t Accuracy 0.9583\n",
      "Epoch [46][50]\t Batch [350][429]\t Training Loss 0.0583\t Accuracy 0.9583\n",
      "Epoch [46][50]\t Batch [400][429]\t Training Loss 0.0589\t Accuracy 0.9575\n",
      "\n",
      "Epoch [46]\t Average training loss 0.0592\t Average training accuracy 0.9570\n",
      "Epoch [46]\t Average validation loss 0.0505\t Average validation accuracy 0.9692\n",
      "\n",
      "Epoch [47][50]\t Batch [0][429]\t Training Loss 0.0453\t Accuracy 0.9922\n",
      "Epoch [47][50]\t Batch [50][429]\t Training Loss 0.0570\t Accuracy 0.9600\n",
      "Epoch [47][50]\t Batch [100][429]\t Training Loss 0.0568\t Accuracy 0.9606\n",
      "Epoch [47][50]\t Batch [150][429]\t Training Loss 0.0578\t Accuracy 0.9588\n",
      "Epoch [47][50]\t Batch [200][429]\t Training Loss 0.0575\t Accuracy 0.9590\n",
      "Epoch [47][50]\t Batch [250][429]\t Training Loss 0.0575\t Accuracy 0.9589\n",
      "Epoch [47][50]\t Batch [300][429]\t Training Loss 0.0580\t Accuracy 0.9585\n",
      "Epoch [47][50]\t Batch [350][429]\t Training Loss 0.0582\t Accuracy 0.9583\n",
      "Epoch [47][50]\t Batch [400][429]\t Training Loss 0.0586\t Accuracy 0.9576\n",
      "\n",
      "Epoch [47]\t Average training loss 0.0590\t Average training accuracy 0.9572\n",
      "Epoch [47]\t Average validation loss 0.0503\t Average validation accuracy 0.9694\n",
      "\n",
      "Epoch [48][50]\t Batch [0][429]\t Training Loss 0.0572\t Accuracy 0.9688\n",
      "Epoch [48][50]\t Batch [50][429]\t Training Loss 0.0572\t Accuracy 0.9596\n",
      "Epoch [48][50]\t Batch [100][429]\t Training Loss 0.0564\t Accuracy 0.9611\n",
      "Epoch [48][50]\t Batch [150][429]\t Training Loss 0.0575\t Accuracy 0.9591\n",
      "Epoch [48][50]\t Batch [200][429]\t Training Loss 0.0574\t Accuracy 0.9593\n",
      "Epoch [48][50]\t Batch [250][429]\t Training Loss 0.0574\t Accuracy 0.9591\n",
      "Epoch [48][50]\t Batch [300][429]\t Training Loss 0.0578\t Accuracy 0.9587\n",
      "Epoch [48][50]\t Batch [350][429]\t Training Loss 0.0580\t Accuracy 0.9586\n",
      "Epoch [48][50]\t Batch [400][429]\t Training Loss 0.0584\t Accuracy 0.9578\n",
      "\n",
      "Epoch [48]\t Average training loss 0.0588\t Average training accuracy 0.9574\n",
      "Epoch [48]\t Average validation loss 0.0501\t Average validation accuracy 0.9698\n",
      "\n",
      "Epoch [49][50]\t Batch [0][429]\t Training Loss 0.0640\t Accuracy 0.9609\n",
      "Epoch [49][50]\t Batch [50][429]\t Training Loss 0.0573\t Accuracy 0.9602\n",
      "Epoch [49][50]\t Batch [100][429]\t Training Loss 0.0561\t Accuracy 0.9616\n",
      "Epoch [49][50]\t Batch [150][429]\t Training Loss 0.0574\t Accuracy 0.9592\n",
      "Epoch [49][50]\t Batch [200][429]\t Training Loss 0.0573\t Accuracy 0.9593\n",
      "Epoch [49][50]\t Batch [250][429]\t Training Loss 0.0572\t Accuracy 0.9592\n",
      "Epoch [49][50]\t Batch [300][429]\t Training Loss 0.0577\t Accuracy 0.9587\n",
      "Epoch [49][50]\t Batch [350][429]\t Training Loss 0.0578\t Accuracy 0.9587\n",
      "Epoch [49][50]\t Batch [400][429]\t Training Loss 0.0583\t Accuracy 0.9579\n",
      "\n",
      "Epoch [49]\t Average training loss 0.0587\t Average training accuracy 0.9574\n",
      "Epoch [49]\t Average validation loss 0.0500\t Average validation accuracy 0.9700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:46:19.928515Z",
     "start_time": "2023-11-10T08:46:16.363018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9585.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "收敛情况：都可以收敛, 但是使用relu收敛速度更快\n",
    "\n",
    "准确率: relu准确率更高\n",
    "梯度消失问题：Sigmoid 函数具有S形曲线，其导数在大部分区域都很小，这可能导致梯度消失问题\n",
    "在深度网络,梯度消失有可能会导致梯度更新变得非常小，从而使得模型无法学习\n",
    "\n",
    "ReLU可以缓解梯度消失问题,提供更快的收敛速度,以及更好的性能 -> ReLU更常用(效果更好)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:46:20.320377Z",
     "start_time": "2023-11-10T08:46:19.929522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用Softmax交叉熵损失训练多层感知机(MLP with Softmax Cross-Entropy Loss)\n",
    "第二部分将使用Softmax交叉熵损失训练多层感知机. \n",
    "分别使用**Sigmoid**激活函数和**ReLU**激活函数.\n",
    "\n",
    "### TODO\n",
    "执行以下代码之前，请完成 **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:46:20.325493Z",
     "start_time": "2023-11-10T08:46:20.320883Z"
    }
   },
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 使用Softmax交叉熵损失和Sigmoid激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用Softmax交叉熵损失和Sigmoid激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:46:20.333890Z",
     "start_time": "2023-11-10T08:46:20.327824Z"
    }
   },
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:47:36.746322Z",
     "start_time": "2023-11-10T08:46:20.335761Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][50]\t Batch [0][429]\t Training Loss 2.7388\t Accuracy 0.0547\n",
      "Epoch [0][50]\t Batch [50][429]\t Training Loss 2.3501\t Accuracy 0.1435\n",
      "Epoch [0][50]\t Batch [100][429]\t Training Loss 2.2828\t Accuracy 0.2188\n",
      "Epoch [0][50]\t Batch [150][429]\t Training Loss 2.2465\t Accuracy 0.2775\n",
      "Epoch [0][50]\t Batch [200][429]\t Training Loss 2.2182\t Accuracy 0.3259\n",
      "Epoch [0][50]\t Batch [250][429]\t Training Loss 2.1958\t Accuracy 0.3590\n",
      "Epoch [0][50]\t Batch [300][429]\t Training Loss 2.1738\t Accuracy 0.3937\n",
      "Epoch [0][50]\t Batch [350][429]\t Training Loss 2.1535\t Accuracy 0.4241\n",
      "Epoch [0][50]\t Batch [400][429]\t Training Loss 2.1340\t Accuracy 0.4490\n",
      "\n",
      "Epoch [0]\t Average training loss 2.1228\t Average training accuracy 0.4634\n",
      "Epoch [0]\t Average validation loss 1.9392\t Average validation accuracy 0.6975\n",
      "\n",
      "Epoch [1][50]\t Batch [0][429]\t Training Loss 2.0064\t Accuracy 0.5156\n",
      "Epoch [1][50]\t Batch [50][429]\t Training Loss 1.9350\t Accuracy 0.6723\n",
      "Epoch [1][50]\t Batch [100][429]\t Training Loss 1.9228\t Accuracy 0.6641\n",
      "Epoch [1][50]\t Batch [150][429]\t Training Loss 1.9158\t Accuracy 0.6605\n",
      "Epoch [1][50]\t Batch [200][429]\t Training Loss 1.8997\t Accuracy 0.6697\n",
      "Epoch [1][50]\t Batch [250][429]\t Training Loss 1.8883\t Accuracy 0.6761\n",
      "Epoch [1][50]\t Batch [300][429]\t Training Loss 1.8763\t Accuracy 0.6845\n",
      "Epoch [1][50]\t Batch [350][429]\t Training Loss 1.8644\t Accuracy 0.6908\n",
      "Epoch [1][50]\t Batch [400][429]\t Training Loss 1.8522\t Accuracy 0.6953\n",
      "\n",
      "Epoch [1]\t Average training loss 1.8445\t Average training accuracy 0.6993\n",
      "Epoch [1]\t Average validation loss 1.7035\t Average validation accuracy 0.7877\n",
      "\n",
      "Epoch [2][50]\t Batch [0][429]\t Training Loss 1.8393\t Accuracy 0.6484\n",
      "Epoch [2][50]\t Batch [50][429]\t Training Loss 1.7135\t Accuracy 0.7440\n",
      "Epoch [2][50]\t Batch [100][429]\t Training Loss 1.7057\t Accuracy 0.7410\n",
      "Epoch [2][50]\t Batch [150][429]\t Training Loss 1.7053\t Accuracy 0.7378\n",
      "Epoch [2][50]\t Batch [200][429]\t Training Loss 1.6918\t Accuracy 0.7438\n",
      "Epoch [2][50]\t Batch [250][429]\t Training Loss 1.6844\t Accuracy 0.7449\n",
      "Epoch [2][50]\t Batch [300][429]\t Training Loss 1.6759\t Accuracy 0.7498\n",
      "Epoch [2][50]\t Batch [350][429]\t Training Loss 1.6672\t Accuracy 0.7521\n",
      "Epoch [2][50]\t Batch [400][429]\t Training Loss 1.6582\t Accuracy 0.7537\n",
      "\n",
      "Epoch [2]\t Average training loss 1.6520\t Average training accuracy 0.7555\n",
      "Epoch [2]\t Average validation loss 1.5271\t Average validation accuracy 0.8175\n",
      "\n",
      "Epoch [3][50]\t Batch [0][429]\t Training Loss 1.5286\t Accuracy 0.8047\n",
      "Epoch [3][50]\t Batch [50][429]\t Training Loss 1.5465\t Accuracy 0.7727\n",
      "Epoch [3][50]\t Batch [100][429]\t Training Loss 1.5432\t Accuracy 0.7724\n",
      "Epoch [3][50]\t Batch [150][429]\t Training Loss 1.5469\t Accuracy 0.7679\n",
      "Epoch [3][50]\t Batch [200][429]\t Training Loss 1.5361\t Accuracy 0.7717\n",
      "Epoch [3][50]\t Batch [250][429]\t Training Loss 1.5310\t Accuracy 0.7712\n",
      "Epoch [3][50]\t Batch [300][429]\t Training Loss 1.5252\t Accuracy 0.7745\n",
      "Epoch [3][50]\t Batch [350][429]\t Training Loss 1.5190\t Accuracy 0.7754\n",
      "Epoch [3][50]\t Batch [400][429]\t Training Loss 1.5120\t Accuracy 0.7763\n",
      "\n",
      "Epoch [3]\t Average training loss 1.5069\t Average training accuracy 0.7779\n",
      "Epoch [3]\t Average validation loss 1.3918\t Average validation accuracy 0.8351\n",
      "\n",
      "Epoch [4][50]\t Batch [0][429]\t Training Loss 1.3326\t Accuracy 0.8594\n",
      "Epoch [4][50]\t Batch [50][429]\t Training Loss 1.4198\t Accuracy 0.7891\n",
      "Epoch [4][50]\t Batch [100][429]\t Training Loss 1.4185\t Accuracy 0.7903\n",
      "Epoch [4][50]\t Batch [150][429]\t Training Loss 1.4259\t Accuracy 0.7837\n",
      "Epoch [4][50]\t Batch [200][429]\t Training Loss 1.4174\t Accuracy 0.7864\n",
      "Epoch [4][50]\t Batch [250][429]\t Training Loss 1.4137\t Accuracy 0.7863\n",
      "Epoch [4][50]\t Batch [300][429]\t Training Loss 1.4100\t Accuracy 0.7880\n",
      "Epoch [4][50]\t Batch [350][429]\t Training Loss 1.4057\t Accuracy 0.7880\n",
      "Epoch [4][50]\t Batch [400][429]\t Training Loss 1.4001\t Accuracy 0.7887\n",
      "\n",
      "Epoch [4]\t Average training loss 1.3956\t Average training accuracy 0.7898\n",
      "Epoch [4]\t Average validation loss 1.2869\t Average validation accuracy 0.8448\n",
      "\n",
      "Epoch [5][50]\t Batch [0][429]\t Training Loss 1.2530\t Accuracy 0.8359\n",
      "Epoch [5][50]\t Batch [50][429]\t Training Loss 1.3221\t Accuracy 0.8002\n",
      "Epoch [5][50]\t Batch [100][429]\t Training Loss 1.3222\t Accuracy 0.8016\n",
      "Epoch [5][50]\t Batch [150][429]\t Training Loss 1.3326\t Accuracy 0.7946\n",
      "Epoch [5][50]\t Batch [200][429]\t Training Loss 1.3251\t Accuracy 0.7964\n",
      "Epoch [5][50]\t Batch [250][429]\t Training Loss 1.3223\t Accuracy 0.7954\n",
      "Epoch [5][50]\t Batch [300][429]\t Training Loss 1.3206\t Accuracy 0.7964\n",
      "Epoch [5][50]\t Batch [350][429]\t Training Loss 1.3177\t Accuracy 0.7964\n",
      "Epoch [5][50]\t Batch [400][429]\t Training Loss 1.3129\t Accuracy 0.7971\n",
      "\n",
      "Epoch [5]\t Average training loss 1.3088\t Average training accuracy 0.7983\n",
      "Epoch [5]\t Average validation loss 1.2049\t Average validation accuracy 0.8516\n",
      "\n",
      "Epoch [6][50]\t Batch [0][429]\t Training Loss 1.2330\t Accuracy 0.7656\n",
      "Epoch [6][50]\t Batch [50][429]\t Training Loss 1.2468\t Accuracy 0.8082\n",
      "Epoch [6][50]\t Batch [100][429]\t Training Loss 1.2464\t Accuracy 0.8103\n",
      "Epoch [6][50]\t Batch [150][429]\t Training Loss 1.2591\t Accuracy 0.8012\n",
      "Epoch [6][50]\t Batch [200][429]\t Training Loss 1.2526\t Accuracy 0.8027\n",
      "Epoch [6][50]\t Batch [250][429]\t Training Loss 1.2502\t Accuracy 0.8020\n",
      "Epoch [6][50]\t Batch [300][429]\t Training Loss 1.2498\t Accuracy 0.8022\n",
      "Epoch [6][50]\t Batch [350][429]\t Training Loss 1.2481\t Accuracy 0.8019\n",
      "Epoch [6][50]\t Batch [400][429]\t Training Loss 1.2439\t Accuracy 0.8025\n",
      "\n",
      "Epoch [6]\t Average training loss 1.2402\t Average training accuracy 0.8035\n",
      "Epoch [6]\t Average validation loss 1.1386\t Average validation accuracy 0.8550\n",
      "\n",
      "Epoch [7][50]\t Batch [0][429]\t Training Loss 1.1330\t Accuracy 0.8047\n",
      "Epoch [7][50]\t Batch [50][429]\t Training Loss 1.1842\t Accuracy 0.8116\n",
      "Epoch [7][50]\t Batch [100][429]\t Training Loss 1.1850\t Accuracy 0.8143\n",
      "Epoch [7][50]\t Batch [150][429]\t Training Loss 1.2005\t Accuracy 0.8050\n",
      "Epoch [7][50]\t Batch [200][429]\t Training Loss 1.1945\t Accuracy 0.8065\n",
      "Epoch [7][50]\t Batch [250][429]\t Training Loss 1.1925\t Accuracy 0.8059\n",
      "Epoch [7][50]\t Batch [300][429]\t Training Loss 1.1932\t Accuracy 0.8060\n",
      "Epoch [7][50]\t Batch [350][429]\t Training Loss 1.1919\t Accuracy 0.8057\n",
      "Epoch [7][50]\t Batch [400][429]\t Training Loss 1.1882\t Accuracy 0.8061\n",
      "\n",
      "Epoch [7]\t Average training loss 1.1851\t Average training accuracy 0.8068\n",
      "Epoch [7]\t Average validation loss 1.0852\t Average validation accuracy 0.8538\n",
      "\n",
      "Epoch [8][50]\t Batch [0][429]\t Training Loss 1.0856\t Accuracy 0.8438\n",
      "Epoch [8][50]\t Batch [50][429]\t Training Loss 1.1323\t Accuracy 0.8157\n",
      "Epoch [8][50]\t Batch [100][429]\t Training Loss 1.1354\t Accuracy 0.8181\n",
      "Epoch [8][50]\t Batch [150][429]\t Training Loss 1.1525\t Accuracy 0.8086\n",
      "Epoch [8][50]\t Batch [200][429]\t Training Loss 1.1480\t Accuracy 0.8101\n",
      "Epoch [8][50]\t Batch [250][429]\t Training Loss 1.1458\t Accuracy 0.8092\n",
      "Epoch [8][50]\t Batch [300][429]\t Training Loss 1.1470\t Accuracy 0.8094\n",
      "Epoch [8][50]\t Batch [350][429]\t Training Loss 1.1460\t Accuracy 0.8092\n",
      "Epoch [8][50]\t Batch [400][429]\t Training Loss 1.1431\t Accuracy 0.8092\n",
      "\n",
      "Epoch [8]\t Average training loss 1.1403\t Average training accuracy 0.8100\n",
      "Epoch [8]\t Average validation loss 1.0409\t Average validation accuracy 0.8548\n",
      "\n",
      "Epoch [9][50]\t Batch [0][429]\t Training Loss 1.0225\t Accuracy 0.8516\n",
      "Epoch [9][50]\t Batch [50][429]\t Training Loss 1.0890\t Accuracy 0.8205\n",
      "Epoch [9][50]\t Batch [100][429]\t Training Loss 1.0948\t Accuracy 0.8214\n",
      "Epoch [9][50]\t Batch [150][429]\t Training Loss 1.1136\t Accuracy 0.8110\n",
      "Epoch [9][50]\t Batch [200][429]\t Training Loss 1.1089\t Accuracy 0.8123\n",
      "Epoch [9][50]\t Batch [250][429]\t Training Loss 1.1068\t Accuracy 0.8116\n",
      "Epoch [9][50]\t Batch [300][429]\t Training Loss 1.1088\t Accuracy 0.8114\n",
      "Epoch [9][50]\t Batch [350][429]\t Training Loss 1.1082\t Accuracy 0.8109\n",
      "Epoch [9][50]\t Batch [400][429]\t Training Loss 1.1057\t Accuracy 0.8111\n",
      "\n",
      "Epoch [9]\t Average training loss 1.1032\t Average training accuracy 0.8116\n",
      "Epoch [9]\t Average validation loss 1.0038\t Average validation accuracy 0.8574\n",
      "\n",
      "Epoch [10][50]\t Batch [0][429]\t Training Loss 0.9659\t Accuracy 0.8750\n",
      "Epoch [10][50]\t Batch [50][429]\t Training Loss 1.0533\t Accuracy 0.8226\n",
      "Epoch [10][50]\t Batch [100][429]\t Training Loss 1.0609\t Accuracy 0.8235\n",
      "Epoch [10][50]\t Batch [150][429]\t Training Loss 1.0804\t Accuracy 0.8125\n",
      "Epoch [10][50]\t Batch [200][429]\t Training Loss 1.0764\t Accuracy 0.8138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10][50]\t Batch [250][429]\t Training Loss 1.0740\t Accuracy 0.8133\n",
      "Epoch [10][50]\t Batch [300][429]\t Training Loss 1.0774\t Accuracy 0.8128\n",
      "Epoch [10][50]\t Batch [350][429]\t Training Loss 1.0765\t Accuracy 0.8124\n",
      "Epoch [10][50]\t Batch [400][429]\t Training Loss 1.0745\t Accuracy 0.8126\n",
      "\n",
      "Epoch [10]\t Average training loss 1.0718\t Average training accuracy 0.8131\n",
      "Epoch [10]\t Average validation loss 0.9721\t Average validation accuracy 0.8558\n",
      "\n",
      "Epoch [11][50]\t Batch [0][429]\t Training Loss 1.1119\t Accuracy 0.8359\n",
      "Epoch [11][50]\t Batch [50][429]\t Training Loss 1.0272\t Accuracy 0.8229\n",
      "Epoch [11][50]\t Batch [100][429]\t Training Loss 1.0340\t Accuracy 0.8229\n",
      "Epoch [11][50]\t Batch [150][429]\t Training Loss 1.0532\t Accuracy 0.8120\n",
      "Epoch [11][50]\t Batch [200][429]\t Training Loss 1.0492\t Accuracy 0.8135\n",
      "Epoch [11][50]\t Batch [250][429]\t Training Loss 1.0476\t Accuracy 0.8129\n",
      "Epoch [11][50]\t Batch [300][429]\t Training Loss 1.0511\t Accuracy 0.8124\n",
      "Epoch [11][50]\t Batch [350][429]\t Training Loss 1.0503\t Accuracy 0.8121\n",
      "Epoch [11][50]\t Batch [400][429]\t Training Loss 1.0481\t Accuracy 0.8124\n",
      "\n",
      "Epoch [11]\t Average training loss 1.0456\t Average training accuracy 0.8128\n",
      "Epoch [11]\t Average validation loss 0.9465\t Average validation accuracy 0.8566\n",
      "\n",
      "Epoch [12][50]\t Batch [0][429]\t Training Loss 1.2231\t Accuracy 0.7812\n",
      "Epoch [12][50]\t Batch [50][429]\t Training Loss 1.0045\t Accuracy 0.8255\n",
      "Epoch [12][50]\t Batch [100][429]\t Training Loss 1.0104\t Accuracy 0.8248\n",
      "Epoch [12][50]\t Batch [150][429]\t Training Loss 1.0303\t Accuracy 0.8134\n",
      "Epoch [12][50]\t Batch [200][429]\t Training Loss 1.0258\t Accuracy 0.8151\n",
      "Epoch [12][50]\t Batch [250][429]\t Training Loss 1.0249\t Accuracy 0.8134\n",
      "Epoch [12][50]\t Batch [300][429]\t Training Loss 1.0284\t Accuracy 0.8129\n",
      "Epoch [12][50]\t Batch [350][429]\t Training Loss 1.0278\t Accuracy 0.8128\n",
      "Epoch [12][50]\t Batch [400][429]\t Training Loss 1.0256\t Accuracy 0.8130\n",
      "\n",
      "Epoch [12]\t Average training loss 1.0233\t Average training accuracy 0.8135\n",
      "Epoch [12]\t Average validation loss 0.9237\t Average validation accuracy 0.8586\n",
      "\n",
      "Epoch [13][50]\t Batch [0][429]\t Training Loss 1.0073\t Accuracy 0.8047\n",
      "Epoch [13][50]\t Batch [50][429]\t Training Loss 0.9829\t Accuracy 0.8251\n",
      "Epoch [13][50]\t Batch [100][429]\t Training Loss 0.9890\t Accuracy 0.8242\n",
      "Epoch [13][50]\t Batch [150][429]\t Training Loss 1.0097\t Accuracy 0.8134\n",
      "Epoch [13][50]\t Batch [200][429]\t Training Loss 1.0051\t Accuracy 0.8155\n",
      "Epoch [13][50]\t Batch [250][429]\t Training Loss 1.0048\t Accuracy 0.8137\n",
      "Epoch [13][50]\t Batch [300][429]\t Training Loss 1.0082\t Accuracy 0.8131\n",
      "Epoch [13][50]\t Batch [350][429]\t Training Loss 1.0079\t Accuracy 0.8129\n",
      "Epoch [13][50]\t Batch [400][429]\t Training Loss 1.0060\t Accuracy 0.8130\n",
      "\n",
      "Epoch [13]\t Average training loss 1.0039\t Average training accuracy 0.8133\n",
      "Epoch [13]\t Average validation loss 0.9036\t Average validation accuracy 0.8586\n",
      "\n",
      "Epoch [14][50]\t Batch [0][429]\t Training Loss 0.8487\t Accuracy 0.8281\n",
      "Epoch [14][50]\t Batch [50][429]\t Training Loss 0.9618\t Accuracy 0.8257\n",
      "Epoch [14][50]\t Batch [100][429]\t Training Loss 0.9688\t Accuracy 0.8243\n",
      "Epoch [14][50]\t Batch [150][429]\t Training Loss 0.9920\t Accuracy 0.8132\n",
      "Epoch [14][50]\t Batch [200][429]\t Training Loss 0.9868\t Accuracy 0.8153\n",
      "Epoch [14][50]\t Batch [250][429]\t Training Loss 0.9869\t Accuracy 0.8137\n",
      "Epoch [14][50]\t Batch [300][429]\t Training Loss 0.9904\t Accuracy 0.8131\n",
      "Epoch [14][50]\t Batch [350][429]\t Training Loss 0.9905\t Accuracy 0.8129\n",
      "Epoch [14][50]\t Batch [400][429]\t Training Loss 0.9888\t Accuracy 0.8129\n",
      "\n",
      "Epoch [14]\t Average training loss 0.9867\t Average training accuracy 0.8134\n",
      "Epoch [14]\t Average validation loss 0.8853\t Average validation accuracy 0.8596\n",
      "\n",
      "Epoch [15][50]\t Batch [0][429]\t Training Loss 0.8691\t Accuracy 0.8594\n",
      "Epoch [15][50]\t Batch [50][429]\t Training Loss 0.9451\t Accuracy 0.8255\n",
      "Epoch [15][50]\t Batch [100][429]\t Training Loss 0.9522\t Accuracy 0.8245\n",
      "Epoch [15][50]\t Batch [150][429]\t Training Loss 0.9761\t Accuracy 0.8133\n",
      "Epoch [15][50]\t Batch [200][429]\t Training Loss 0.9712\t Accuracy 0.8155\n",
      "Epoch [15][50]\t Batch [250][429]\t Training Loss 0.9712\t Accuracy 0.8137\n",
      "Epoch [15][50]\t Batch [300][429]\t Training Loss 0.9748\t Accuracy 0.8131\n",
      "Epoch [15][50]\t Batch [350][429]\t Training Loss 0.9754\t Accuracy 0.8127\n",
      "Epoch [15][50]\t Batch [400][429]\t Training Loss 0.9736\t Accuracy 0.8129\n",
      "\n",
      "Epoch [15]\t Average training loss 0.9717\t Average training accuracy 0.8133\n",
      "Epoch [15]\t Average validation loss 0.8704\t Average validation accuracy 0.8592\n",
      "\n",
      "Epoch [16][50]\t Batch [0][429]\t Training Loss 0.8494\t Accuracy 0.9219\n",
      "Epoch [16][50]\t Batch [50][429]\t Training Loss 0.9286\t Accuracy 0.8280\n",
      "Epoch [16][50]\t Batch [100][429]\t Training Loss 0.9370\t Accuracy 0.8261\n",
      "Epoch [16][50]\t Batch [150][429]\t Training Loss 0.9623\t Accuracy 0.8139\n",
      "Epoch [16][50]\t Batch [200][429]\t Training Loss 0.9576\t Accuracy 0.8156\n",
      "Epoch [16][50]\t Batch [250][429]\t Training Loss 0.9572\t Accuracy 0.8140\n",
      "Epoch [16][50]\t Batch [300][429]\t Training Loss 0.9608\t Accuracy 0.8134\n",
      "Epoch [16][50]\t Batch [350][429]\t Training Loss 0.9621\t Accuracy 0.8128\n",
      "Epoch [16][50]\t Batch [400][429]\t Training Loss 0.9601\t Accuracy 0.8133\n",
      "\n",
      "Epoch [16]\t Average training loss 0.9582\t Average training accuracy 0.8137\n",
      "Epoch [16]\t Average validation loss 0.8561\t Average validation accuracy 0.8602\n",
      "\n",
      "Epoch [17][50]\t Batch [0][429]\t Training Loss 0.9306\t Accuracy 0.8125\n",
      "Epoch [17][50]\t Batch [50][429]\t Training Loss 0.9155\t Accuracy 0.8286\n",
      "Epoch [17][50]\t Batch [100][429]\t Training Loss 0.9259\t Accuracy 0.8259\n",
      "Epoch [17][50]\t Batch [150][429]\t Training Loss 0.9503\t Accuracy 0.8142\n",
      "Epoch [17][50]\t Batch [200][429]\t Training Loss 0.9449\t Accuracy 0.8162\n",
      "Epoch [17][50]\t Batch [250][429]\t Training Loss 0.9450\t Accuracy 0.8143\n",
      "Epoch [17][50]\t Batch [300][429]\t Training Loss 0.9485\t Accuracy 0.8141\n",
      "Epoch [17][50]\t Batch [350][429]\t Training Loss 0.9506\t Accuracy 0.8130\n",
      "Epoch [17][50]\t Batch [400][429]\t Training Loss 0.9482\t Accuracy 0.8138\n",
      "\n",
      "Epoch [17]\t Average training loss 0.9466\t Average training accuracy 0.8141\n",
      "Epoch [17]\t Average validation loss 0.8432\t Average validation accuracy 0.8614\n",
      "\n",
      "Epoch [18][50]\t Batch [0][429]\t Training Loss 0.8188\t Accuracy 0.8438\n",
      "Epoch [18][50]\t Batch [50][429]\t Training Loss 0.9033\t Accuracy 0.8292\n",
      "Epoch [18][50]\t Batch [100][429]\t Training Loss 0.9140\t Accuracy 0.8260\n",
      "Epoch [18][50]\t Batch [150][429]\t Training Loss 0.9382\t Accuracy 0.8148\n",
      "Epoch [18][50]\t Batch [200][429]\t Training Loss 0.9338\t Accuracy 0.8167\n",
      "Epoch [18][50]\t Batch [250][429]\t Training Loss 0.9337\t Accuracy 0.8146\n",
      "Epoch [18][50]\t Batch [300][429]\t Training Loss 0.9376\t Accuracy 0.8140\n",
      "Epoch [18][50]\t Batch [350][429]\t Training Loss 0.9400\t Accuracy 0.8127\n",
      "Epoch [18][50]\t Batch [400][429]\t Training Loss 0.9373\t Accuracy 0.8136\n",
      "\n",
      "Epoch [18]\t Average training loss 0.9360\t Average training accuracy 0.8138\n",
      "Epoch [18]\t Average validation loss 0.8323\t Average validation accuracy 0.8606\n",
      "\n",
      "Epoch [19][50]\t Batch [0][429]\t Training Loss 0.7583\t Accuracy 0.8516\n",
      "Epoch [19][50]\t Batch [50][429]\t Training Loss 0.8923\t Accuracy 0.8301\n",
      "Epoch [19][50]\t Batch [100][429]\t Training Loss 0.9044\t Accuracy 0.8263\n",
      "Epoch [19][50]\t Batch [150][429]\t Training Loss 0.9267\t Accuracy 0.8158\n",
      "Epoch [19][50]\t Batch [200][429]\t Training Loss 0.9236\t Accuracy 0.8168\n",
      "Epoch [19][50]\t Batch [250][429]\t Training Loss 0.9230\t Accuracy 0.8152\n",
      "Epoch [19][50]\t Batch [300][429]\t Training Loss 0.9275\t Accuracy 0.8142\n",
      "Epoch [19][50]\t Batch [350][429]\t Training Loss 0.9305\t Accuracy 0.8129\n",
      "Epoch [19][50]\t Batch [400][429]\t Training Loss 0.9272\t Accuracy 0.8138\n",
      "\n",
      "Epoch [19]\t Average training loss 0.9263\t Average training accuracy 0.8137\n",
      "Epoch [19]\t Average validation loss 0.8223\t Average validation accuracy 0.8604\n",
      "\n",
      "Epoch [20][50]\t Batch [0][429]\t Training Loss 0.8166\t Accuracy 0.8438\n",
      "Epoch [20][50]\t Batch [50][429]\t Training Loss 0.8818\t Accuracy 0.8312\n",
      "Epoch [20][50]\t Batch [100][429]\t Training Loss 0.8951\t Accuracy 0.8269\n",
      "Epoch [20][50]\t Batch [150][429]\t Training Loss 0.9174\t Accuracy 0.8167\n",
      "Epoch [20][50]\t Batch [200][429]\t Training Loss 0.9147\t Accuracy 0.8171\n",
      "Epoch [20][50]\t Batch [250][429]\t Training Loss 0.9138\t Accuracy 0.8158\n",
      "Epoch [20][50]\t Batch [300][429]\t Training Loss 0.9185\t Accuracy 0.8144\n",
      "Epoch [20][50]\t Batch [350][429]\t Training Loss 0.9217\t Accuracy 0.8131\n",
      "Epoch [20][50]\t Batch [400][429]\t Training Loss 0.9180\t Accuracy 0.8141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [20]\t Average training loss 0.9175\t Average training accuracy 0.8140\n",
      "Epoch [20]\t Average validation loss 0.8125\t Average validation accuracy 0.8600\n",
      "\n",
      "Epoch [21][50]\t Batch [0][429]\t Training Loss 0.9864\t Accuracy 0.7969\n",
      "Epoch [21][50]\t Batch [50][429]\t Training Loss 0.8744\t Accuracy 0.8290\n",
      "Epoch [21][50]\t Batch [100][429]\t Training Loss 0.8882\t Accuracy 0.8256\n",
      "Epoch [21][50]\t Batch [150][429]\t Training Loss 0.9088\t Accuracy 0.8161\n",
      "Epoch [21][50]\t Batch [200][429]\t Training Loss 0.9072\t Accuracy 0.8169\n",
      "Epoch [21][50]\t Batch [250][429]\t Training Loss 0.9063\t Accuracy 0.8155\n",
      "Epoch [21][50]\t Batch [300][429]\t Training Loss 0.9106\t Accuracy 0.8142\n",
      "Epoch [21][50]\t Batch [350][429]\t Training Loss 0.9139\t Accuracy 0.8130\n",
      "Epoch [21][50]\t Batch [400][429]\t Training Loss 0.9096\t Accuracy 0.8143\n",
      "\n",
      "Epoch [21]\t Average training loss 0.9097\t Average training accuracy 0.8139\n",
      "Epoch [21]\t Average validation loss 0.8048\t Average validation accuracy 0.8614\n",
      "\n",
      "Epoch [22][50]\t Batch [0][429]\t Training Loss 0.9891\t Accuracy 0.8125\n",
      "Epoch [22][50]\t Batch [50][429]\t Training Loss 0.8695\t Accuracy 0.8290\n",
      "Epoch [22][50]\t Batch [100][429]\t Training Loss 0.8825\t Accuracy 0.8259\n",
      "Epoch [22][50]\t Batch [150][429]\t Training Loss 0.9019\t Accuracy 0.8166\n",
      "Epoch [22][50]\t Batch [200][429]\t Training Loss 0.9003\t Accuracy 0.8174\n",
      "Epoch [22][50]\t Batch [250][429]\t Training Loss 0.8995\t Accuracy 0.8157\n",
      "Epoch [22][50]\t Batch [300][429]\t Training Loss 0.9035\t Accuracy 0.8141\n",
      "Epoch [22][50]\t Batch [350][429]\t Training Loss 0.9066\t Accuracy 0.8133\n",
      "Epoch [22][50]\t Batch [400][429]\t Training Loss 0.9026\t Accuracy 0.8143\n",
      "\n",
      "Epoch [22]\t Average training loss 0.9027\t Average training accuracy 0.8140\n",
      "Epoch [22]\t Average validation loss 0.7969\t Average validation accuracy 0.8608\n",
      "\n",
      "Epoch [23][50]\t Batch [0][429]\t Training Loss 0.9209\t Accuracy 0.8203\n",
      "Epoch [23][50]\t Batch [50][429]\t Training Loss 0.8668\t Accuracy 0.8287\n",
      "Epoch [23][50]\t Batch [100][429]\t Training Loss 0.8760\t Accuracy 0.8256\n",
      "Epoch [23][50]\t Batch [150][429]\t Training Loss 0.8958\t Accuracy 0.8169\n",
      "Epoch [23][50]\t Batch [200][429]\t Training Loss 0.8936\t Accuracy 0.8177\n",
      "Epoch [23][50]\t Batch [250][429]\t Training Loss 0.8923\t Accuracy 0.8164\n",
      "Epoch [23][50]\t Batch [300][429]\t Training Loss 0.8972\t Accuracy 0.8143\n",
      "Epoch [23][50]\t Batch [350][429]\t Training Loss 0.9001\t Accuracy 0.8136\n",
      "Epoch [23][50]\t Batch [400][429]\t Training Loss 0.8966\t Accuracy 0.8143\n",
      "\n",
      "Epoch [23]\t Average training loss 0.8960\t Average training accuracy 0.8142\n",
      "Epoch [23]\t Average validation loss 0.7897\t Average validation accuracy 0.8616\n",
      "\n",
      "Epoch [24][50]\t Batch [0][429]\t Training Loss 1.0259\t Accuracy 0.7656\n",
      "Epoch [24][50]\t Batch [50][429]\t Training Loss 0.8631\t Accuracy 0.8275\n",
      "Epoch [24][50]\t Batch [100][429]\t Training Loss 0.8721\t Accuracy 0.8247\n",
      "Epoch [24][50]\t Batch [150][429]\t Training Loss 0.8906\t Accuracy 0.8164\n",
      "Epoch [24][50]\t Batch [200][429]\t Training Loss 0.8883\t Accuracy 0.8175\n",
      "Epoch [24][50]\t Batch [250][429]\t Training Loss 0.8864\t Accuracy 0.8163\n",
      "Epoch [24][50]\t Batch [300][429]\t Training Loss 0.8915\t Accuracy 0.8140\n",
      "Epoch [24][50]\t Batch [350][429]\t Training Loss 0.8946\t Accuracy 0.8133\n",
      "Epoch [24][50]\t Batch [400][429]\t Training Loss 0.8912\t Accuracy 0.8139\n",
      "\n",
      "Epoch [24]\t Average training loss 0.8904\t Average training accuracy 0.8138\n",
      "Epoch [24]\t Average validation loss 0.7845\t Average validation accuracy 0.8608\n",
      "\n",
      "Epoch [25][50]\t Batch [0][429]\t Training Loss 0.9802\t Accuracy 0.8047\n",
      "Epoch [25][50]\t Batch [50][429]\t Training Loss 0.8591\t Accuracy 0.8254\n",
      "Epoch [25][50]\t Batch [100][429]\t Training Loss 0.8669\t Accuracy 0.8243\n",
      "Epoch [25][50]\t Batch [150][429]\t Training Loss 0.8850\t Accuracy 0.8165\n",
      "Epoch [25][50]\t Batch [200][429]\t Training Loss 0.8837\t Accuracy 0.8171\n",
      "Epoch [25][50]\t Batch [250][429]\t Training Loss 0.8810\t Accuracy 0.8161\n",
      "Epoch [25][50]\t Batch [300][429]\t Training Loss 0.8865\t Accuracy 0.8137\n",
      "Epoch [25][50]\t Batch [350][429]\t Training Loss 0.8896\t Accuracy 0.8131\n",
      "Epoch [25][50]\t Batch [400][429]\t Training Loss 0.8857\t Accuracy 0.8139\n",
      "\n",
      "Epoch [25]\t Average training loss 0.8850\t Average training accuracy 0.8138\n",
      "Epoch [25]\t Average validation loss 0.7785\t Average validation accuracy 0.8604\n",
      "\n",
      "Epoch [26][50]\t Batch [0][429]\t Training Loss 0.9953\t Accuracy 0.7422\n",
      "Epoch [26][50]\t Batch [50][429]\t Training Loss 0.8556\t Accuracy 0.8249\n",
      "Epoch [26][50]\t Batch [100][429]\t Training Loss 0.8634\t Accuracy 0.8232\n",
      "Epoch [26][50]\t Batch [150][429]\t Training Loss 0.8804\t Accuracy 0.8164\n",
      "Epoch [26][50]\t Batch [200][429]\t Training Loss 0.8795\t Accuracy 0.8169\n",
      "Epoch [26][50]\t Batch [250][429]\t Training Loss 0.8761\t Accuracy 0.8161\n",
      "Epoch [26][50]\t Batch [300][429]\t Training Loss 0.8824\t Accuracy 0.8131\n",
      "Epoch [26][50]\t Batch [350][429]\t Training Loss 0.8848\t Accuracy 0.8130\n",
      "Epoch [26][50]\t Batch [400][429]\t Training Loss 0.8810\t Accuracy 0.8138\n",
      "\n",
      "Epoch [26]\t Average training loss 0.8805\t Average training accuracy 0.8135\n",
      "Epoch [26]\t Average validation loss 0.7739\t Average validation accuracy 0.8600\n",
      "\n",
      "Epoch [27][50]\t Batch [0][429]\t Training Loss 0.8729\t Accuracy 0.8125\n",
      "Epoch [27][50]\t Batch [50][429]\t Training Loss 0.8509\t Accuracy 0.8243\n",
      "Epoch [27][50]\t Batch [100][429]\t Training Loss 0.8595\t Accuracy 0.8221\n",
      "Epoch [27][50]\t Batch [150][429]\t Training Loss 0.8749\t Accuracy 0.8163\n",
      "Epoch [27][50]\t Batch [200][429]\t Training Loss 0.8751\t Accuracy 0.8162\n",
      "Epoch [27][50]\t Batch [250][429]\t Training Loss 0.8712\t Accuracy 0.8159\n",
      "Epoch [27][50]\t Batch [300][429]\t Training Loss 0.8779\t Accuracy 0.8129\n",
      "Epoch [27][50]\t Batch [350][429]\t Training Loss 0.8806\t Accuracy 0.8127\n",
      "Epoch [27][50]\t Batch [400][429]\t Training Loss 0.8761\t Accuracy 0.8139\n",
      "\n",
      "Epoch [27]\t Average training loss 0.8765\t Average training accuracy 0.8132\n",
      "Epoch [27]\t Average validation loss 0.7682\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [28][50]\t Batch [0][429]\t Training Loss 0.6131\t Accuracy 0.8828\n",
      "Epoch [28][50]\t Batch [50][429]\t Training Loss 0.8433\t Accuracy 0.8248\n",
      "Epoch [28][50]\t Batch [100][429]\t Training Loss 0.8542\t Accuracy 0.8211\n",
      "Epoch [28][50]\t Batch [150][429]\t Training Loss 0.8696\t Accuracy 0.8160\n",
      "Epoch [28][50]\t Batch [200][429]\t Training Loss 0.8705\t Accuracy 0.8158\n",
      "Epoch [28][50]\t Batch [250][429]\t Training Loss 0.8660\t Accuracy 0.8159\n",
      "Epoch [28][50]\t Batch [300][429]\t Training Loss 0.8732\t Accuracy 0.8126\n",
      "Epoch [28][50]\t Batch [350][429]\t Training Loss 0.8756\t Accuracy 0.8126\n",
      "Epoch [28][50]\t Batch [400][429]\t Training Loss 0.8715\t Accuracy 0.8138\n",
      "\n",
      "Epoch [28]\t Average training loss 0.8723\t Average training accuracy 0.8129\n",
      "Epoch [28]\t Average validation loss 0.7653\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [29][50]\t Batch [0][429]\t Training Loss 0.6699\t Accuracy 0.9062\n",
      "Epoch [29][50]\t Batch [50][429]\t Training Loss 0.8386\t Accuracy 0.8260\n",
      "Epoch [29][50]\t Batch [100][429]\t Training Loss 0.8499\t Accuracy 0.8216\n",
      "Epoch [29][50]\t Batch [150][429]\t Training Loss 0.8648\t Accuracy 0.8169\n",
      "Epoch [29][50]\t Batch [200][429]\t Training Loss 0.8664\t Accuracy 0.8157\n",
      "Epoch [29][50]\t Batch [250][429]\t Training Loss 0.8620\t Accuracy 0.8157\n",
      "Epoch [29][50]\t Batch [300][429]\t Training Loss 0.8690\t Accuracy 0.8125\n",
      "Epoch [29][50]\t Batch [350][429]\t Training Loss 0.8716\t Accuracy 0.8126\n",
      "Epoch [29][50]\t Batch [400][429]\t Training Loss 0.8678\t Accuracy 0.8136\n",
      "\n",
      "Epoch [29]\t Average training loss 0.8688\t Average training accuracy 0.8128\n",
      "Epoch [29]\t Average validation loss 0.7607\t Average validation accuracy 0.8584\n",
      "\n",
      "Epoch [30][50]\t Batch [0][429]\t Training Loss 0.7126\t Accuracy 0.8672\n",
      "Epoch [30][50]\t Batch [50][429]\t Training Loss 0.8340\t Accuracy 0.8255\n",
      "Epoch [30][50]\t Batch [100][429]\t Training Loss 0.8460\t Accuracy 0.8218\n",
      "Epoch [30][50]\t Batch [150][429]\t Training Loss 0.8606\t Accuracy 0.8172\n",
      "Epoch [30][50]\t Batch [200][429]\t Training Loss 0.8626\t Accuracy 0.8157\n",
      "Epoch [30][50]\t Batch [250][429]\t Training Loss 0.8585\t Accuracy 0.8158\n",
      "Epoch [30][50]\t Batch [300][429]\t Training Loss 0.8653\t Accuracy 0.8127\n",
      "Epoch [30][50]\t Batch [350][429]\t Training Loss 0.8676\t Accuracy 0.8129\n",
      "Epoch [30][50]\t Batch [400][429]\t Training Loss 0.8641\t Accuracy 0.8138\n",
      "\n",
      "Epoch [30]\t Average training loss 0.8653\t Average training accuracy 0.8130\n",
      "Epoch [30]\t Average validation loss 0.7574\t Average validation accuracy 0.8586\n",
      "\n",
      "Epoch [31][50]\t Batch [0][429]\t Training Loss 0.7728\t Accuracy 0.8281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31][50]\t Batch [50][429]\t Training Loss 0.8321\t Accuracy 0.8249\n",
      "Epoch [31][50]\t Batch [100][429]\t Training Loss 0.8386\t Accuracy 0.8235\n",
      "Epoch [31][50]\t Batch [150][429]\t Training Loss 0.8574\t Accuracy 0.8172\n",
      "Epoch [31][50]\t Batch [200][429]\t Training Loss 0.8589\t Accuracy 0.8162\n",
      "Epoch [31][50]\t Batch [250][429]\t Training Loss 0.8558\t Accuracy 0.8160\n",
      "Epoch [31][50]\t Batch [300][429]\t Training Loss 0.8624\t Accuracy 0.8128\n",
      "Epoch [31][50]\t Batch [350][429]\t Training Loss 0.8638\t Accuracy 0.8131\n",
      "Epoch [31][50]\t Batch [400][429]\t Training Loss 0.8611\t Accuracy 0.8136\n",
      "\n",
      "Epoch [31]\t Average training loss 0.8620\t Average training accuracy 0.8129\n",
      "Epoch [31]\t Average validation loss 0.7540\t Average validation accuracy 0.8582\n",
      "\n",
      "Epoch [32][50]\t Batch [0][429]\t Training Loss 0.8740\t Accuracy 0.8359\n",
      "Epoch [32][50]\t Batch [50][429]\t Training Loss 0.8308\t Accuracy 0.8255\n",
      "Epoch [32][50]\t Batch [100][429]\t Training Loss 0.8368\t Accuracy 0.8235\n",
      "Epoch [32][50]\t Batch [150][429]\t Training Loss 0.8555\t Accuracy 0.8173\n",
      "Epoch [32][50]\t Batch [200][429]\t Training Loss 0.8565\t Accuracy 0.8160\n",
      "Epoch [32][50]\t Batch [250][429]\t Training Loss 0.8531\t Accuracy 0.8161\n",
      "Epoch [32][50]\t Batch [300][429]\t Training Loss 0.8598\t Accuracy 0.8126\n",
      "Epoch [32][50]\t Batch [350][429]\t Training Loss 0.8607\t Accuracy 0.8132\n",
      "Epoch [32][50]\t Batch [400][429]\t Training Loss 0.8584\t Accuracy 0.8137\n",
      "\n",
      "Epoch [32]\t Average training loss 0.8588\t Average training accuracy 0.8133\n",
      "Epoch [32]\t Average validation loss 0.7519\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [33][50]\t Batch [0][429]\t Training Loss 1.0914\t Accuracy 0.7109\n",
      "Epoch [33][50]\t Batch [50][429]\t Training Loss 0.8323\t Accuracy 0.8249\n",
      "Epoch [33][50]\t Batch [100][429]\t Training Loss 0.8363\t Accuracy 0.8235\n",
      "Epoch [33][50]\t Batch [150][429]\t Training Loss 0.8549\t Accuracy 0.8173\n",
      "Epoch [33][50]\t Batch [200][429]\t Training Loss 0.8549\t Accuracy 0.8161\n",
      "Epoch [33][50]\t Batch [250][429]\t Training Loss 0.8514\t Accuracy 0.8159\n",
      "Epoch [33][50]\t Batch [300][429]\t Training Loss 0.8578\t Accuracy 0.8126\n",
      "Epoch [33][50]\t Batch [350][429]\t Training Loss 0.8588\t Accuracy 0.8129\n",
      "Epoch [33][50]\t Batch [400][429]\t Training Loss 0.8566\t Accuracy 0.8134\n",
      "\n",
      "Epoch [33]\t Average training loss 0.8568\t Average training accuracy 0.8130\n",
      "Epoch [33]\t Average validation loss 0.7479\t Average validation accuracy 0.8586\n",
      "\n",
      "Epoch [34][50]\t Batch [0][429]\t Training Loss 1.0186\t Accuracy 0.7344\n",
      "Epoch [34][50]\t Batch [50][429]\t Training Loss 0.8336\t Accuracy 0.8223\n",
      "Epoch [34][50]\t Batch [100][429]\t Training Loss 0.8349\t Accuracy 0.8226\n",
      "Epoch [34][50]\t Batch [150][429]\t Training Loss 0.8531\t Accuracy 0.8166\n",
      "Epoch [34][50]\t Batch [200][429]\t Training Loss 0.8525\t Accuracy 0.8155\n",
      "Epoch [34][50]\t Batch [250][429]\t Training Loss 0.8491\t Accuracy 0.8150\n",
      "Epoch [34][50]\t Batch [300][429]\t Training Loss 0.8553\t Accuracy 0.8121\n",
      "Epoch [34][50]\t Batch [350][429]\t Training Loss 0.8566\t Accuracy 0.8124\n",
      "Epoch [34][50]\t Batch [400][429]\t Training Loss 0.8543\t Accuracy 0.8129\n",
      "\n",
      "Epoch [34]\t Average training loss 0.8547\t Average training accuracy 0.8126\n",
      "Epoch [34]\t Average validation loss 0.7456\t Average validation accuracy 0.8578\n",
      "\n",
      "Epoch [35][50]\t Batch [0][429]\t Training Loss 0.8091\t Accuracy 0.7891\n",
      "Epoch [35][50]\t Batch [50][429]\t Training Loss 0.8314\t Accuracy 0.8212\n",
      "Epoch [35][50]\t Batch [100][429]\t Training Loss 0.8341\t Accuracy 0.8217\n",
      "Epoch [35][50]\t Batch [150][429]\t Training Loss 0.8517\t Accuracy 0.8159\n",
      "Epoch [35][50]\t Batch [200][429]\t Training Loss 0.8498\t Accuracy 0.8155\n",
      "Epoch [35][50]\t Batch [250][429]\t Training Loss 0.8466\t Accuracy 0.8150\n",
      "Epoch [35][50]\t Batch [300][429]\t Training Loss 0.8532\t Accuracy 0.8119\n",
      "Epoch [35][50]\t Batch [350][429]\t Training Loss 0.8544\t Accuracy 0.8121\n",
      "Epoch [35][50]\t Batch [400][429]\t Training Loss 0.8524\t Accuracy 0.8129\n",
      "\n",
      "Epoch [35]\t Average training loss 0.8527\t Average training accuracy 0.8125\n",
      "Epoch [35]\t Average validation loss 0.7434\t Average validation accuracy 0.8576\n",
      "\n",
      "Epoch [36][50]\t Batch [0][429]\t Training Loss 0.7763\t Accuracy 0.8438\n",
      "Epoch [36][50]\t Batch [50][429]\t Training Loss 0.8289\t Accuracy 0.8189\n",
      "Epoch [36][50]\t Batch [100][429]\t Training Loss 0.8320\t Accuracy 0.8211\n",
      "Epoch [36][50]\t Batch [150][429]\t Training Loss 0.8491\t Accuracy 0.8157\n",
      "Epoch [36][50]\t Batch [200][429]\t Training Loss 0.8468\t Accuracy 0.8156\n",
      "Epoch [36][50]\t Batch [250][429]\t Training Loss 0.8446\t Accuracy 0.8147\n",
      "Epoch [36][50]\t Batch [300][429]\t Training Loss 0.8510\t Accuracy 0.8117\n",
      "Epoch [36][50]\t Batch [350][429]\t Training Loss 0.8523\t Accuracy 0.8118\n",
      "Epoch [36][50]\t Batch [400][429]\t Training Loss 0.8505\t Accuracy 0.8127\n",
      "\n",
      "Epoch [36]\t Average training loss 0.8510\t Average training accuracy 0.8121\n",
      "Epoch [36]\t Average validation loss 0.7427\t Average validation accuracy 0.8580\n",
      "\n",
      "Epoch [37][50]\t Batch [0][429]\t Training Loss 0.7076\t Accuracy 0.8750\n",
      "Epoch [37][50]\t Batch [50][429]\t Training Loss 0.8259\t Accuracy 0.8200\n",
      "Epoch [37][50]\t Batch [100][429]\t Training Loss 0.8306\t Accuracy 0.8209\n",
      "Epoch [37][50]\t Batch [150][429]\t Training Loss 0.8458\t Accuracy 0.8159\n",
      "Epoch [37][50]\t Batch [200][429]\t Training Loss 0.8429\t Accuracy 0.8163\n",
      "Epoch [37][50]\t Batch [250][429]\t Training Loss 0.8423\t Accuracy 0.8148\n",
      "Epoch [37][50]\t Batch [300][429]\t Training Loss 0.8490\t Accuracy 0.8116\n",
      "Epoch [37][50]\t Batch [350][429]\t Training Loss 0.8504\t Accuracy 0.8117\n",
      "Epoch [37][50]\t Batch [400][429]\t Training Loss 0.8486\t Accuracy 0.8125\n",
      "\n",
      "Epoch [37]\t Average training loss 0.8492\t Average training accuracy 0.8119\n",
      "Epoch [37]\t Average validation loss 0.7407\t Average validation accuracy 0.8574\n",
      "\n",
      "Epoch [38][50]\t Batch [0][429]\t Training Loss 0.6973\t Accuracy 0.8516\n",
      "Epoch [38][50]\t Batch [50][429]\t Training Loss 0.8240\t Accuracy 0.8189\n",
      "Epoch [38][50]\t Batch [100][429]\t Training Loss 0.8283\t Accuracy 0.8202\n",
      "Epoch [38][50]\t Batch [150][429]\t Training Loss 0.8422\t Accuracy 0.8156\n",
      "Epoch [38][50]\t Batch [200][429]\t Training Loss 0.8407\t Accuracy 0.8159\n",
      "Epoch [38][50]\t Batch [250][429]\t Training Loss 0.8407\t Accuracy 0.8143\n",
      "Epoch [38][50]\t Batch [300][429]\t Training Loss 0.8468\t Accuracy 0.8114\n",
      "Epoch [38][50]\t Batch [350][429]\t Training Loss 0.8486\t Accuracy 0.8113\n",
      "Epoch [38][50]\t Batch [400][429]\t Training Loss 0.8467\t Accuracy 0.8124\n",
      "\n",
      "Epoch [38]\t Average training loss 0.8476\t Average training accuracy 0.8118\n",
      "Epoch [38]\t Average validation loss 0.7396\t Average validation accuracy 0.8578\n",
      "\n",
      "Epoch [39][50]\t Batch [0][429]\t Training Loss 0.7211\t Accuracy 0.8438\n",
      "Epoch [39][50]\t Batch [50][429]\t Training Loss 0.8223\t Accuracy 0.8185\n",
      "Epoch [39][50]\t Batch [100][429]\t Training Loss 0.8261\t Accuracy 0.8208\n",
      "Epoch [39][50]\t Batch [150][429]\t Training Loss 0.8403\t Accuracy 0.8155\n",
      "Epoch [39][50]\t Batch [200][429]\t Training Loss 0.8395\t Accuracy 0.8155\n",
      "Epoch [39][50]\t Batch [250][429]\t Training Loss 0.8391\t Accuracy 0.8141\n",
      "Epoch [39][50]\t Batch [300][429]\t Training Loss 0.8449\t Accuracy 0.8112\n",
      "Epoch [39][50]\t Batch [350][429]\t Training Loss 0.8463\t Accuracy 0.8113\n",
      "Epoch [39][50]\t Batch [400][429]\t Training Loss 0.8451\t Accuracy 0.8122\n",
      "\n",
      "Epoch [39]\t Average training loss 0.8463\t Average training accuracy 0.8114\n",
      "Epoch [39]\t Average validation loss 0.7371\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [40][50]\t Batch [0][429]\t Training Loss 0.6926\t Accuracy 0.8516\n",
      "Epoch [40][50]\t Batch [50][429]\t Training Loss 0.8177\t Accuracy 0.8191\n",
      "Epoch [40][50]\t Batch [100][429]\t Training Loss 0.8240\t Accuracy 0.8201\n",
      "Epoch [40][50]\t Batch [150][429]\t Training Loss 0.8375\t Accuracy 0.8149\n",
      "Epoch [40][50]\t Batch [200][429]\t Training Loss 0.8374\t Accuracy 0.8145\n",
      "Epoch [40][50]\t Batch [250][429]\t Training Loss 0.8371\t Accuracy 0.8136\n",
      "Epoch [40][50]\t Batch [300][429]\t Training Loss 0.8432\t Accuracy 0.8106\n",
      "Epoch [40][50]\t Batch [350][429]\t Training Loss 0.8444\t Accuracy 0.8110\n",
      "Epoch [40][50]\t Batch [400][429]\t Training Loss 0.8437\t Accuracy 0.8116\n",
      "\n",
      "Epoch [40]\t Average training loss 0.8450\t Average training accuracy 0.8109\n",
      "Epoch [40]\t Average validation loss 0.7362\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [41][50]\t Batch [0][429]\t Training Loss 0.6888\t Accuracy 0.8906\n",
      "Epoch [41][50]\t Batch [50][429]\t Training Loss 0.8161\t Accuracy 0.8194\n",
      "Epoch [41][50]\t Batch [100][429]\t Training Loss 0.8220\t Accuracy 0.8208\n",
      "Epoch [41][50]\t Batch [150][429]\t Training Loss 0.8345\t Accuracy 0.8157\n",
      "Epoch [41][50]\t Batch [200][429]\t Training Loss 0.8355\t Accuracy 0.8147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41][50]\t Batch [250][429]\t Training Loss 0.8357\t Accuracy 0.8135\n",
      "Epoch [41][50]\t Batch [300][429]\t Training Loss 0.8415\t Accuracy 0.8107\n",
      "Epoch [41][50]\t Batch [350][429]\t Training Loss 0.8423\t Accuracy 0.8111\n",
      "Epoch [41][50]\t Batch [400][429]\t Training Loss 0.8426\t Accuracy 0.8114\n",
      "\n",
      "Epoch [41]\t Average training loss 0.8437\t Average training accuracy 0.8110\n",
      "Epoch [41]\t Average validation loss 0.7348\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [42][50]\t Batch [0][429]\t Training Loss 0.7297\t Accuracy 0.8359\n",
      "Epoch [42][50]\t Batch [50][429]\t Training Loss 0.8155\t Accuracy 0.8202\n",
      "Epoch [42][50]\t Batch [100][429]\t Training Loss 0.8211\t Accuracy 0.8203\n",
      "Epoch [42][50]\t Batch [150][429]\t Training Loss 0.8341\t Accuracy 0.8147\n",
      "Epoch [42][50]\t Batch [200][429]\t Training Loss 0.8343\t Accuracy 0.8140\n",
      "Epoch [42][50]\t Batch [250][429]\t Training Loss 0.8347\t Accuracy 0.8130\n",
      "Epoch [42][50]\t Batch [300][429]\t Training Loss 0.8401\t Accuracy 0.8103\n",
      "Epoch [42][50]\t Batch [350][429]\t Training Loss 0.8408\t Accuracy 0.8110\n",
      "Epoch [42][50]\t Batch [400][429]\t Training Loss 0.8415\t Accuracy 0.8111\n",
      "\n",
      "Epoch [42]\t Average training loss 0.8424\t Average training accuracy 0.8108\n",
      "Epoch [42]\t Average validation loss 0.7329\t Average validation accuracy 0.8602\n",
      "\n",
      "Epoch [43][50]\t Batch [0][429]\t Training Loss 0.9760\t Accuracy 0.7500\n",
      "Epoch [43][50]\t Batch [50][429]\t Training Loss 0.8178\t Accuracy 0.8183\n",
      "Epoch [43][50]\t Batch [100][429]\t Training Loss 0.8223\t Accuracy 0.8191\n",
      "Epoch [43][50]\t Batch [150][429]\t Training Loss 0.8344\t Accuracy 0.8138\n",
      "Epoch [43][50]\t Batch [200][429]\t Training Loss 0.8345\t Accuracy 0.8136\n",
      "Epoch [43][50]\t Batch [250][429]\t Training Loss 0.8339\t Accuracy 0.8130\n",
      "Epoch [43][50]\t Batch [300][429]\t Training Loss 0.8391\t Accuracy 0.8102\n",
      "Epoch [43][50]\t Batch [350][429]\t Training Loss 0.8404\t Accuracy 0.8106\n",
      "Epoch [43][50]\t Batch [400][429]\t Training Loss 0.8407\t Accuracy 0.8108\n",
      "\n",
      "Epoch [43]\t Average training loss 0.8416\t Average training accuracy 0.8106\n",
      "Epoch [43]\t Average validation loss 0.7326\t Average validation accuracy 0.8602\n",
      "\n",
      "Epoch [44][50]\t Batch [0][429]\t Training Loss 0.9807\t Accuracy 0.7734\n",
      "Epoch [44][50]\t Batch [50][429]\t Training Loss 0.8187\t Accuracy 0.8171\n",
      "Epoch [44][50]\t Batch [100][429]\t Training Loss 0.8227\t Accuracy 0.8176\n",
      "Epoch [44][50]\t Batch [150][429]\t Training Loss 0.8342\t Accuracy 0.8130\n",
      "Epoch [44][50]\t Batch [200][429]\t Training Loss 0.8344\t Accuracy 0.8128\n",
      "Epoch [44][50]\t Batch [250][429]\t Training Loss 0.8334\t Accuracy 0.8124\n",
      "Epoch [44][50]\t Batch [300][429]\t Training Loss 0.8384\t Accuracy 0.8100\n",
      "Epoch [44][50]\t Batch [350][429]\t Training Loss 0.8397\t Accuracy 0.8104\n",
      "Epoch [44][50]\t Batch [400][429]\t Training Loss 0.8398\t Accuracy 0.8104\n",
      "\n",
      "Epoch [44]\t Average training loss 0.8410\t Average training accuracy 0.8103\n",
      "Epoch [44]\t Average validation loss 0.7317\t Average validation accuracy 0.8604\n",
      "\n",
      "Epoch [45][50]\t Batch [0][429]\t Training Loss 0.8420\t Accuracy 0.8125\n",
      "Epoch [45][50]\t Batch [50][429]\t Training Loss 0.8193\t Accuracy 0.8174\n",
      "Epoch [45][50]\t Batch [100][429]\t Training Loss 0.8234\t Accuracy 0.8174\n",
      "Epoch [45][50]\t Batch [150][429]\t Training Loss 0.8335\t Accuracy 0.8127\n",
      "Epoch [45][50]\t Batch [200][429]\t Training Loss 0.8341\t Accuracy 0.8125\n",
      "Epoch [45][50]\t Batch [250][429]\t Training Loss 0.8327\t Accuracy 0.8123\n",
      "Epoch [45][50]\t Batch [300][429]\t Training Loss 0.8375\t Accuracy 0.8100\n",
      "Epoch [45][50]\t Batch [350][429]\t Training Loss 0.8390\t Accuracy 0.8102\n",
      "Epoch [45][50]\t Batch [400][429]\t Training Loss 0.8389\t Accuracy 0.8104\n",
      "\n",
      "Epoch [45]\t Average training loss 0.8405\t Average training accuracy 0.8100\n",
      "Epoch [45]\t Average validation loss 0.7306\t Average validation accuracy 0.8588\n",
      "\n",
      "Epoch [46][50]\t Batch [0][429]\t Training Loss 0.6843\t Accuracy 0.8516\n",
      "Epoch [46][50]\t Batch [50][429]\t Training Loss 0.8160\t Accuracy 0.8180\n",
      "Epoch [46][50]\t Batch [100][429]\t Training Loss 0.8213\t Accuracy 0.8181\n",
      "Epoch [46][50]\t Batch [150][429]\t Training Loss 0.8323\t Accuracy 0.8127\n",
      "Epoch [46][50]\t Batch [200][429]\t Training Loss 0.8332\t Accuracy 0.8126\n",
      "Epoch [46][50]\t Batch [250][429]\t Training Loss 0.8317\t Accuracy 0.8124\n",
      "Epoch [46][50]\t Batch [300][429]\t Training Loss 0.8363\t Accuracy 0.8103\n",
      "Epoch [46][50]\t Batch [350][429]\t Training Loss 0.8383\t Accuracy 0.8102\n",
      "Epoch [46][50]\t Batch [400][429]\t Training Loss 0.8377\t Accuracy 0.8104\n",
      "\n",
      "Epoch [46]\t Average training loss 0.8395\t Average training accuracy 0.8101\n",
      "Epoch [46]\t Average validation loss 0.7300\t Average validation accuracy 0.8590\n",
      "\n",
      "Epoch [47][50]\t Batch [0][429]\t Training Loss 0.7486\t Accuracy 0.8359\n",
      "Epoch [47][50]\t Batch [50][429]\t Training Loss 0.8141\t Accuracy 0.8174\n",
      "Epoch [47][50]\t Batch [100][429]\t Training Loss 0.8196\t Accuracy 0.8180\n",
      "Epoch [47][50]\t Batch [150][429]\t Training Loss 0.8318\t Accuracy 0.8123\n",
      "Epoch [47][50]\t Batch [200][429]\t Training Loss 0.8331\t Accuracy 0.8121\n",
      "Epoch [47][50]\t Batch [250][429]\t Training Loss 0.8315\t Accuracy 0.8117\n",
      "Epoch [47][50]\t Batch [300][429]\t Training Loss 0.8353\t Accuracy 0.8100\n",
      "Epoch [47][50]\t Batch [350][429]\t Training Loss 0.8382\t Accuracy 0.8097\n",
      "Epoch [47][50]\t Batch [400][429]\t Training Loss 0.8372\t Accuracy 0.8101\n",
      "\n",
      "Epoch [47]\t Average training loss 0.8386\t Average training accuracy 0.8098\n",
      "Epoch [47]\t Average validation loss 0.7301\t Average validation accuracy 0.8578\n",
      "\n",
      "Epoch [48][50]\t Batch [0][429]\t Training Loss 1.0233\t Accuracy 0.7578\n",
      "Epoch [48][50]\t Batch [50][429]\t Training Loss 0.8179\t Accuracy 0.8148\n",
      "Epoch [48][50]\t Batch [100][429]\t Training Loss 0.8190\t Accuracy 0.8170\n",
      "Epoch [48][50]\t Batch [150][429]\t Training Loss 0.8320\t Accuracy 0.8112\n",
      "Epoch [48][50]\t Batch [200][429]\t Training Loss 0.8336\t Accuracy 0.8109\n",
      "Epoch [48][50]\t Batch [250][429]\t Training Loss 0.8319\t Accuracy 0.8108\n",
      "Epoch [48][50]\t Batch [300][429]\t Training Loss 0.8355\t Accuracy 0.8091\n",
      "Epoch [48][50]\t Batch [350][429]\t Training Loss 0.8383\t Accuracy 0.8088\n",
      "Epoch [48][50]\t Batch [400][429]\t Training Loss 0.8373\t Accuracy 0.8092\n",
      "\n",
      "Epoch [48]\t Average training loss 0.8383\t Average training accuracy 0.8093\n",
      "Epoch [48]\t Average validation loss 0.7295\t Average validation accuracy 0.8580\n",
      "\n",
      "Epoch [49][50]\t Batch [0][429]\t Training Loss 0.9805\t Accuracy 0.7500\n",
      "Epoch [49][50]\t Batch [50][429]\t Training Loss 0.8212\t Accuracy 0.8145\n",
      "Epoch [49][50]\t Batch [100][429]\t Training Loss 0.8179\t Accuracy 0.8185\n",
      "Epoch [49][50]\t Batch [150][429]\t Training Loss 0.8329\t Accuracy 0.8106\n",
      "Epoch [49][50]\t Batch [200][429]\t Training Loss 0.8336\t Accuracy 0.8106\n",
      "Epoch [49][50]\t Batch [250][429]\t Training Loss 0.8323\t Accuracy 0.8108\n",
      "Epoch [49][50]\t Batch [300][429]\t Training Loss 0.8360\t Accuracy 0.8088\n",
      "Epoch [49][50]\t Batch [350][429]\t Training Loss 0.8382\t Accuracy 0.8086\n",
      "Epoch [49][50]\t Batch [400][429]\t Training Loss 0.8373\t Accuracy 0.8091\n",
      "\n",
      "Epoch [49]\t Average training loss 0.8383\t Average training accuracy 0.8091\n",
      "Epoch [49]\t Average validation loss 0.7286\t Average validation accuracy 0.8576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:47:41.644776Z",
     "start_time": "2023-11-10T08:47:36.748195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8215.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 使用Softmax交叉熵损失和ReLU激活函数训练多层感知机\n",
    "训练带有一个隐含层且神经元个数为128的多层感知机，使用Softmax交叉熵损失和ReLU激活函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:47:41.658211Z",
     "start_time": "2023-11-10T08:47:41.646173Z"
    }
   },
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# 使用FCLayer和SigmoidLayer构建多层感知机\n",
    "# 128为隐含层的神经元数目\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:49:00.723315Z",
     "start_time": "2023-11-10T08:47:41.660383Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][50]\t Batch [0][429]\t Training Loss 2.4462\t Accuracy 0.0703\n",
      "Epoch [0][50]\t Batch [50][429]\t Training Loss 2.1443\t Accuracy 0.2600\n",
      "Epoch [0][50]\t Batch [100][429]\t Training Loss 1.8819\t Accuracy 0.4253\n",
      "Epoch [0][50]\t Batch [150][429]\t Training Loss 1.6924\t Accuracy 0.5126\n",
      "Epoch [0][50]\t Batch [200][429]\t Training Loss 1.5210\t Accuracy 0.5787\n",
      "Epoch [0][50]\t Batch [250][429]\t Training Loss 1.4002\t Accuracy 0.6208\n",
      "Epoch [0][50]\t Batch [300][429]\t Training Loss 1.2983\t Accuracy 0.6524\n",
      "Epoch [0][50]\t Batch [350][429]\t Training Loss 1.2161\t Accuracy 0.6770\n",
      "Epoch [0][50]\t Batch [400][429]\t Training Loss 1.1481\t Accuracy 0.6963\n",
      "\n",
      "Epoch [0]\t Average training loss 1.1121\t Average training accuracy 0.7066\n",
      "Epoch [0]\t Average validation loss 0.5220\t Average validation accuracy 0.8844\n",
      "\n",
      "Epoch [1][50]\t Batch [0][429]\t Training Loss 0.8465\t Accuracy 0.7344\n",
      "Epoch [1][50]\t Batch [50][429]\t Training Loss 0.5638\t Accuracy 0.8581\n",
      "Epoch [1][50]\t Batch [100][429]\t Training Loss 0.5644\t Accuracy 0.8567\n",
      "Epoch [1][50]\t Batch [150][429]\t Training Loss 0.5699\t Accuracy 0.8528\n",
      "Epoch [1][50]\t Batch [200][429]\t Training Loss 0.5520\t Accuracy 0.8570\n",
      "Epoch [1][50]\t Batch [250][429]\t Training Loss 0.5476\t Accuracy 0.8581\n",
      "Epoch [1][50]\t Batch [300][429]\t Training Loss 0.5378\t Accuracy 0.8598\n",
      "Epoch [1][50]\t Batch [350][429]\t Training Loss 0.5296\t Accuracy 0.8618\n",
      "Epoch [1][50]\t Batch [400][429]\t Training Loss 0.5241\t Accuracy 0.8626\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5185\t Average training accuracy 0.8637\n",
      "Epoch [1]\t Average validation loss 0.3688\t Average validation accuracy 0.9093\n",
      "\n",
      "Epoch [2][50]\t Batch [0][429]\t Training Loss 0.6542\t Accuracy 0.8047\n",
      "Epoch [2][50]\t Batch [50][429]\t Training Loss 0.4217\t Accuracy 0.8874\n",
      "Epoch [2][50]\t Batch [100][429]\t Training Loss 0.4305\t Accuracy 0.8847\n",
      "Epoch [2][50]\t Batch [150][429]\t Training Loss 0.4419\t Accuracy 0.8806\n",
      "Epoch [2][50]\t Batch [200][429]\t Training Loss 0.4331\t Accuracy 0.8830\n",
      "Epoch [2][50]\t Batch [250][429]\t Training Loss 0.4354\t Accuracy 0.8824\n",
      "Epoch [2][50]\t Batch [300][429]\t Training Loss 0.4312\t Accuracy 0.8835\n",
      "Epoch [2][50]\t Batch [350][429]\t Training Loss 0.4283\t Accuracy 0.8847\n",
      "Epoch [2][50]\t Batch [400][429]\t Training Loss 0.4275\t Accuracy 0.8843\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4250\t Average training accuracy 0.8850\n",
      "Epoch [2]\t Average validation loss 0.3175\t Average validation accuracy 0.9185\n",
      "\n",
      "Epoch [3][50]\t Batch [0][429]\t Training Loss 0.3308\t Accuracy 0.8906\n",
      "Epoch [3][50]\t Batch [50][429]\t Training Loss 0.3683\t Accuracy 0.8998\n",
      "Epoch [3][50]\t Batch [100][429]\t Training Loss 0.3789\t Accuracy 0.8973\n",
      "Epoch [3][50]\t Batch [150][429]\t Training Loss 0.3900\t Accuracy 0.8934\n",
      "Epoch [3][50]\t Batch [200][429]\t Training Loss 0.3839\t Accuracy 0.8949\n",
      "Epoch [3][50]\t Batch [250][429]\t Training Loss 0.3878\t Accuracy 0.8938\n",
      "Epoch [3][50]\t Batch [300][429]\t Training Loss 0.3852\t Accuracy 0.8945\n",
      "Epoch [3][50]\t Batch [350][429]\t Training Loss 0.3839\t Accuracy 0.8952\n",
      "Epoch [3][50]\t Batch [400][429]\t Training Loss 0.3845\t Accuracy 0.8947\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3830\t Average training accuracy 0.8950\n",
      "Epoch [3]\t Average validation loss 0.2907\t Average validation accuracy 0.9245\n",
      "\n",
      "Epoch [4][50]\t Batch [0][429]\t Training Loss 0.2156\t Accuracy 0.9375\n",
      "Epoch [4][50]\t Batch [50][429]\t Training Loss 0.3389\t Accuracy 0.9095\n",
      "Epoch [4][50]\t Batch [100][429]\t Training Loss 0.3472\t Accuracy 0.9066\n",
      "Epoch [4][50]\t Batch [150][429]\t Training Loss 0.3600\t Accuracy 0.9019\n",
      "Epoch [4][50]\t Batch [200][429]\t Training Loss 0.3557\t Accuracy 0.9027\n",
      "Epoch [4][50]\t Batch [250][429]\t Training Loss 0.3597\t Accuracy 0.9012\n",
      "Epoch [4][50]\t Batch [300][429]\t Training Loss 0.3579\t Accuracy 0.9017\n",
      "Epoch [4][50]\t Batch [350][429]\t Training Loss 0.3574\t Accuracy 0.9017\n",
      "Epoch [4][50]\t Batch [400][429]\t Training Loss 0.3586\t Accuracy 0.9014\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3574\t Average training accuracy 0.9017\n",
      "Epoch [4]\t Average validation loss 0.2743\t Average validation accuracy 0.9289\n",
      "\n",
      "Epoch [5][50]\t Batch [0][429]\t Training Loss 0.2943\t Accuracy 0.9062\n",
      "Epoch [5][50]\t Batch [50][429]\t Training Loss 0.3193\t Accuracy 0.9128\n",
      "Epoch [5][50]\t Batch [100][429]\t Training Loss 0.3271\t Accuracy 0.9104\n",
      "Epoch [5][50]\t Batch [150][429]\t Training Loss 0.3405\t Accuracy 0.9062\n",
      "Epoch [5][50]\t Batch [200][429]\t Training Loss 0.3364\t Accuracy 0.9071\n",
      "Epoch [5][50]\t Batch [250][429]\t Training Loss 0.3403\t Accuracy 0.9061\n",
      "Epoch [5][50]\t Batch [300][429]\t Training Loss 0.3395\t Accuracy 0.9061\n",
      "Epoch [5][50]\t Batch [350][429]\t Training Loss 0.3393\t Accuracy 0.9061\n",
      "Epoch [5][50]\t Batch [400][429]\t Training Loss 0.3408\t Accuracy 0.9056\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3397\t Average training accuracy 0.9059\n",
      "Epoch [5]\t Average validation loss 0.2622\t Average validation accuracy 0.9323\n",
      "\n",
      "Epoch [6][50]\t Batch [0][429]\t Training Loss 0.3354\t Accuracy 0.8984\n",
      "Epoch [6][50]\t Batch [50][429]\t Training Loss 0.3075\t Accuracy 0.9162\n",
      "Epoch [6][50]\t Batch [100][429]\t Training Loss 0.3134\t Accuracy 0.9148\n",
      "Epoch [6][50]\t Batch [150][429]\t Training Loss 0.3261\t Accuracy 0.9113\n",
      "Epoch [6][50]\t Batch [200][429]\t Training Loss 0.3224\t Accuracy 0.9124\n",
      "Epoch [6][50]\t Batch [250][429]\t Training Loss 0.3260\t Accuracy 0.9112\n",
      "Epoch [6][50]\t Batch [300][429]\t Training Loss 0.3259\t Accuracy 0.9108\n",
      "Epoch [6][50]\t Batch [350][429]\t Training Loss 0.3255\t Accuracy 0.9109\n",
      "Epoch [6][50]\t Batch [400][429]\t Training Loss 0.3275\t Accuracy 0.9102\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3266\t Average training accuracy 0.9104\n",
      "Epoch [6]\t Average validation loss 0.2532\t Average validation accuracy 0.9335\n",
      "\n",
      "Epoch [7][50]\t Batch [0][429]\t Training Loss 0.2297\t Accuracy 0.9453\n",
      "Epoch [7][50]\t Batch [50][429]\t Training Loss 0.2967\t Accuracy 0.9190\n",
      "Epoch [7][50]\t Batch [100][429]\t Training Loss 0.3021\t Accuracy 0.9186\n",
      "Epoch [7][50]\t Batch [150][429]\t Training Loss 0.3147\t Accuracy 0.9150\n",
      "Epoch [7][50]\t Batch [200][429]\t Training Loss 0.3113\t Accuracy 0.9157\n",
      "Epoch [7][50]\t Batch [250][429]\t Training Loss 0.3149\t Accuracy 0.9143\n",
      "Epoch [7][50]\t Batch [300][429]\t Training Loss 0.3147\t Accuracy 0.9140\n",
      "Epoch [7][50]\t Batch [350][429]\t Training Loss 0.3144\t Accuracy 0.9140\n",
      "Epoch [7][50]\t Batch [400][429]\t Training Loss 0.3167\t Accuracy 0.9132\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3160\t Average training accuracy 0.9134\n",
      "Epoch [7]\t Average validation loss 0.2459\t Average validation accuracy 0.9353\n",
      "\n",
      "Epoch [8][50]\t Batch [0][429]\t Training Loss 0.2456\t Accuracy 0.9453\n",
      "Epoch [8][50]\t Batch [50][429]\t Training Loss 0.2866\t Accuracy 0.9214\n",
      "Epoch [8][50]\t Batch [100][429]\t Training Loss 0.2934\t Accuracy 0.9210\n",
      "Epoch [8][50]\t Batch [150][429]\t Training Loss 0.3055\t Accuracy 0.9174\n",
      "Epoch [8][50]\t Batch [200][429]\t Training Loss 0.3025\t Accuracy 0.9179\n",
      "Epoch [8][50]\t Batch [250][429]\t Training Loss 0.3060\t Accuracy 0.9170\n",
      "Epoch [8][50]\t Batch [300][429]\t Training Loss 0.3059\t Accuracy 0.9167\n",
      "Epoch [8][50]\t Batch [350][429]\t Training Loss 0.3053\t Accuracy 0.9169\n",
      "Epoch [8][50]\t Batch [400][429]\t Training Loss 0.3080\t Accuracy 0.9160\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3074\t Average training accuracy 0.9161\n",
      "Epoch [8]\t Average validation loss 0.2397\t Average validation accuracy 0.9381\n",
      "\n",
      "Epoch [9][50]\t Batch [0][429]\t Training Loss 0.2161\t Accuracy 0.9531\n",
      "Epoch [9][50]\t Batch [50][429]\t Training Loss 0.2783\t Accuracy 0.9248\n",
      "Epoch [9][50]\t Batch [100][429]\t Training Loss 0.2848\t Accuracy 0.9236\n",
      "Epoch [9][50]\t Batch [150][429]\t Training Loss 0.2976\t Accuracy 0.9196\n",
      "Epoch [9][50]\t Batch [200][429]\t Training Loss 0.2944\t Accuracy 0.9207\n",
      "Epoch [9][50]\t Batch [250][429]\t Training Loss 0.2977\t Accuracy 0.9198\n",
      "Epoch [9][50]\t Batch [300][429]\t Training Loss 0.2981\t Accuracy 0.9190\n",
      "Epoch [9][50]\t Batch [350][429]\t Training Loss 0.2978\t Accuracy 0.9192\n",
      "Epoch [9][50]\t Batch [400][429]\t Training Loss 0.3004\t Accuracy 0.9183\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3000\t Average training accuracy 0.9183\n",
      "Epoch [9]\t Average validation loss 0.2354\t Average validation accuracy 0.9391\n",
      "\n",
      "Epoch [10][50]\t Batch [0][429]\t Training Loss 0.2433\t Accuracy 0.9219\n",
      "Epoch [10][50]\t Batch [50][429]\t Training Loss 0.2725\t Accuracy 0.9265\n",
      "Epoch [10][50]\t Batch [100][429]\t Training Loss 0.2787\t Accuracy 0.9252\n",
      "Epoch [10][50]\t Batch [150][429]\t Training Loss 0.2907\t Accuracy 0.9215\n",
      "Epoch [10][50]\t Batch [200][429]\t Training Loss 0.2878\t Accuracy 0.9224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10][50]\t Batch [250][429]\t Training Loss 0.2908\t Accuracy 0.9218\n",
      "Epoch [10][50]\t Batch [300][429]\t Training Loss 0.2916\t Accuracy 0.9211\n",
      "Epoch [10][50]\t Batch [350][429]\t Training Loss 0.2914\t Accuracy 0.9212\n",
      "Epoch [10][50]\t Batch [400][429]\t Training Loss 0.2940\t Accuracy 0.9203\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2936\t Average training accuracy 0.9204\n",
      "Epoch [10]\t Average validation loss 0.2288\t Average validation accuracy 0.9411\n",
      "\n",
      "Epoch [11][50]\t Batch [0][429]\t Training Loss 0.2684\t Accuracy 0.9297\n",
      "Epoch [11][50]\t Batch [50][429]\t Training Loss 0.2683\t Accuracy 0.9280\n",
      "Epoch [11][50]\t Batch [100][429]\t Training Loss 0.2744\t Accuracy 0.9263\n",
      "Epoch [11][50]\t Batch [150][429]\t Training Loss 0.2855\t Accuracy 0.9224\n",
      "Epoch [11][50]\t Batch [200][429]\t Training Loss 0.2822\t Accuracy 0.9235\n",
      "Epoch [11][50]\t Batch [250][429]\t Training Loss 0.2851\t Accuracy 0.9230\n",
      "Epoch [11][50]\t Batch [300][429]\t Training Loss 0.2862\t Accuracy 0.9225\n",
      "Epoch [11][50]\t Batch [350][429]\t Training Loss 0.2860\t Accuracy 0.9226\n",
      "Epoch [11][50]\t Batch [400][429]\t Training Loss 0.2880\t Accuracy 0.9219\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2880\t Average training accuracy 0.9219\n",
      "Epoch [11]\t Average validation loss 0.2272\t Average validation accuracy 0.9411\n",
      "\n",
      "Epoch [12][50]\t Batch [0][429]\t Training Loss 0.4136\t Accuracy 0.8750\n",
      "Epoch [12][50]\t Batch [50][429]\t Training Loss 0.2658\t Accuracy 0.9292\n",
      "Epoch [12][50]\t Batch [100][429]\t Training Loss 0.2708\t Accuracy 0.9273\n",
      "Epoch [12][50]\t Batch [150][429]\t Training Loss 0.2812\t Accuracy 0.9236\n",
      "Epoch [12][50]\t Batch [200][429]\t Training Loss 0.2775\t Accuracy 0.9253\n",
      "Epoch [12][50]\t Batch [250][429]\t Training Loss 0.2803\t Accuracy 0.9246\n",
      "Epoch [12][50]\t Batch [300][429]\t Training Loss 0.2813\t Accuracy 0.9242\n",
      "Epoch [12][50]\t Batch [350][429]\t Training Loss 0.2812\t Accuracy 0.9241\n",
      "Epoch [12][50]\t Batch [400][429]\t Training Loss 0.2831\t Accuracy 0.9234\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2832\t Average training accuracy 0.9234\n",
      "Epoch [12]\t Average validation loss 0.2237\t Average validation accuracy 0.9431\n",
      "\n",
      "Epoch [13][50]\t Batch [0][429]\t Training Loss 0.3631\t Accuracy 0.9062\n",
      "Epoch [13][50]\t Batch [50][429]\t Training Loss 0.2633\t Accuracy 0.9309\n",
      "Epoch [13][50]\t Batch [100][429]\t Training Loss 0.2670\t Accuracy 0.9291\n",
      "Epoch [13][50]\t Batch [150][429]\t Training Loss 0.2773\t Accuracy 0.9255\n",
      "Epoch [13][50]\t Batch [200][429]\t Training Loss 0.2728\t Accuracy 0.9272\n",
      "Epoch [13][50]\t Batch [250][429]\t Training Loss 0.2760\t Accuracy 0.9261\n",
      "Epoch [13][50]\t Batch [300][429]\t Training Loss 0.2763\t Accuracy 0.9259\n",
      "Epoch [13][50]\t Batch [350][429]\t Training Loss 0.2768\t Accuracy 0.9258\n",
      "Epoch [13][50]\t Batch [400][429]\t Training Loss 0.2785\t Accuracy 0.9251\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2790\t Average training accuracy 0.9250\n",
      "Epoch [13]\t Average validation loss 0.2207\t Average validation accuracy 0.9435\n",
      "\n",
      "Epoch [14][50]\t Batch [0][429]\t Training Loss 0.2625\t Accuracy 0.9297\n",
      "Epoch [14][50]\t Batch [50][429]\t Training Loss 0.2572\t Accuracy 0.9337\n",
      "Epoch [14][50]\t Batch [100][429]\t Training Loss 0.2610\t Accuracy 0.9314\n",
      "Epoch [14][50]\t Batch [150][429]\t Training Loss 0.2733\t Accuracy 0.9269\n",
      "Epoch [14][50]\t Batch [200][429]\t Training Loss 0.2685\t Accuracy 0.9287\n",
      "Epoch [14][50]\t Batch [250][429]\t Training Loss 0.2716\t Accuracy 0.9277\n",
      "Epoch [14][50]\t Batch [300][429]\t Training Loss 0.2717\t Accuracy 0.9275\n",
      "Epoch [14][50]\t Batch [350][429]\t Training Loss 0.2728\t Accuracy 0.9273\n",
      "Epoch [14][50]\t Batch [400][429]\t Training Loss 0.2745\t Accuracy 0.9266\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2749\t Average training accuracy 0.9265\n",
      "Epoch [14]\t Average validation loss 0.2178\t Average validation accuracy 0.9445\n",
      "\n",
      "Epoch [15][50]\t Batch [0][429]\t Training Loss 0.2540\t Accuracy 0.9219\n",
      "Epoch [15][50]\t Batch [50][429]\t Training Loss 0.2554\t Accuracy 0.9335\n",
      "Epoch [15][50]\t Batch [100][429]\t Training Loss 0.2570\t Accuracy 0.9326\n",
      "Epoch [15][50]\t Batch [150][429]\t Training Loss 0.2700\t Accuracy 0.9279\n",
      "Epoch [15][50]\t Batch [200][429]\t Training Loss 0.2650\t Accuracy 0.9295\n",
      "Epoch [15][50]\t Batch [250][429]\t Training Loss 0.2676\t Accuracy 0.9288\n",
      "Epoch [15][50]\t Batch [300][429]\t Training Loss 0.2679\t Accuracy 0.9286\n",
      "Epoch [15][50]\t Batch [350][429]\t Training Loss 0.2692\t Accuracy 0.9283\n",
      "Epoch [15][50]\t Batch [400][429]\t Training Loss 0.2709\t Accuracy 0.9277\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2713\t Average training accuracy 0.9275\n",
      "Epoch [15]\t Average validation loss 0.2152\t Average validation accuracy 0.9449\n",
      "\n",
      "Epoch [16][50]\t Batch [0][429]\t Training Loss 0.2297\t Accuracy 0.9297\n",
      "Epoch [16][50]\t Batch [50][429]\t Training Loss 0.2497\t Accuracy 0.9344\n",
      "Epoch [16][50]\t Batch [100][429]\t Training Loss 0.2523\t Accuracy 0.9332\n",
      "Epoch [16][50]\t Batch [150][429]\t Training Loss 0.2664\t Accuracy 0.9282\n",
      "Epoch [16][50]\t Batch [200][429]\t Training Loss 0.2618\t Accuracy 0.9300\n",
      "Epoch [16][50]\t Batch [250][429]\t Training Loss 0.2641\t Accuracy 0.9296\n",
      "Epoch [16][50]\t Batch [300][429]\t Training Loss 0.2643\t Accuracy 0.9296\n",
      "Epoch [16][50]\t Batch [350][429]\t Training Loss 0.2659\t Accuracy 0.9292\n",
      "Epoch [16][50]\t Batch [400][429]\t Training Loss 0.2674\t Accuracy 0.9285\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2677\t Average training accuracy 0.9285\n",
      "Epoch [16]\t Average validation loss 0.2126\t Average validation accuracy 0.9459\n",
      "\n",
      "Epoch [17][50]\t Batch [0][429]\t Training Loss 0.3156\t Accuracy 0.9141\n",
      "Epoch [17][50]\t Batch [50][429]\t Training Loss 0.2487\t Accuracy 0.9361\n",
      "Epoch [17][50]\t Batch [100][429]\t Training Loss 0.2504\t Accuracy 0.9349\n",
      "Epoch [17][50]\t Batch [150][429]\t Training Loss 0.2639\t Accuracy 0.9297\n",
      "Epoch [17][50]\t Batch [200][429]\t Training Loss 0.2589\t Accuracy 0.9314\n",
      "Epoch [17][50]\t Batch [250][429]\t Training Loss 0.2611\t Accuracy 0.9308\n",
      "Epoch [17][50]\t Batch [300][429]\t Training Loss 0.2613\t Accuracy 0.9309\n",
      "Epoch [17][50]\t Batch [350][429]\t Training Loss 0.2632\t Accuracy 0.9302\n",
      "Epoch [17][50]\t Batch [400][429]\t Training Loss 0.2641\t Accuracy 0.9295\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2649\t Average training accuracy 0.9293\n",
      "Epoch [17]\t Average validation loss 0.2103\t Average validation accuracy 0.9465\n",
      "\n",
      "Epoch [18][50]\t Batch [0][429]\t Training Loss 0.1963\t Accuracy 0.9531\n",
      "Epoch [18][50]\t Batch [50][429]\t Training Loss 0.2439\t Accuracy 0.9363\n",
      "Epoch [18][50]\t Batch [100][429]\t Training Loss 0.2476\t Accuracy 0.9356\n",
      "Epoch [18][50]\t Batch [150][429]\t Training Loss 0.2601\t Accuracy 0.9310\n",
      "Epoch [18][50]\t Batch [200][429]\t Training Loss 0.2558\t Accuracy 0.9323\n",
      "Epoch [18][50]\t Batch [250][429]\t Training Loss 0.2578\t Accuracy 0.9318\n",
      "Epoch [18][50]\t Batch [300][429]\t Training Loss 0.2585\t Accuracy 0.9317\n",
      "Epoch [18][50]\t Batch [350][429]\t Training Loss 0.2604\t Accuracy 0.9310\n",
      "Epoch [18][50]\t Batch [400][429]\t Training Loss 0.2612\t Accuracy 0.9304\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2622\t Average training accuracy 0.9301\n",
      "Epoch [18]\t Average validation loss 0.2083\t Average validation accuracy 0.9477\n",
      "\n",
      "Epoch [19][50]\t Batch [0][429]\t Training Loss 0.0927\t Accuracy 0.9922\n",
      "Epoch [19][50]\t Batch [50][429]\t Training Loss 0.2402\t Accuracy 0.9386\n",
      "Epoch [19][50]\t Batch [100][429]\t Training Loss 0.2446\t Accuracy 0.9366\n",
      "Epoch [19][50]\t Batch [150][429]\t Training Loss 0.2563\t Accuracy 0.9322\n",
      "Epoch [19][50]\t Batch [200][429]\t Training Loss 0.2529\t Accuracy 0.9332\n",
      "Epoch [19][50]\t Batch [250][429]\t Training Loss 0.2546\t Accuracy 0.9328\n",
      "Epoch [19][50]\t Batch [300][429]\t Training Loss 0.2553\t Accuracy 0.9327\n",
      "Epoch [19][50]\t Batch [350][429]\t Training Loss 0.2576\t Accuracy 0.9318\n",
      "Epoch [19][50]\t Batch [400][429]\t Training Loss 0.2581\t Accuracy 0.9312\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2593\t Average training accuracy 0.9309\n",
      "Epoch [19]\t Average validation loss 0.2065\t Average validation accuracy 0.9479\n",
      "\n",
      "Epoch [20][50]\t Batch [0][429]\t Training Loss 0.1514\t Accuracy 0.9609\n",
      "Epoch [20][50]\t Batch [50][429]\t Training Loss 0.2371\t Accuracy 0.9395\n",
      "Epoch [20][50]\t Batch [100][429]\t Training Loss 0.2416\t Accuracy 0.9374\n",
      "Epoch [20][50]\t Batch [150][429]\t Training Loss 0.2532\t Accuracy 0.9326\n",
      "Epoch [20][50]\t Batch [200][429]\t Training Loss 0.2501\t Accuracy 0.9337\n",
      "Epoch [20][50]\t Batch [250][429]\t Training Loss 0.2521\t Accuracy 0.9334\n",
      "Epoch [20][50]\t Batch [300][429]\t Training Loss 0.2527\t Accuracy 0.9332\n",
      "Epoch [20][50]\t Batch [350][429]\t Training Loss 0.2549\t Accuracy 0.9324\n",
      "Epoch [20][50]\t Batch [400][429]\t Training Loss 0.2554\t Accuracy 0.9319\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2567\t Average training accuracy 0.9315\n",
      "Epoch [20]\t Average validation loss 0.2048\t Average validation accuracy 0.9481\n",
      "\n",
      "Epoch [21][50]\t Batch [0][429]\t Training Loss 0.2641\t Accuracy 0.9453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21][50]\t Batch [50][429]\t Training Loss 0.2361\t Accuracy 0.9396\n",
      "Epoch [21][50]\t Batch [100][429]\t Training Loss 0.2405\t Accuracy 0.9376\n",
      "Epoch [21][50]\t Batch [150][429]\t Training Loss 0.2505\t Accuracy 0.9331\n",
      "Epoch [21][50]\t Batch [200][429]\t Training Loss 0.2478\t Accuracy 0.9339\n",
      "Epoch [21][50]\t Batch [250][429]\t Training Loss 0.2500\t Accuracy 0.9338\n",
      "Epoch [21][50]\t Batch [300][429]\t Training Loss 0.2505\t Accuracy 0.9335\n",
      "Epoch [21][50]\t Batch [350][429]\t Training Loss 0.2526\t Accuracy 0.9327\n",
      "Epoch [21][50]\t Batch [400][429]\t Training Loss 0.2525\t Accuracy 0.9324\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2545\t Average training accuracy 0.9318\n",
      "Epoch [21]\t Average validation loss 0.2030\t Average validation accuracy 0.9485\n",
      "\n",
      "Epoch [22][50]\t Batch [0][429]\t Training Loss 0.2231\t Accuracy 0.9453\n",
      "Epoch [22][50]\t Batch [50][429]\t Training Loss 0.2329\t Accuracy 0.9403\n",
      "Epoch [22][50]\t Batch [100][429]\t Training Loss 0.2378\t Accuracy 0.9380\n",
      "Epoch [22][50]\t Batch [150][429]\t Training Loss 0.2477\t Accuracy 0.9338\n",
      "Epoch [22][50]\t Batch [200][429]\t Training Loss 0.2452\t Accuracy 0.9349\n",
      "Epoch [22][50]\t Batch [250][429]\t Training Loss 0.2476\t Accuracy 0.9345\n",
      "Epoch [22][50]\t Batch [300][429]\t Training Loss 0.2482\t Accuracy 0.9342\n",
      "Epoch [22][50]\t Batch [350][429]\t Training Loss 0.2501\t Accuracy 0.9335\n",
      "Epoch [22][50]\t Batch [400][429]\t Training Loss 0.2501\t Accuracy 0.9332\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2521\t Average training accuracy 0.9327\n",
      "Epoch [22]\t Average validation loss 0.2014\t Average validation accuracy 0.9487\n",
      "\n",
      "Epoch [23][50]\t Batch [0][429]\t Training Loss 0.2203\t Accuracy 0.9219\n",
      "Epoch [23][50]\t Batch [50][429]\t Training Loss 0.2330\t Accuracy 0.9403\n",
      "Epoch [23][50]\t Batch [100][429]\t Training Loss 0.2366\t Accuracy 0.9387\n",
      "Epoch [23][50]\t Batch [150][429]\t Training Loss 0.2463\t Accuracy 0.9342\n",
      "Epoch [23][50]\t Batch [200][429]\t Training Loss 0.2430\t Accuracy 0.9356\n",
      "Epoch [23][50]\t Batch [250][429]\t Training Loss 0.2452\t Accuracy 0.9353\n",
      "Epoch [23][50]\t Batch [300][429]\t Training Loss 0.2464\t Accuracy 0.9348\n",
      "Epoch [23][50]\t Batch [350][429]\t Training Loss 0.2478\t Accuracy 0.9342\n",
      "Epoch [23][50]\t Batch [400][429]\t Training Loss 0.2483\t Accuracy 0.9337\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2495\t Average training accuracy 0.9335\n",
      "Epoch [23]\t Average validation loss 0.1995\t Average validation accuracy 0.9493\n",
      "\n",
      "Epoch [24][50]\t Batch [0][429]\t Training Loss 0.4864\t Accuracy 0.8672\n",
      "Epoch [24][50]\t Batch [50][429]\t Training Loss 0.2359\t Accuracy 0.9398\n",
      "Epoch [24][50]\t Batch [100][429]\t Training Loss 0.2376\t Accuracy 0.9387\n",
      "Epoch [24][50]\t Batch [150][429]\t Training Loss 0.2458\t Accuracy 0.9347\n",
      "Epoch [24][50]\t Batch [200][429]\t Training Loss 0.2421\t Accuracy 0.9363\n",
      "Epoch [24][50]\t Batch [250][429]\t Training Loss 0.2440\t Accuracy 0.9358\n",
      "Epoch [24][50]\t Batch [300][429]\t Training Loss 0.2450\t Accuracy 0.9353\n",
      "Epoch [24][50]\t Batch [350][429]\t Training Loss 0.2464\t Accuracy 0.9347\n",
      "Epoch [24][50]\t Batch [400][429]\t Training Loss 0.2469\t Accuracy 0.9343\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2479\t Average training accuracy 0.9341\n",
      "Epoch [24]\t Average validation loss 0.1983\t Average validation accuracy 0.9505\n",
      "\n",
      "Epoch [25][50]\t Batch [0][429]\t Training Loss 0.3685\t Accuracy 0.8828\n",
      "Epoch [25][50]\t Batch [50][429]\t Training Loss 0.2363\t Accuracy 0.9389\n",
      "Epoch [25][50]\t Batch [100][429]\t Training Loss 0.2360\t Accuracy 0.9388\n",
      "Epoch [25][50]\t Batch [150][429]\t Training Loss 0.2442\t Accuracy 0.9348\n",
      "Epoch [25][50]\t Batch [200][429]\t Training Loss 0.2408\t Accuracy 0.9367\n",
      "Epoch [25][50]\t Batch [250][429]\t Training Loss 0.2421\t Accuracy 0.9364\n",
      "Epoch [25][50]\t Batch [300][429]\t Training Loss 0.2433\t Accuracy 0.9359\n",
      "Epoch [25][50]\t Batch [350][429]\t Training Loss 0.2447\t Accuracy 0.9353\n",
      "Epoch [25][50]\t Batch [400][429]\t Training Loss 0.2451\t Accuracy 0.9349\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2460\t Average training accuracy 0.9348\n",
      "Epoch [25]\t Average validation loss 0.1968\t Average validation accuracy 0.9521\n",
      "\n",
      "Epoch [26][50]\t Batch [0][429]\t Training Loss 0.3453\t Accuracy 0.8750\n",
      "Epoch [26][50]\t Batch [50][429]\t Training Loss 0.2357\t Accuracy 0.9393\n",
      "Epoch [26][50]\t Batch [100][429]\t Training Loss 0.2350\t Accuracy 0.9390\n",
      "Epoch [26][50]\t Batch [150][429]\t Training Loss 0.2425\t Accuracy 0.9355\n",
      "Epoch [26][50]\t Batch [200][429]\t Training Loss 0.2393\t Accuracy 0.9374\n",
      "Epoch [26][50]\t Batch [250][429]\t Training Loss 0.2401\t Accuracy 0.9370\n",
      "Epoch [26][50]\t Batch [300][429]\t Training Loss 0.2419\t Accuracy 0.9364\n",
      "Epoch [26][50]\t Batch [350][429]\t Training Loss 0.2431\t Accuracy 0.9359\n",
      "Epoch [26][50]\t Batch [400][429]\t Training Loss 0.2433\t Accuracy 0.9357\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2442\t Average training accuracy 0.9355\n",
      "Epoch [26]\t Average validation loss 0.1955\t Average validation accuracy 0.9523\n",
      "\n",
      "Epoch [27][50]\t Batch [0][429]\t Training Loss 0.3600\t Accuracy 0.8750\n",
      "Epoch [27][50]\t Batch [50][429]\t Training Loss 0.2342\t Accuracy 0.9396\n",
      "Epoch [27][50]\t Batch [100][429]\t Training Loss 0.2348\t Accuracy 0.9389\n",
      "Epoch [27][50]\t Batch [150][429]\t Training Loss 0.2407\t Accuracy 0.9359\n",
      "Epoch [27][50]\t Batch [200][429]\t Training Loss 0.2380\t Accuracy 0.9376\n",
      "Epoch [27][50]\t Batch [250][429]\t Training Loss 0.2383\t Accuracy 0.9373\n",
      "Epoch [27][50]\t Batch [300][429]\t Training Loss 0.2404\t Accuracy 0.9366\n",
      "Epoch [27][50]\t Batch [350][429]\t Training Loss 0.2416\t Accuracy 0.9361\n",
      "Epoch [27][50]\t Batch [400][429]\t Training Loss 0.2412\t Accuracy 0.9361\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2428\t Average training accuracy 0.9357\n",
      "Epoch [27]\t Average validation loss 0.1938\t Average validation accuracy 0.9523\n",
      "\n",
      "Epoch [28][50]\t Batch [0][429]\t Training Loss 0.1595\t Accuracy 0.9531\n",
      "Epoch [28][50]\t Batch [50][429]\t Training Loss 0.2318\t Accuracy 0.9404\n",
      "Epoch [28][50]\t Batch [100][429]\t Training Loss 0.2335\t Accuracy 0.9392\n",
      "Epoch [28][50]\t Batch [150][429]\t Training Loss 0.2386\t Accuracy 0.9369\n",
      "Epoch [28][50]\t Batch [200][429]\t Training Loss 0.2362\t Accuracy 0.9382\n",
      "Epoch [28][50]\t Batch [250][429]\t Training Loss 0.2362\t Accuracy 0.9381\n",
      "Epoch [28][50]\t Batch [300][429]\t Training Loss 0.2384\t Accuracy 0.9373\n",
      "Epoch [28][50]\t Batch [350][429]\t Training Loss 0.2394\t Accuracy 0.9369\n",
      "Epoch [28][50]\t Batch [400][429]\t Training Loss 0.2392\t Accuracy 0.9368\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2411\t Average training accuracy 0.9363\n",
      "Epoch [28]\t Average validation loss 0.1931\t Average validation accuracy 0.9525\n",
      "\n",
      "Epoch [29][50]\t Batch [0][429]\t Training Loss 0.1242\t Accuracy 0.9766\n",
      "Epoch [29][50]\t Batch [50][429]\t Training Loss 0.2288\t Accuracy 0.9409\n",
      "Epoch [29][50]\t Batch [100][429]\t Training Loss 0.2317\t Accuracy 0.9398\n",
      "Epoch [29][50]\t Batch [150][429]\t Training Loss 0.2364\t Accuracy 0.9376\n",
      "Epoch [29][50]\t Batch [200][429]\t Training Loss 0.2343\t Accuracy 0.9389\n",
      "Epoch [29][50]\t Batch [250][429]\t Training Loss 0.2344\t Accuracy 0.9386\n",
      "Epoch [29][50]\t Batch [300][429]\t Training Loss 0.2364\t Accuracy 0.9378\n",
      "Epoch [29][50]\t Batch [350][429]\t Training Loss 0.2374\t Accuracy 0.9374\n",
      "Epoch [29][50]\t Batch [400][429]\t Training Loss 0.2376\t Accuracy 0.9373\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2394\t Average training accuracy 0.9368\n",
      "Epoch [29]\t Average validation loss 0.1916\t Average validation accuracy 0.9535\n",
      "\n",
      "Epoch [30][50]\t Batch [0][429]\t Training Loss 0.1901\t Accuracy 0.9375\n",
      "Epoch [30][50]\t Batch [50][429]\t Training Loss 0.2287\t Accuracy 0.9406\n",
      "Epoch [30][50]\t Batch [100][429]\t Training Loss 0.2308\t Accuracy 0.9400\n",
      "Epoch [30][50]\t Batch [150][429]\t Training Loss 0.2347\t Accuracy 0.9382\n",
      "Epoch [30][50]\t Batch [200][429]\t Training Loss 0.2327\t Accuracy 0.9394\n",
      "Epoch [30][50]\t Batch [250][429]\t Training Loss 0.2329\t Accuracy 0.9391\n",
      "Epoch [30][50]\t Batch [300][429]\t Training Loss 0.2349\t Accuracy 0.9384\n",
      "Epoch [30][50]\t Batch [350][429]\t Training Loss 0.2357\t Accuracy 0.9380\n",
      "Epoch [30][50]\t Batch [400][429]\t Training Loss 0.2362\t Accuracy 0.9378\n",
      "\n",
      "Epoch [30]\t Average training loss 0.2378\t Average training accuracy 0.9374\n",
      "Epoch [30]\t Average validation loss 0.1906\t Average validation accuracy 0.9533\n",
      "\n",
      "Epoch [31][50]\t Batch [0][429]\t Training Loss 0.2919\t Accuracy 0.9141\n",
      "Epoch [31][50]\t Batch [50][429]\t Training Loss 0.2295\t Accuracy 0.9406\n",
      "Epoch [31][50]\t Batch [100][429]\t Training Loss 0.2287\t Accuracy 0.9406\n",
      "Epoch [31][50]\t Batch [150][429]\t Training Loss 0.2334\t Accuracy 0.9384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31][50]\t Batch [200][429]\t Training Loss 0.2315\t Accuracy 0.9397\n",
      "Epoch [31][50]\t Batch [250][429]\t Training Loss 0.2318\t Accuracy 0.9394\n",
      "Epoch [31][50]\t Batch [300][429]\t Training Loss 0.2337\t Accuracy 0.9388\n",
      "Epoch [31][50]\t Batch [350][429]\t Training Loss 0.2344\t Accuracy 0.9383\n",
      "Epoch [31][50]\t Batch [400][429]\t Training Loss 0.2349\t Accuracy 0.9381\n",
      "\n",
      "Epoch [31]\t Average training loss 0.2364\t Average training accuracy 0.9377\n",
      "Epoch [31]\t Average validation loss 0.1889\t Average validation accuracy 0.9535\n",
      "\n",
      "Epoch [32][50]\t Batch [0][429]\t Training Loss 0.2412\t Accuracy 0.9453\n",
      "Epoch [32][50]\t Batch [50][429]\t Training Loss 0.2277\t Accuracy 0.9419\n",
      "Epoch [32][50]\t Batch [100][429]\t Training Loss 0.2275\t Accuracy 0.9413\n",
      "Epoch [32][50]\t Batch [150][429]\t Training Loss 0.2319\t Accuracy 0.9389\n",
      "Epoch [32][50]\t Batch [200][429]\t Training Loss 0.2307\t Accuracy 0.9399\n",
      "Epoch [32][50]\t Batch [250][429]\t Training Loss 0.2303\t Accuracy 0.9399\n",
      "Epoch [32][50]\t Batch [300][429]\t Training Loss 0.2325\t Accuracy 0.9391\n",
      "Epoch [32][50]\t Batch [350][429]\t Training Loss 0.2328\t Accuracy 0.9388\n",
      "Epoch [32][50]\t Batch [400][429]\t Training Loss 0.2336\t Accuracy 0.9385\n",
      "\n",
      "Epoch [32]\t Average training loss 0.2347\t Average training accuracy 0.9382\n",
      "Epoch [32]\t Average validation loss 0.1885\t Average validation accuracy 0.9543\n",
      "\n",
      "Epoch [33][50]\t Batch [0][429]\t Training Loss 0.3551\t Accuracy 0.9141\n",
      "Epoch [33][50]\t Batch [50][429]\t Training Loss 0.2281\t Accuracy 0.9413\n",
      "Epoch [33][50]\t Batch [100][429]\t Training Loss 0.2277\t Accuracy 0.9409\n",
      "Epoch [33][50]\t Batch [150][429]\t Training Loss 0.2315\t Accuracy 0.9388\n",
      "Epoch [33][50]\t Batch [200][429]\t Training Loss 0.2302\t Accuracy 0.9401\n",
      "Epoch [33][50]\t Batch [250][429]\t Training Loss 0.2295\t Accuracy 0.9401\n",
      "Epoch [33][50]\t Batch [300][429]\t Training Loss 0.2316\t Accuracy 0.9393\n",
      "Epoch [33][50]\t Batch [350][429]\t Training Loss 0.2320\t Accuracy 0.9390\n",
      "Epoch [33][50]\t Batch [400][429]\t Training Loss 0.2328\t Accuracy 0.9386\n",
      "\n",
      "Epoch [33]\t Average training loss 0.2337\t Average training accuracy 0.9384\n",
      "Epoch [33]\t Average validation loss 0.1854\t Average validation accuracy 0.9551\n",
      "\n",
      "Epoch [34][50]\t Batch [0][429]\t Training Loss 0.3196\t Accuracy 0.9219\n",
      "Epoch [34][50]\t Batch [50][429]\t Training Loss 0.2292\t Accuracy 0.9409\n",
      "Epoch [34][50]\t Batch [100][429]\t Training Loss 0.2270\t Accuracy 0.9408\n",
      "Epoch [34][50]\t Batch [150][429]\t Training Loss 0.2308\t Accuracy 0.9391\n",
      "Epoch [34][50]\t Batch [200][429]\t Training Loss 0.2294\t Accuracy 0.9404\n",
      "Epoch [34][50]\t Batch [250][429]\t Training Loss 0.2281\t Accuracy 0.9406\n",
      "Epoch [34][50]\t Batch [300][429]\t Training Loss 0.2305\t Accuracy 0.9397\n",
      "Epoch [34][50]\t Batch [350][429]\t Training Loss 0.2308\t Accuracy 0.9395\n",
      "Epoch [34][50]\t Batch [400][429]\t Training Loss 0.2315\t Accuracy 0.9391\n",
      "\n",
      "Epoch [34]\t Average training loss 0.2323\t Average training accuracy 0.9389\n",
      "Epoch [34]\t Average validation loss 0.1850\t Average validation accuracy 0.9551\n",
      "\n",
      "Epoch [35][50]\t Batch [0][429]\t Training Loss 0.2778\t Accuracy 0.9219\n",
      "Epoch [35][50]\t Batch [50][429]\t Training Loss 0.2294\t Accuracy 0.9406\n",
      "Epoch [35][50]\t Batch [100][429]\t Training Loss 0.2272\t Accuracy 0.9407\n",
      "Epoch [35][50]\t Batch [150][429]\t Training Loss 0.2303\t Accuracy 0.9391\n",
      "Epoch [35][50]\t Batch [200][429]\t Training Loss 0.2285\t Accuracy 0.9406\n",
      "Epoch [35][50]\t Batch [250][429]\t Training Loss 0.2270\t Accuracy 0.9408\n",
      "Epoch [35][50]\t Batch [300][429]\t Training Loss 0.2294\t Accuracy 0.9399\n",
      "Epoch [35][50]\t Batch [350][429]\t Training Loss 0.2298\t Accuracy 0.9398\n",
      "Epoch [35][50]\t Batch [400][429]\t Training Loss 0.2305\t Accuracy 0.9394\n",
      "\n",
      "Epoch [35]\t Average training loss 0.2314\t Average training accuracy 0.9391\n",
      "Epoch [35]\t Average validation loss 0.1844\t Average validation accuracy 0.9553\n",
      "\n",
      "Epoch [36][50]\t Batch [0][429]\t Training Loss 0.1812\t Accuracy 0.9609\n",
      "Epoch [36][50]\t Batch [50][429]\t Training Loss 0.2271\t Accuracy 0.9413\n",
      "Epoch [36][50]\t Batch [100][429]\t Training Loss 0.2254\t Accuracy 0.9414\n",
      "Epoch [36][50]\t Batch [150][429]\t Training Loss 0.2291\t Accuracy 0.9396\n",
      "Epoch [36][50]\t Batch [200][429]\t Training Loss 0.2263\t Accuracy 0.9415\n",
      "Epoch [36][50]\t Batch [250][429]\t Training Loss 0.2258\t Accuracy 0.9413\n",
      "Epoch [36][50]\t Batch [300][429]\t Training Loss 0.2281\t Accuracy 0.9404\n",
      "Epoch [36][50]\t Batch [350][429]\t Training Loss 0.2285\t Accuracy 0.9403\n",
      "Epoch [36][50]\t Batch [400][429]\t Training Loss 0.2294\t Accuracy 0.9398\n",
      "\n",
      "Epoch [36]\t Average training loss 0.2301\t Average training accuracy 0.9396\n",
      "Epoch [36]\t Average validation loss 0.1851\t Average validation accuracy 0.9545\n",
      "\n",
      "Epoch [37][50]\t Batch [0][429]\t Training Loss 0.1840\t Accuracy 0.9531\n",
      "Epoch [37][50]\t Batch [50][429]\t Training Loss 0.2261\t Accuracy 0.9419\n",
      "Epoch [37][50]\t Batch [100][429]\t Training Loss 0.2247\t Accuracy 0.9417\n",
      "Epoch [37][50]\t Batch [150][429]\t Training Loss 0.2280\t Accuracy 0.9400\n",
      "Epoch [37][50]\t Batch [200][429]\t Training Loss 0.2244\t Accuracy 0.9421\n",
      "Epoch [37][50]\t Batch [250][429]\t Training Loss 0.2247\t Accuracy 0.9416\n",
      "Epoch [37][50]\t Batch [300][429]\t Training Loss 0.2270\t Accuracy 0.9407\n",
      "Epoch [37][50]\t Batch [350][429]\t Training Loss 0.2272\t Accuracy 0.9407\n",
      "Epoch [37][50]\t Batch [400][429]\t Training Loss 0.2283\t Accuracy 0.9401\n",
      "\n",
      "Epoch [37]\t Average training loss 0.2290\t Average training accuracy 0.9399\n",
      "Epoch [37]\t Average validation loss 0.1826\t Average validation accuracy 0.9551\n",
      "\n",
      "Epoch [38][50]\t Batch [0][429]\t Training Loss 0.2006\t Accuracy 0.9531\n",
      "Epoch [38][50]\t Batch [50][429]\t Training Loss 0.2248\t Accuracy 0.9424\n",
      "Epoch [38][50]\t Batch [100][429]\t Training Loss 0.2234\t Accuracy 0.9422\n",
      "Epoch [38][50]\t Batch [150][429]\t Training Loss 0.2262\t Accuracy 0.9408\n",
      "Epoch [38][50]\t Batch [200][429]\t Training Loss 0.2234\t Accuracy 0.9426\n",
      "Epoch [38][50]\t Batch [250][429]\t Training Loss 0.2237\t Accuracy 0.9420\n",
      "Epoch [38][50]\t Batch [300][429]\t Training Loss 0.2257\t Accuracy 0.9411\n",
      "Epoch [38][50]\t Batch [350][429]\t Training Loss 0.2261\t Accuracy 0.9411\n",
      "Epoch [38][50]\t Batch [400][429]\t Training Loss 0.2273\t Accuracy 0.9405\n",
      "\n",
      "Epoch [38]\t Average training loss 0.2280\t Average training accuracy 0.9403\n",
      "Epoch [38]\t Average validation loss 0.1835\t Average validation accuracy 0.9551\n",
      "\n",
      "Epoch [39][50]\t Batch [0][429]\t Training Loss 0.1270\t Accuracy 0.9766\n",
      "Epoch [39][50]\t Batch [50][429]\t Training Loss 0.2238\t Accuracy 0.9429\n",
      "Epoch [39][50]\t Batch [100][429]\t Training Loss 0.2218\t Accuracy 0.9429\n",
      "Epoch [39][50]\t Batch [150][429]\t Training Loss 0.2247\t Accuracy 0.9412\n",
      "Epoch [39][50]\t Batch [200][429]\t Training Loss 0.2224\t Accuracy 0.9429\n",
      "Epoch [39][50]\t Batch [250][429]\t Training Loss 0.2225\t Accuracy 0.9425\n",
      "Epoch [39][50]\t Batch [300][429]\t Training Loss 0.2245\t Accuracy 0.9416\n",
      "Epoch [39][50]\t Batch [350][429]\t Training Loss 0.2246\t Accuracy 0.9416\n",
      "Epoch [39][50]\t Batch [400][429]\t Training Loss 0.2259\t Accuracy 0.9410\n",
      "\n",
      "Epoch [39]\t Average training loss 0.2268\t Average training accuracy 0.9407\n",
      "Epoch [39]\t Average validation loss 0.1827\t Average validation accuracy 0.9557\n",
      "\n",
      "Epoch [40][50]\t Batch [0][429]\t Training Loss 0.1577\t Accuracy 0.9609\n",
      "Epoch [40][50]\t Batch [50][429]\t Training Loss 0.2209\t Accuracy 0.9436\n",
      "Epoch [40][50]\t Batch [100][429]\t Training Loss 0.2200\t Accuracy 0.9432\n",
      "Epoch [40][50]\t Batch [150][429]\t Training Loss 0.2231\t Accuracy 0.9416\n",
      "Epoch [40][50]\t Batch [200][429]\t Training Loss 0.2210\t Accuracy 0.9432\n",
      "Epoch [40][50]\t Batch [250][429]\t Training Loss 0.2209\t Accuracy 0.9428\n",
      "Epoch [40][50]\t Batch [300][429]\t Training Loss 0.2233\t Accuracy 0.9418\n",
      "Epoch [40][50]\t Batch [350][429]\t Training Loss 0.2233\t Accuracy 0.9418\n",
      "Epoch [40][50]\t Batch [400][429]\t Training Loss 0.2249\t Accuracy 0.9412\n",
      "\n",
      "Epoch [40]\t Average training loss 0.2258\t Average training accuracy 0.9409\n",
      "Epoch [40]\t Average validation loss 0.1820\t Average validation accuracy 0.9561\n",
      "\n",
      "Epoch [41][50]\t Batch [0][429]\t Training Loss 0.1436\t Accuracy 0.9609\n",
      "Epoch [41][50]\t Batch [50][429]\t Training Loss 0.2197\t Accuracy 0.9438\n",
      "Epoch [41][50]\t Batch [100][429]\t Training Loss 0.2187\t Accuracy 0.9438\n",
      "Epoch [41][50]\t Batch [150][429]\t Training Loss 0.2212\t Accuracy 0.9421\n",
      "Epoch [41][50]\t Batch [200][429]\t Training Loss 0.2195\t Accuracy 0.9435\n",
      "Epoch [41][50]\t Batch [250][429]\t Training Loss 0.2198\t Accuracy 0.9430\n",
      "Epoch [41][50]\t Batch [300][429]\t Training Loss 0.2223\t Accuracy 0.9420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41][50]\t Batch [350][429]\t Training Loss 0.2219\t Accuracy 0.9421\n",
      "Epoch [41][50]\t Batch [400][429]\t Training Loss 0.2238\t Accuracy 0.9415\n",
      "\n",
      "Epoch [41]\t Average training loss 0.2248\t Average training accuracy 0.9412\n",
      "Epoch [41]\t Average validation loss 0.1813\t Average validation accuracy 0.9565\n",
      "\n",
      "Epoch [42][50]\t Batch [0][429]\t Training Loss 0.1535\t Accuracy 0.9453\n",
      "Epoch [42][50]\t Batch [50][429]\t Training Loss 0.2189\t Accuracy 0.9442\n",
      "Epoch [42][50]\t Batch [100][429]\t Training Loss 0.2173\t Accuracy 0.9441\n",
      "Epoch [42][50]\t Batch [150][429]\t Training Loss 0.2203\t Accuracy 0.9423\n",
      "Epoch [42][50]\t Batch [200][429]\t Training Loss 0.2185\t Accuracy 0.9436\n",
      "Epoch [42][50]\t Batch [250][429]\t Training Loss 0.2189\t Accuracy 0.9432\n",
      "Epoch [42][50]\t Batch [300][429]\t Training Loss 0.2213\t Accuracy 0.9424\n",
      "Epoch [42][50]\t Batch [350][429]\t Training Loss 0.2207\t Accuracy 0.9426\n",
      "Epoch [42][50]\t Batch [400][429]\t Training Loss 0.2228\t Accuracy 0.9419\n",
      "\n",
      "Epoch [42]\t Average training loss 0.2237\t Average training accuracy 0.9418\n",
      "Epoch [42]\t Average validation loss 0.1804\t Average validation accuracy 0.9565\n",
      "\n",
      "Epoch [43][50]\t Batch [0][429]\t Training Loss 0.2441\t Accuracy 0.9219\n",
      "Epoch [43][50]\t Batch [50][429]\t Training Loss 0.2188\t Accuracy 0.9444\n",
      "Epoch [43][50]\t Batch [100][429]\t Training Loss 0.2168\t Accuracy 0.9444\n",
      "Epoch [43][50]\t Batch [150][429]\t Training Loss 0.2199\t Accuracy 0.9425\n",
      "Epoch [43][50]\t Batch [200][429]\t Training Loss 0.2179\t Accuracy 0.9439\n",
      "Epoch [43][50]\t Batch [250][429]\t Training Loss 0.2177\t Accuracy 0.9435\n",
      "Epoch [43][50]\t Batch [300][429]\t Training Loss 0.2203\t Accuracy 0.9427\n",
      "Epoch [43][50]\t Batch [350][429]\t Training Loss 0.2200\t Accuracy 0.9428\n",
      "Epoch [43][50]\t Batch [400][429]\t Training Loss 0.2218\t Accuracy 0.9421\n",
      "\n",
      "Epoch [43]\t Average training loss 0.2228\t Average training accuracy 0.9419\n",
      "Epoch [43]\t Average validation loss 0.1792\t Average validation accuracy 0.9569\n",
      "\n",
      "Epoch [44][50]\t Batch [0][429]\t Training Loss 0.2183\t Accuracy 0.9375\n",
      "Epoch [44][50]\t Batch [50][429]\t Training Loss 0.2170\t Accuracy 0.9444\n",
      "Epoch [44][50]\t Batch [100][429]\t Training Loss 0.2156\t Accuracy 0.9446\n",
      "Epoch [44][50]\t Batch [150][429]\t Training Loss 0.2190\t Accuracy 0.9426\n",
      "Epoch [44][50]\t Batch [200][429]\t Training Loss 0.2172\t Accuracy 0.9440\n",
      "Epoch [44][50]\t Batch [250][429]\t Training Loss 0.2169\t Accuracy 0.9437\n",
      "Epoch [44][50]\t Batch [300][429]\t Training Loss 0.2191\t Accuracy 0.9431\n",
      "Epoch [44][50]\t Batch [350][429]\t Training Loss 0.2190\t Accuracy 0.9431\n",
      "Epoch [44][50]\t Batch [400][429]\t Training Loss 0.2209\t Accuracy 0.9424\n",
      "\n",
      "Epoch [44]\t Average training loss 0.2220\t Average training accuracy 0.9422\n",
      "Epoch [44]\t Average validation loss 0.1792\t Average validation accuracy 0.9571\n",
      "\n",
      "Epoch [45][50]\t Batch [0][429]\t Training Loss 0.1664\t Accuracy 0.9688\n",
      "Epoch [45][50]\t Batch [50][429]\t Training Loss 0.2164\t Accuracy 0.9450\n",
      "Epoch [45][50]\t Batch [100][429]\t Training Loss 0.2152\t Accuracy 0.9451\n",
      "Epoch [45][50]\t Batch [150][429]\t Training Loss 0.2179\t Accuracy 0.9430\n",
      "Epoch [45][50]\t Batch [200][429]\t Training Loss 0.2166\t Accuracy 0.9442\n",
      "Epoch [45][50]\t Batch [250][429]\t Training Loss 0.2161\t Accuracy 0.9440\n",
      "Epoch [45][50]\t Batch [300][429]\t Training Loss 0.2181\t Accuracy 0.9434\n",
      "Epoch [45][50]\t Batch [350][429]\t Training Loss 0.2179\t Accuracy 0.9436\n",
      "Epoch [45][50]\t Batch [400][429]\t Training Loss 0.2199\t Accuracy 0.9429\n",
      "\n",
      "Epoch [45]\t Average training loss 0.2212\t Average training accuracy 0.9425\n",
      "Epoch [45]\t Average validation loss 0.1785\t Average validation accuracy 0.9575\n",
      "\n",
      "Epoch [46][50]\t Batch [0][429]\t Training Loss 0.1255\t Accuracy 0.9766\n",
      "Epoch [46][50]\t Batch [50][429]\t Training Loss 0.2151\t Accuracy 0.9452\n",
      "Epoch [46][50]\t Batch [100][429]\t Training Loss 0.2127\t Accuracy 0.9457\n",
      "Epoch [46][50]\t Batch [150][429]\t Training Loss 0.2168\t Accuracy 0.9432\n",
      "Epoch [46][50]\t Batch [200][429]\t Training Loss 0.2154\t Accuracy 0.9446\n",
      "Epoch [46][50]\t Batch [250][429]\t Training Loss 0.2149\t Accuracy 0.9443\n",
      "Epoch [46][50]\t Batch [300][429]\t Training Loss 0.2169\t Accuracy 0.9438\n",
      "Epoch [46][50]\t Batch [350][429]\t Training Loss 0.2169\t Accuracy 0.9439\n",
      "Epoch [46][50]\t Batch [400][429]\t Training Loss 0.2188\t Accuracy 0.9432\n",
      "\n",
      "Epoch [46]\t Average training loss 0.2203\t Average training accuracy 0.9428\n",
      "Epoch [46]\t Average validation loss 0.1780\t Average validation accuracy 0.9571\n",
      "\n",
      "Epoch [47][50]\t Batch [0][429]\t Training Loss 0.1251\t Accuracy 0.9766\n",
      "Epoch [47][50]\t Batch [50][429]\t Training Loss 0.2135\t Accuracy 0.9459\n",
      "Epoch [47][50]\t Batch [100][429]\t Training Loss 0.2104\t Accuracy 0.9465\n",
      "Epoch [47][50]\t Batch [150][429]\t Training Loss 0.2156\t Accuracy 0.9434\n",
      "Epoch [47][50]\t Batch [200][429]\t Training Loss 0.2144\t Accuracy 0.9448\n",
      "Epoch [47][50]\t Batch [250][429]\t Training Loss 0.2143\t Accuracy 0.9444\n",
      "Epoch [47][50]\t Batch [300][429]\t Training Loss 0.2156\t Accuracy 0.9441\n",
      "Epoch [47][50]\t Batch [350][429]\t Training Loss 0.2161\t Accuracy 0.9441\n",
      "Epoch [47][50]\t Batch [400][429]\t Training Loss 0.2176\t Accuracy 0.9435\n",
      "\n",
      "Epoch [47]\t Average training loss 0.2194\t Average training accuracy 0.9431\n",
      "Epoch [47]\t Average validation loss 0.1775\t Average validation accuracy 0.9573\n",
      "\n",
      "Epoch [48][50]\t Batch [0][429]\t Training Loss 0.1746\t Accuracy 0.9297\n",
      "Epoch [48][50]\t Batch [50][429]\t Training Loss 0.2137\t Accuracy 0.9447\n",
      "Epoch [48][50]\t Batch [100][429]\t Training Loss 0.2071\t Accuracy 0.9470\n",
      "Epoch [48][50]\t Batch [150][429]\t Training Loss 0.2139\t Accuracy 0.9438\n",
      "Epoch [48][50]\t Batch [200][429]\t Training Loss 0.2134\t Accuracy 0.9450\n",
      "Epoch [48][50]\t Batch [250][429]\t Training Loss 0.2135\t Accuracy 0.9446\n",
      "Epoch [48][50]\t Batch [300][429]\t Training Loss 0.2149\t Accuracy 0.9441\n",
      "Epoch [48][50]\t Batch [350][429]\t Training Loss 0.2152\t Accuracy 0.9442\n",
      "Epoch [48][50]\t Batch [400][429]\t Training Loss 0.2168\t Accuracy 0.9436\n",
      "\n",
      "Epoch [48]\t Average training loss 0.2185\t Average training accuracy 0.9432\n",
      "Epoch [48]\t Average validation loss 0.1770\t Average validation accuracy 0.9573\n",
      "\n",
      "Epoch [49][50]\t Batch [0][429]\t Training Loss 0.2031\t Accuracy 0.9453\n",
      "Epoch [49][50]\t Batch [50][429]\t Training Loss 0.2138\t Accuracy 0.9456\n",
      "Epoch [49][50]\t Batch [100][429]\t Training Loss 0.2055\t Accuracy 0.9479\n",
      "Epoch [49][50]\t Batch [150][429]\t Training Loss 0.2133\t Accuracy 0.9442\n",
      "Epoch [49][50]\t Batch [200][429]\t Training Loss 0.2129\t Accuracy 0.9454\n",
      "Epoch [49][50]\t Batch [250][429]\t Training Loss 0.2128\t Accuracy 0.9450\n",
      "Epoch [49][50]\t Batch [300][429]\t Training Loss 0.2144\t Accuracy 0.9444\n",
      "Epoch [49][50]\t Batch [350][429]\t Training Loss 0.2145\t Accuracy 0.9446\n",
      "Epoch [49][50]\t Batch [400][429]\t Training Loss 0.2161\t Accuracy 0.9440\n",
      "\n",
      "Epoch [49]\t Average training loss 0.2177\t Average training accuracy 0.9436\n",
      "Epoch [49]\t Average validation loss 0.1764\t Average validation accuracy 0.9573\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:49:08.242132Z",
     "start_time": "2023-11-10T08:49:00.732634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9453.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T08:49:08.694079Z",
     "start_time": "2023-11-10T08:49:08.244270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具有两层隐含层的多层感知机\n",
    "\n",
    "接下来，根据案例要求，还需要完成**构造具有两个隐含层的多层感知机，自行选取合适的激活函数和损失函数，与只有一个隐含层的结果相比较**.\n",
    "\n",
    "注意: 请在下方插入新的代码块，不要直接修改上面的代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T09:03:12.174311Z",
     "start_time": "2023-11-10T08:56:58.971690Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.2465\t Accuracy 0.1500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.2872\t Accuracy 0.1588\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.2621\t Accuracy 0.1792\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.2419\t Accuracy 0.1934\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.2237\t Accuracy 0.2081\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2056\t Accuracy 0.2222\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.1871\t Accuracy 0.2369\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.1683\t Accuracy 0.2511\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.1460\t Accuracy 0.2673\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.1264\t Accuracy 0.2816\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.1058\t Accuracy 0.2964\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0845\t Average training accuracy 0.3119\n",
      "Epoch [0]\t Average validation loss 1.8223\t Average validation accuracy 0.4938\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.7618\t Accuracy 0.5300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.8115\t Accuracy 0.4873\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.7833\t Accuracy 0.5076\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.7654\t Accuracy 0.5180\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.7474\t Accuracy 0.5277\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.7265\t Accuracy 0.5398\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.7065\t Accuracy 0.5507\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.6905\t Accuracy 0.5589\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.6692\t Accuracy 0.5692\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.6516\t Accuracy 0.5775\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.6331\t Accuracy 0.5850\n",
      "\n",
      "Epoch [1]\t Average training loss 1.6138\t Average training accuracy 0.5932\n",
      "Epoch [1]\t Average validation loss 1.3531\t Average validation accuracy 0.7050\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.3066\t Accuracy 0.7500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.3739\t Accuracy 0.6867\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.3538\t Accuracy 0.6949\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.3444\t Accuracy 0.6931\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.3340\t Accuracy 0.6942\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.3191\t Accuracy 0.6980\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.3053\t Accuracy 0.7011\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.2977\t Accuracy 0.7024\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.2833\t Accuracy 0.7069\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.2722\t Accuracy 0.7101\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.2603\t Accuracy 0.7125\n",
      "\n",
      "Epoch [2]\t Average training loss 1.2473\t Average training accuracy 0.7156\n",
      "Epoch [2]\t Average validation loss 1.0409\t Average validation accuracy 0.7810\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.0158\t Accuracy 0.8400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.0829\t Accuracy 0.7598\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.0729\t Accuracy 0.7611\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.0699\t Accuracy 0.7589\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.0651\t Accuracy 0.7585\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.0548\t Accuracy 0.7602\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.0459\t Accuracy 0.7621\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.0439\t Accuracy 0.7613\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.0347\t Accuracy 0.7642\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.0281\t Accuracy 0.7653\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.0211\t Accuracy 0.7664\n",
      "\n",
      "Epoch [3]\t Average training loss 1.0124\t Average training accuracy 0.7684\n",
      "Epoch [3]\t Average validation loss 0.8463\t Average validation accuracy 0.8172\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.8391\t Accuracy 0.8600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.9003\t Accuracy 0.7998\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.8968\t Accuracy 0.7982\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.8976\t Accuracy 0.7945\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.8960\t Accuracy 0.7924\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.8883\t Accuracy 0.7945\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.8825\t Accuracy 0.7958\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.8836\t Accuracy 0.7950\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.8775\t Accuracy 0.7965\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.8736\t Accuracy 0.7968\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.8695\t Accuracy 0.7975\n",
      "\n",
      "Epoch [4]\t Average training loss 0.8635\t Average training accuracy 0.7984\n",
      "Epoch [4]\t Average validation loss 0.7220\t Average validation accuracy 0.8466\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.7267\t Accuracy 0.8600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.7821\t Accuracy 0.8218\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.7826\t Accuracy 0.8194\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.7856\t Accuracy 0.8156\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.7856\t Accuracy 0.8130\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.7795\t Accuracy 0.8148\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.7755\t Accuracy 0.8155\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.7782\t Accuracy 0.8147\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.7740\t Accuracy 0.8162\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.7716\t Accuracy 0.8163\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.7693\t Accuracy 0.8164\n",
      "\n",
      "Epoch [5]\t Average training loss 0.7649\t Average training accuracy 0.8172\n",
      "Epoch [5]\t Average validation loss 0.6382\t Average validation accuracy 0.8662\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.6514\t Accuracy 0.8600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.7014\t Accuracy 0.8376\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.7042\t Accuracy 0.8346\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.7087\t Accuracy 0.8305\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.7095\t Accuracy 0.8280\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.7043\t Accuracy 0.8296\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.7015\t Accuracy 0.8300\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.7050\t Accuracy 0.8289\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.7020\t Accuracy 0.8302\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.7006\t Accuracy 0.8301\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.6993\t Accuracy 0.8300\n",
      "\n",
      "Epoch [6]\t Average training loss 0.6959\t Average training accuracy 0.8305\n",
      "Epoch [6]\t Average validation loss 0.5784\t Average validation accuracy 0.8764\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.5993\t Accuracy 0.8600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.6431\t Accuracy 0.8488\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.6476\t Accuracy 0.8463\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.6530\t Accuracy 0.8425\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.6544\t Accuracy 0.8405\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.6497\t Accuracy 0.8412\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.6477\t Accuracy 0.8413\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.6516\t Accuracy 0.8401\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.6494\t Accuracy 0.8410\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.6485\t Accuracy 0.8408\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.6480\t Accuracy 0.8405\n",
      "\n",
      "Epoch [7]\t Average training loss 0.6453\t Average training accuracy 0.8407\n",
      "Epoch [7]\t Average validation loss 0.5340\t Average validation accuracy 0.8840\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.5613\t Accuracy 0.8700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.5991\t Accuracy 0.8541\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.6050\t Accuracy 0.8535\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.6110\t Accuracy 0.8499\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.6126\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.6083\t Accuracy 0.8494\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.6068\t Accuracy 0.8498\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.6110\t Accuracy 0.8484\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.6094\t Accuracy 0.8491\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.6089\t Accuracy 0.8488\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.6088\t Accuracy 0.8482\n",
      "\n",
      "Epoch [8]\t Average training loss 0.6067\t Average training accuracy 0.8483\n",
      "Epoch [8]\t Average validation loss 0.4998\t Average validation accuracy 0.8902\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.5317\t Accuracy 0.8700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.5648\t Accuracy 0.8600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.5716\t Accuracy 0.8596\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.5782\t Accuracy 0.8560\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.5799\t Accuracy 0.8554\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.5759\t Accuracy 0.8562\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.5747\t Accuracy 0.8565\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.5790\t Accuracy 0.8550\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.5778\t Accuracy 0.8555\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.5776\t Accuracy 0.8554\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.5779\t Accuracy 0.8548\n",
      "\n",
      "Epoch [9]\t Average training loss 0.5761\t Average training accuracy 0.8549\n",
      "Epoch [9]\t Average validation loss 0.4727\t Average validation accuracy 0.8954\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.5081\t Accuracy 0.8700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.5372\t Accuracy 0.8667\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.5448\t Accuracy 0.8648\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.5517\t Accuracy 0.8621\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.5534\t Accuracy 0.8619\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.5497\t Accuracy 0.8623\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.5488\t Accuracy 0.8626\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.5532\t Accuracy 0.8611\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.5523\t Accuracy 0.8616\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.5522\t Accuracy 0.8614\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.5528\t Accuracy 0.8605\n",
      "\n",
      "Epoch [10]\t Average training loss 0.5513\t Average training accuracy 0.8607\n",
      "Epoch [10]\t Average validation loss 0.4507\t Average validation accuracy 0.8988\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.4888\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.5144\t Accuracy 0.8729\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.5226\t Accuracy 0.8698\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.5298\t Accuracy 0.8668\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.5316\t Accuracy 0.8666\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.5280\t Accuracy 0.8670\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.5274\t Accuracy 0.8672\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.5318\t Accuracy 0.8657\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.5311\t Accuracy 0.8659\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.5312\t Accuracy 0.8658\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.5320\t Accuracy 0.8649\n",
      "\n",
      "Epoch [11]\t Average training loss 0.5307\t Average training accuracy 0.8650\n",
      "Epoch [11]\t Average validation loss 0.4324\t Average validation accuracy 0.9016\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.4724\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.4953\t Accuracy 0.8780\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.5040\t Accuracy 0.8753\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.5115\t Accuracy 0.8717\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.5131\t Accuracy 0.8714\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.5098\t Accuracy 0.8717\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.5094\t Accuracy 0.8717\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.5138\t Accuracy 0.8700\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.5132\t Accuracy 0.8701\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.5134\t Accuracy 0.8698\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.5144\t Accuracy 0.8688\n",
      "\n",
      "Epoch [12]\t Average training loss 0.5132\t Average training accuracy 0.8689\n",
      "Epoch [12]\t Average validation loss 0.4170\t Average validation accuracy 0.9038\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.4584\t Accuracy 0.8900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.4790\t Accuracy 0.8825\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.4882\t Accuracy 0.8792\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.4958\t Accuracy 0.8757\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.4974\t Accuracy 0.8754\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.4942\t Accuracy 0.8755\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.4939\t Accuracy 0.8752\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.4983\t Accuracy 0.8732\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.4979\t Accuracy 0.8733\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.4982\t Accuracy 0.8733\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.4993\t Accuracy 0.8722\n",
      "\n",
      "Epoch [13]\t Average training loss 0.4983\t Average training accuracy 0.8722\n",
      "Epoch [13]\t Average validation loss 0.4039\t Average validation accuracy 0.9096\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.4463\t Accuracy 0.9000\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.4649\t Accuracy 0.8859\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.4744\t Accuracy 0.8818\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.4822\t Accuracy 0.8779\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.4838\t Accuracy 0.8781\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.4807\t Accuracy 0.8783\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.4806\t Accuracy 0.8777\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.4849\t Accuracy 0.8758\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.4847\t Accuracy 0.8758\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.4850\t Accuracy 0.8759\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.4862\t Accuracy 0.8748\n",
      "\n",
      "Epoch [14]\t Average training loss 0.4853\t Average training accuracy 0.8748\n",
      "Epoch [14]\t Average validation loss 0.3925\t Average validation accuracy 0.9108\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.4357\t Accuracy 0.9000\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.4525\t Accuracy 0.8878\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.4624\t Accuracy 0.8836\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.4703\t Accuracy 0.8799\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.4718\t Accuracy 0.8801\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.4689\t Accuracy 0.8805\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.4689\t Accuracy 0.8798\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.4732\t Accuracy 0.8780\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.4730\t Accuracy 0.8781\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.4735\t Accuracy 0.8781\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.4746\t Accuracy 0.8770\n",
      "\n",
      "Epoch [15]\t Average training loss 0.4739\t Average training accuracy 0.8771\n",
      "Epoch [15]\t Average validation loss 0.3826\t Average validation accuracy 0.9126\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.4263\t Accuracy 0.9000\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.4417\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.4517\t Accuracy 0.8857\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.4597\t Accuracy 0.8814\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.4612\t Accuracy 0.8816\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.4585\t Accuracy 0.8820\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.4585\t Accuracy 0.8815\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.4628\t Accuracy 0.8799\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.4627\t Accuracy 0.8799\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.4632\t Accuracy 0.8799\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.4645\t Accuracy 0.8788\n",
      "\n",
      "Epoch [16]\t Average training loss 0.4638\t Average training accuracy 0.8789\n",
      "Epoch [16]\t Average validation loss 0.3738\t Average validation accuracy 0.9130\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.4178\t Accuracy 0.9100\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.4320\t Accuracy 0.8920\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.4423\t Accuracy 0.8874\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.4504\t Accuracy 0.8834\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.4518\t Accuracy 0.8837\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.4492\t Accuracy 0.8841\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.4493\t Accuracy 0.8836\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.4536\t Accuracy 0.8818\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.4535\t Accuracy 0.8818\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.4540\t Accuracy 0.8818\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.4554\t Accuracy 0.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [17]\t Average training loss 0.4548\t Average training accuracy 0.8810\n",
      "Epoch [17]\t Average validation loss 0.3660\t Average validation accuracy 0.9142\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.4103\t Accuracy 0.9100\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.4233\t Accuracy 0.8933\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.4338\t Accuracy 0.8892\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.4420\t Accuracy 0.8860\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.4434\t Accuracy 0.8858\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.4408\t Accuracy 0.8861\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.4411\t Accuracy 0.8854\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.4453\t Accuracy 0.8838\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.4453\t Accuracy 0.8837\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.4458\t Accuracy 0.8837\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.4472\t Accuracy 0.8827\n",
      "\n",
      "Epoch [18]\t Average training loss 0.4467\t Average training accuracy 0.8829\n",
      "Epoch [18]\t Average validation loss 0.3590\t Average validation accuracy 0.9152\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.4034\t Accuracy 0.9100\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.4155\t Accuracy 0.8947\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.4262\t Accuracy 0.8908\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.4344\t Accuracy 0.8875\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.4357\t Accuracy 0.8872\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.4333\t Accuracy 0.8877\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.4336\t Accuracy 0.8870\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.4378\t Accuracy 0.8856\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.4378\t Accuracy 0.8855\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.4384\t Accuracy 0.8855\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.4398\t Accuracy 0.8846\n",
      "\n",
      "Epoch [19]\t Average training loss 0.4394\t Average training accuracy 0.8848\n",
      "Epoch [19]\t Average validation loss 0.3528\t Average validation accuracy 0.9154\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.8967.\n",
      "\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4730\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.4132\t Accuracy 0.1143\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3579\t Accuracy 0.1292\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3177\t Accuracy 0.1445\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.2844\t Accuracy 0.1602\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2481\t Accuracy 0.1831\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2156\t Accuracy 0.2036\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.1861\t Accuracy 0.2241\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.1550\t Accuracy 0.2472\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.1255\t Accuracy 0.2679\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.0947\t Accuracy 0.2904\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0625\t Average training accuracy 0.3135\n",
      "Epoch [0]\t Average validation loss 1.6606\t Average validation accuracy 0.5938\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.6458\t Accuracy 0.5700\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.6744\t Accuracy 0.5657\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.6400\t Accuracy 0.5848\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.6155\t Accuracy 0.5913\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.5935\t Accuracy 0.6000\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.5651\t Accuracy 0.6100\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.5377\t Accuracy 0.6199\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.5199\t Accuracy 0.6244\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.4969\t Accuracy 0.6314\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.4769\t Accuracy 0.6367\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.4551\t Accuracy 0.6417\n",
      "\n",
      "Epoch [1]\t Average training loss 1.4312\t Average training accuracy 0.6493\n",
      "Epoch [1]\t Average validation loss 1.0925\t Average validation accuracy 0.7734\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.0989\t Accuracy 0.7900\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.1487\t Accuracy 0.7298\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.1280\t Accuracy 0.7376\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.1185\t Accuracy 0.7353\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.1072\t Accuracy 0.7369\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.0896\t Accuracy 0.7412\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.0728\t Accuracy 0.7446\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.0669\t Accuracy 0.7443\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.0540\t Accuracy 0.7466\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.0440\t Accuracy 0.7479\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.0326\t Accuracy 0.7495\n",
      "\n",
      "Epoch [2]\t Average training loss 1.0184\t Average training accuracy 0.7528\n",
      "Epoch [2]\t Average validation loss 0.7759\t Average validation accuracy 0.8328\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.7988\t Accuracy 0.8400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.8478\t Accuracy 0.7961\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.8398\t Accuracy 0.7976\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.8413\t Accuracy 0.7903\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.8366\t Accuracy 0.7911\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.8264\t Accuracy 0.7936\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.8170\t Accuracy 0.7952\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.8173\t Accuracy 0.7939\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.8107\t Accuracy 0.7951\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.8064\t Accuracy 0.7950\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.8012\t Accuracy 0.7951\n",
      "\n",
      "Epoch [3]\t Average training loss 0.7929\t Average training accuracy 0.7970\n",
      "Epoch [3]\t Average validation loss 0.6117\t Average validation accuracy 0.8564\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.6446\t Accuracy 0.8700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.6868\t Accuracy 0.8261\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.6864\t Accuracy 0.8256\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.6928\t Accuracy 0.8193\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.6909\t Accuracy 0.8204\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.6842\t Accuracy 0.8221\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.6785\t Accuracy 0.8234\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.6814\t Accuracy 0.8220\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.6777\t Accuracy 0.8227\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.6761\t Accuracy 0.8221\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.6738\t Accuracy 0.8216\n",
      "\n",
      "Epoch [4]\t Average training loss 0.6684\t Average training accuracy 0.8230\n",
      "Epoch [4]\t Average validation loss 0.5191\t Average validation accuracy 0.8746\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.5560\t Accuracy 0.8700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.5930\t Accuracy 0.8475\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.5967\t Accuracy 0.8451\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.6054\t Accuracy 0.8379\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.6046\t Accuracy 0.8381\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.5996\t Accuracy 0.8403\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.5958\t Accuracy 0.8408\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.5998\t Accuracy 0.8392\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.5975\t Accuracy 0.8398\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.5970\t Accuracy 0.8392\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.5962\t Accuracy 0.8386\n",
      "\n",
      "Epoch [5]\t Average training loss 0.5924\t Average training accuracy 0.8397\n",
      "Epoch [5]\t Average validation loss 0.4608\t Average validation accuracy 0.8856\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.4996\t Accuracy 0.8800\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.5324\t Accuracy 0.8598\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.5385\t Accuracy 0.8553\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.5484\t Accuracy 0.8501\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.5480\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.5440\t Accuracy 0.8525\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.5414\t Accuracy 0.8529\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.5458\t Accuracy 0.8512\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.5442\t Accuracy 0.8518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.5444\t Accuracy 0.8511\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.5444\t Accuracy 0.8506\n",
      "\n",
      "Epoch [6]\t Average training loss 0.5416\t Average training accuracy 0.8515\n",
      "Epoch [6]\t Average validation loss 0.4209\t Average validation accuracy 0.8944\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.4607\t Accuracy 0.9000\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.4903\t Accuracy 0.8716\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.4977\t Accuracy 0.8667\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.5083\t Accuracy 0.8609\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.5081\t Accuracy 0.8605\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.5048\t Accuracy 0.8621\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.5029\t Accuracy 0.8625\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.5074\t Accuracy 0.8606\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.5063\t Accuracy 0.8611\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.5068\t Accuracy 0.8605\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.5073\t Accuracy 0.8599\n",
      "\n",
      "Epoch [7]\t Average training loss 0.5051\t Average training accuracy 0.8606\n",
      "Epoch [7]\t Average validation loss 0.3921\t Average validation accuracy 0.9010\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.4325\t Accuracy 0.9000\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.4593\t Accuracy 0.8775\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.4675\t Accuracy 0.8724\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.4784\t Accuracy 0.8674\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.4782\t Accuracy 0.8671\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.4753\t Accuracy 0.8688\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.4740\t Accuracy 0.8694\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.4786\t Accuracy 0.8674\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.4777\t Accuracy 0.8678\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.4784\t Accuracy 0.8671\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.4792\t Accuracy 0.8666\n",
      "\n",
      "Epoch [8]\t Average training loss 0.4775\t Average training accuracy 0.8671\n",
      "Epoch [8]\t Average validation loss 0.3704\t Average validation accuracy 0.9060\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.4110\t Accuracy 0.9100\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.4354\t Accuracy 0.8831\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.4440\t Accuracy 0.8778\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.4552\t Accuracy 0.8730\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.4550\t Accuracy 0.8728\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.4524\t Accuracy 0.8745\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.4515\t Accuracy 0.8750\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.4560\t Accuracy 0.8732\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.4554\t Accuracy 0.8737\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.4562\t Accuracy 0.8730\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.4572\t Accuracy 0.8724\n",
      "\n",
      "Epoch [9]\t Average training loss 0.4558\t Average training accuracy 0.8730\n",
      "Epoch [9]\t Average validation loss 0.3532\t Average validation accuracy 0.9100\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.3941\t Accuracy 0.9200\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.4165\t Accuracy 0.8892\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.4253\t Accuracy 0.8837\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.4366\t Accuracy 0.8786\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.4363\t Accuracy 0.8783\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.4340\t Accuracy 0.8800\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.4333\t Accuracy 0.8800\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.4378\t Accuracy 0.8779\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.4373\t Accuracy 0.8782\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.4383\t Accuracy 0.8777\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.4394\t Accuracy 0.8771\n",
      "\n",
      "Epoch [10]\t Average training loss 0.4382\t Average training accuracy 0.8775\n",
      "Epoch [10]\t Average validation loss 0.3393\t Average validation accuracy 0.9136\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.3804\t Accuracy 0.9200\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.4009\t Accuracy 0.8918\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.4099\t Accuracy 0.8876\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.4212\t Accuracy 0.8827\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.4209\t Accuracy 0.8828\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.4188\t Accuracy 0.8845\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.4183\t Accuracy 0.8844\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.4228\t Accuracy 0.8825\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.4223\t Accuracy 0.8826\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.4233\t Accuracy 0.8821\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.4245\t Accuracy 0.8814\n",
      "\n",
      "Epoch [11]\t Average training loss 0.4236\t Average training accuracy 0.8817\n",
      "Epoch [11]\t Average validation loss 0.3278\t Average validation accuracy 0.9166\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.3690\t Accuracy 0.9100\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.3878\t Accuracy 0.8965\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.3969\t Accuracy 0.8919\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.4082\t Accuracy 0.8876\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.4078\t Accuracy 0.8877\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.4059\t Accuracy 0.8891\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.4056\t Accuracy 0.8888\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.4100\t Accuracy 0.8871\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.4096\t Accuracy 0.8872\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.4107\t Accuracy 0.8866\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.4120\t Accuracy 0.8858\n",
      "\n",
      "Epoch [12]\t Average training loss 0.4112\t Average training accuracy 0.8858\n",
      "Epoch [12]\t Average validation loss 0.3181\t Average validation accuracy 0.9176\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.3592\t Accuracy 0.9100\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.3766\t Accuracy 0.8994\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.3857\t Accuracy 0.8952\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.3970\t Accuracy 0.8907\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.3966\t Accuracy 0.8907\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.3948\t Accuracy 0.8918\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.3947\t Accuracy 0.8912\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.3990\t Accuracy 0.8897\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.3987\t Accuracy 0.8898\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.3998\t Accuracy 0.8892\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.4011\t Accuracy 0.8884\n",
      "\n",
      "Epoch [13]\t Average training loss 0.4004\t Average training accuracy 0.8886\n",
      "Epoch [13]\t Average validation loss 0.3098\t Average validation accuracy 0.9198\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.3506\t Accuracy 0.9100\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.3669\t Accuracy 0.9029\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.3761\t Accuracy 0.8985\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.3872\t Accuracy 0.8940\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.3868\t Accuracy 0.8940\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.3852\t Accuracy 0.8951\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.3852\t Accuracy 0.8943\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.3895\t Accuracy 0.8928\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.3892\t Accuracy 0.8928\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.3903\t Accuracy 0.8920\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.3917\t Accuracy 0.8913\n",
      "\n",
      "Epoch [14]\t Average training loss 0.3911\t Average training accuracy 0.8913\n",
      "Epoch [14]\t Average validation loss 0.3025\t Average validation accuracy 0.9210\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.3429\t Accuracy 0.9200\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.3583\t Accuracy 0.9065\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.3675\t Accuracy 0.9015\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.3786\t Accuracy 0.8970\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.3782\t Accuracy 0.8967\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.3767\t Accuracy 0.8976\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.3768\t Accuracy 0.8967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.3810\t Accuracy 0.8954\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.3808\t Accuracy 0.8954\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.3819\t Accuracy 0.8946\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.3833\t Accuracy 0.8940\n",
      "\n",
      "Epoch [15]\t Average training loss 0.3828\t Average training accuracy 0.8940\n",
      "Epoch [15]\t Average validation loss 0.2961\t Average validation accuracy 0.9220\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.3360\t Accuracy 0.9300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.3507\t Accuracy 0.9080\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.3599\t Accuracy 0.9040\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.3710\t Accuracy 0.8993\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.3705\t Accuracy 0.8992\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.3691\t Accuracy 0.9002\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.3693\t Accuracy 0.8991\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.3735\t Accuracy 0.8976\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.3733\t Accuracy 0.8977\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.3745\t Accuracy 0.8969\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.3758\t Accuracy 0.8964\n",
      "\n",
      "Epoch [16]\t Average training loss 0.3754\t Average training accuracy 0.8964\n",
      "Epoch [16]\t Average validation loss 0.2904\t Average validation accuracy 0.9242\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.3299\t Accuracy 0.9300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.3440\t Accuracy 0.9120\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.3532\t Accuracy 0.9069\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.3641\t Accuracy 0.9021\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.3636\t Accuracy 0.9018\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.3623\t Accuracy 0.9025\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.3626\t Accuracy 0.9013\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.3667\t Accuracy 0.8997\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.3666\t Accuracy 0.8998\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.3677\t Accuracy 0.8990\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.3691\t Accuracy 0.8984\n",
      "\n",
      "Epoch [17]\t Average training loss 0.3687\t Average training accuracy 0.8983\n",
      "Epoch [17]\t Average validation loss 0.2854\t Average validation accuracy 0.9258\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.3244\t Accuracy 0.9300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.3379\t Accuracy 0.9133\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.3470\t Accuracy 0.9089\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.3579\t Accuracy 0.9034\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.3573\t Accuracy 0.9034\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.3561\t Accuracy 0.9042\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.3565\t Accuracy 0.9029\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.3606\t Accuracy 0.9011\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.3605\t Accuracy 0.9012\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.3616\t Accuracy 0.9005\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.3630\t Accuracy 0.8999\n",
      "\n",
      "Epoch [18]\t Average training loss 0.3627\t Average training accuracy 0.8999\n",
      "Epoch [18]\t Average validation loss 0.2808\t Average validation accuracy 0.9266\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.3194\t Accuracy 0.9400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.3323\t Accuracy 0.9139\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.3415\t Accuracy 0.9099\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.3523\t Accuracy 0.9048\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.3517\t Accuracy 0.9048\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.3506\t Accuracy 0.9056\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.3510\t Accuracy 0.9043\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.3550\t Accuracy 0.9025\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.3550\t Accuracy 0.9027\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.3561\t Accuracy 0.9019\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.3575\t Accuracy 0.9014\n",
      "\n",
      "Epoch [19]\t Average training loss 0.3572\t Average training accuracy 0.9013\n",
      "Epoch [19]\t Average validation loss 0.2766\t Average validation accuracy 0.9272\n",
      "\n",
      "Testing...\n",
      "The test accuracy is 0.9052.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc\n",
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "from layers import FCLayer, ReLULayer, SigmoidLayer\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 数据预处理函数\n",
    "def decode_image(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    return tf.one_hot(label, depth=10)\n",
    "\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))\n",
    "\n",
    "batch_size = 128\n",
    "max_epoch = 50\n",
    "init_std = 0.02\n",
    "learning_rate_SGD = 0.01\n",
    "weight_decay = 0.005\n",
    "disp_freq = 50\n",
    "\n",
    "# 创建单层隐含层的MLP\n",
    "single_layer_mlp = Network()\n",
    "single_layer_mlp.add(FCLayer(784, 128))  # 第一个隐含层\n",
    "single_layer_mlp.add(ReLULayer())        # 第一个隐含层的激活函数\n",
    "single_layer_mlp.add(FCLayer(128, 10))   # 输出层\n",
    "single_layer_mlp, single_layer_loss, single_layer_acc = train(single_layer_mlp, SoftmaxCrossEntropyLossLayer(), SGD(learning_rate_SGD, weight_decay), data_train, max_epoch, batch_size, disp_freq)\n",
    "test(single_layer_mlp, SoftmaxCrossEntropyLossLayer(), data_test, batch_size, disp_freq)\n",
    "\n",
    "# 创建具有两个隐含层的MLP\n",
    "two_layer_mlp = Network()\n",
    "two_layer_mlp.add(FCLayer(784, 128))    # 第一个隐含层\n",
    "two_layer_mlp.add(ReLULayer())          # 第一个隐含层的激活函数\n",
    "two_layer_mlp.add(FCLayer(128, 64))     # 第二个隐含层\n",
    "two_layer_mlp.add(ReLULayer())          # 第二个隐含层的激活函数\n",
    "two_layer_mlp.add(FCLayer(64, 10))      # 输出层\n",
    "two_layer_mlp, two_layer_loss, two_layer_acc = train(two_layer_mlp, SoftmaxCrossEntropyLossLayer(), SGD(learning_rate_SGD, weight_decay), data_train, max_epoch, batch_size, disp_freq)\n",
    "test(two_layer_mlp, SoftmaxCrossEntropyLossLayer(), data_test, batch_size, disp_freq)\n",
    "\n",
    "# 绘制损失和准确率曲线以进行比较\n",
    "plot_loss_and_acc({'Single Layer MLP': [single_layer_loss, single_layer_acc],\n",
    "                   'Two Layer MLP': [two_layer_loss, two_layer_acc]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "收敛速度上 & 训练的准确率上，多隐含层的网络结构有着更好的表现"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
